{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solved-diary",
   "metadata": {},
   "source": [
    "# StudentPerformance_GridSearchCrossValidation\n",
    "This is a duplicate notebook from StudentPerformance_with_NN. In this notebook I will use Grid Search Cross Validation to search for optimum hyperparameter instead of doing it empirically in the previous notebook.\n",
    "\n",
    "Hyperparameters Grid Search 101: \n",
    "\n",
    "https://elutins.medium.com/grid-searching-in-machine-learning-quick-explanation-and-python-implementation-550552200596\n",
    "\n",
    "References: \n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-algeria",
   "metadata": {},
   "source": [
    "### Other Notebooks(in this learnign \"series\"):\n",
    "1. [Genetic Algorithm with Python](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_with_Python.ipynb) --> Understanding Genetic Algorithm with Python(without libraries)\n",
    "2. [Predicting Student Performance(Empirically)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_with_NN.ipynb) --> Empirically tune hyper-parameters\n",
    "\n",
    "***This Notebook*** 3. [Predicting Student Performance(SKLearn's GridSearchCV)](ww) --> Tune hyper-parameters with Sci-Kit Learn's GridSearchCV\n",
    "\n",
    "4. [Predicting Student Performance(Genetic Algorithm with PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_PyGAD.ipynb) --> Tune hyper-parameters with GA using PyGAD\n",
    "5. [Predicting Student Performance(Genetic Algorithm with Trained Model + PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_Tensorflow_PyGAD.ipynb) --> Tune GA hyper-parameters using PyGAD of a trained Model\n",
    "6. [Some Findings, Comparison, Summary](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_summary.ipynb) --> Summary of this learning \"series\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-trinity",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "1. Dataset\n",
    "2. Packages Needed\n",
    "3. Data Preprocessing\n",
    "4. Grid Search Cross Validation\n",
    "5. Inference\n",
    "6. Some findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-manufacturer",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-provider",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. \n",
    "\n",
    "Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). \n",
    "\n",
    "### Note: In this notebook, I used the Dataset with Portuguese Language\n",
    "\n",
    "In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. \n",
    "\n",
    "Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. \n",
    "\n",
    "This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd-period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-small",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "\n",
    "### Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n",
    "\n",
    "* 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)* \n",
    "* 2 sex - student's sex (binary: 'F' - female or 'M' - male)\n",
    "* 3 age - student's age (numeric: from 15 to 22)\n",
    "* 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n",
    "* 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n",
    "* 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n",
    "* 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
    "* 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
    "* 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "* 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "* 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
    "* 12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n",
    "* 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "* 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "* 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "* 16 schoolsup - extra educational support (binary: yes or no)\n",
    "* 17 famsup - family educational support (binary: yes or no)\n",
    "* 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "* 19 activities - extra-curricular activities (binary: yes or no)\n",
    "* 20 nursery - attended nursery school (binary: yes or no)\n",
    "* 21 higher - wants to take higher education (binary: yes or no)\n",
    "* 22 internet - Internet access at home (binary: yes or no)\n",
    "* 23 romantic - with a romantic relationship (binary: yes or no)\n",
    "* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "* 30 absences - number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "#### these grades are related with the course subject, Math or Portuguese:\n",
    "* 31 G1 - first period grade (numeric: from 0 to 20)\n",
    "* 31 G2 - second period grade (numeric: from 0 to 20)\n",
    "* 32 G3 - final grade (numeric: from 0 to 20, output target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-staff",
   "metadata": {},
   "source": [
    "## 2. Packages Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "integrated-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-roulette",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "Here, I will find for empty data, encode categorical data, plot some graph to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  G3  \n",
       "0       1     1       3         4   0  11  11  \n",
       "1       1     1       3         2   9  11  11  \n",
       "2       2     3       3         6  12  13  12  \n",
       "3       1     1       5         0  14  14  14  \n",
       "4       1     2       5         0  11  13  13  \n",
       "..    ...   ...     ...       ...  ..  ..  ..  \n",
       "644     1     2       5         4  10  11  10  \n",
       "645     1     1       1         4  15  15  16  \n",
       "646     1     1       5         6  11  12   9  \n",
       "647     3     4       2         6  10  10  10  \n",
       "648     3     4       5         4  10  11  11  \n",
       "\n",
       "[649 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"student-por.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-sending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.744222</td>\n",
       "      <td>2.514638</td>\n",
       "      <td>2.306626</td>\n",
       "      <td>1.568567</td>\n",
       "      <td>1.930663</td>\n",
       "      <td>0.221880</td>\n",
       "      <td>3.930663</td>\n",
       "      <td>3.180277</td>\n",
       "      <td>3.184900</td>\n",
       "      <td>1.502311</td>\n",
       "      <td>2.280431</td>\n",
       "      <td>3.536210</td>\n",
       "      <td>3.659476</td>\n",
       "      <td>11.399076</td>\n",
       "      <td>11.570108</td>\n",
       "      <td>11.906009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.218138</td>\n",
       "      <td>1.134552</td>\n",
       "      <td>1.099931</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.593235</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>1.051093</td>\n",
       "      <td>1.175766</td>\n",
       "      <td>0.924834</td>\n",
       "      <td>1.284380</td>\n",
       "      <td>1.446259</td>\n",
       "      <td>4.640759</td>\n",
       "      <td>2.745265</td>\n",
       "      <td>2.913639</td>\n",
       "      <td>3.230656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean    16.744222    2.514638    2.306626    1.568567    1.930663    0.221880   \n",
       "std      1.218138    1.134552    1.099931    0.748660    0.829510    0.593235   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    2.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean     3.930663    3.180277    3.184900    1.502311    2.280431    3.536210   \n",
       "std      0.955717    1.051093    1.175766    0.924834    1.284380    1.446259   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    2.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences          G1          G2          G3  \n",
       "count  649.000000  649.000000  649.000000  649.000000  \n",
       "mean     3.659476   11.399076   11.570108   11.906009  \n",
       "std      4.640759    2.745265    2.913639    3.230656  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000   10.000000   10.000000   10.000000  \n",
       "50%      2.000000   11.000000   11.000000   12.000000  \n",
       "75%      6.000000   13.000000   13.000000   14.000000  \n",
       "max     32.000000   19.000000   19.000000   19.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infectious-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum() \n",
    "# Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entitled-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
      "\n",
      "Number of Features: 32\n"
     ]
    }
   ],
   "source": [
    "features_list = list(df.columns)[:-1]\n",
    "print(\"Features: {x}\".format(x = features_list))\n",
    "print(\"\\nNumber of Features: {x}\".format(x = len(features_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brutal-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "# # Check datatype of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brilliant-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  \n",
       "0       1     1       3         4   0  11  \n",
       "1       1     1       3         2   9  11  \n",
       "2       2     3       3         6  12  13  \n",
       "3       1     1       5         0  14  14  \n",
       "4       1     2       5         0  11  13  \n",
       "..    ...   ...     ...       ...  ..  ..  \n",
       "644     1     2       5         4  10  11  \n",
       "645     1     1       1         4  15  15  \n",
       "646     1     1       5         6  11  12  \n",
       "647     3     4       2         6  10  10  \n",
       "648     3     4       5         4  10  11  \n",
       "\n",
       "[649 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop([\"G3\"], axis = 1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "small-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     G3\n",
       "0    11\n",
       "1    11\n",
       "2    12\n",
       "3    14\n",
       "4    13\n",
       "..   ..\n",
       "644  10\n",
       "645  16\n",
       "646   9\n",
       "647  10\n",
       "648  11\n",
       "\n",
       "[649 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.drop(features_list, axis = 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opponent-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_school = {'GP' : 0, 'MS' : 1}\n",
    "mapping_sex = {'F' : 0, 'M' : 1}\n",
    "mapping_address = {'U' : 0, 'R' : 1}\n",
    "mapping_famsize = {'GT3' : 0, 'LE3' : 1}\n",
    "mapping_pstatus = {'A' : 0, 'T' : 1}\n",
    "mapping_mjob = {'at_home' : 0, 'health' : 1, 'other' : 2, 'services' : 3, 'teacher' : 4}\n",
    "mapping_fjob = {'at_home' : 0, 'health' : 1, 'other' : 2, 'services' : 3, 'teacher' : 4}\n",
    "mapping_reason = {'course' : 0, 'other' : 1, 'home' : 2, 'reputation' : 3}\n",
    "mapping_guardian = {'mother' : 0, 'father' : 1, 'other': 2}\n",
    "mapping_schoolsup = {'no' : 0, 'yes' : 1}\n",
    "mapping_famsup = {'no' : 0, 'yes' : 1}\n",
    "mapping_romantic = {'no' : 0, 'yes' : 1}\n",
    "mapping_paid = {'no' : 0, 'yes' : 1}\n",
    "mapping_activities = {'no' : 0, 'yes' : 1}\n",
    "mapping_nursery = {'no' : 0, 'yes' : 1}\n",
    "mapping_higher = {'no' : 0, 'yes' : 1}\n",
    "mapping_internet = {'no' : 0, 'yes' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strong-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['school'] = features['school'].map(mapping_school)\n",
    "df['sex'] = features['sex'].map(mapping_sex)\n",
    "df['address'] = features['address'].map(mapping_address)\n",
    "df['famsize'] = features['famsize'].map(mapping_famsize)\n",
    "df['Pstatus'] = features['Pstatus'].map(mapping_pstatus)\n",
    "df['Mjob'] = features['Mjob'].map(mapping_mjob)\n",
    "df['Fjob'] = features['Fjob'].map(mapping_fjob)\n",
    "df['reason'] = features['reason'].map(mapping_reason)\n",
    "df['guardian'] = features['guardian'].map(mapping_guardian).astype('Int64')\n",
    "df['famsup'] = features['famsup'].map(mapping_famsup)\n",
    "df['schoolsup'] = features['schoolsup'].map(mapping_schoolsup)\n",
    "df['romantic'] = features['romantic'].map(mapping_romantic)\n",
    "df['paid'] = features['paid'].map(mapping_paid)\n",
    "df['activities'] = features['activities'].map(mapping_activities)\n",
    "df['nursery'] = features['nursery'].map(mapping_nursery)\n",
    "df['higher'] = features['higher'].map(mapping_higher)\n",
    "df['internet'] = features['internet'].map(mapping_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conditional-italic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  \n",
       "0       1     1       3         4   0  11  \n",
       "1       1     1       3         2   9  11  \n",
       "2       2     3       3         6  12  13  \n",
       "3       1     1       5         0  14  14  \n",
       "4       1     2       5         0  11  13  \n",
       "..    ...   ...     ...       ...  ..  ..  \n",
       "644     1     2       5         4  10  11  \n",
       "645     1     1       1         4  15  15  \n",
       "646     1     1       5         6  11  12  \n",
       "647     3     4       2         6  10  10  \n",
       "648     3     4       5         4  10  11  \n",
       "\n",
       "[649 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "literary-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "failing-qualification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='school', data=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vocational-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features.hist(bins=20, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-secretariat",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "As I go through the data, I found there are students who achieve 0 in their G3 depsite having more than half during their G1 & G2 examinations. \n",
    "The outliers might affect model's performance, hence I decided to drop those outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "young-preview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=target[\"G3\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-frost",
   "metadata": {},
   "source": [
    "#### Drop Outliers data if certain requirement satisfied.\n",
    "\n",
    "G3 with score of 2.5 and less considered as Outliers. To understand this data, I will list down these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "twenty-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  \\\n",
       "163       0    1   18        0        1        1     1     1     2     2   \n",
       "172       0    1   16        0        0        1     3     3     2     3   \n",
       "440       1    1   16        0        0        1     1     1     0     3   \n",
       "519       1    1   16        1        0        1     2     1     2     3   \n",
       "563       1    1   17        0        0        1     2     2     2     2   \n",
       "567       1    1   18        1        0        1     3     2     3     2   \n",
       "583       1    0   18        1        0        1     2     2     2     2   \n",
       "586       1    0   17        0        0        1     4     2     4     3   \n",
       "597       1    0   18        1        0        1     2     2     0     2   \n",
       "603       1    0   18        1        1        0     4     2     4     2   \n",
       "605       1    0   19        0        0        1     1     1     0     3   \n",
       "610       1    0   19        1        0        0     1     1     0     0   \n",
       "626       1    0   18        1        0        1     4     4     2     4   \n",
       "637       1    1   18        1        0        1     2     1     2     2   \n",
       "639       1    1   19        1        0        1     1     1     2     3   \n",
       "640       1    1   18        1        0        1     4     2     2     2   \n",
       "\n",
       "     reason  guardian  traveltime  studytime  failures  schoolsup  famsup  \\\n",
       "163       0         0           1          1         2          0       0   \n",
       "172       0         1           1          2         1          0       1   \n",
       "440       2         0           2          2         0          0       1   \n",
       "519       3         0           2          2         0          0       0   \n",
       "563       0         0           1          1         1          0       0   \n",
       "567       0         0           1          1         1          0       0   \n",
       "583       1         0           2          1         1          0       0   \n",
       "586       2         0           1          2         0          1       1   \n",
       "597       0         0           3          2         1          0       0   \n",
       "603       3         0           1          2         0          0       0   \n",
       "605       1         1           2          1         1          0       0   \n",
       "610       0         2           2          2         3          0       1   \n",
       "626       1         1           3          2         0          0       1   \n",
       "637       1         0           2          1         0          0       0   \n",
       "639       1         0           2          1         1          0       0   \n",
       "640       2         1           2          1         1          0       0   \n",
       "\n",
       "     paid  activities  nursery  higher  internet  romantic  famrel  freetime  \\\n",
       "163     0           0        1       0         1         1       2         3   \n",
       "172     0           0        1       1         1         1       4         5   \n",
       "440     0           1        1       1         0         1       5         4   \n",
       "519     0           1        1       1         1         0       5         2   \n",
       "563     0           1        1       1         0         1       1         2   \n",
       "567     0           0        1       0         1         0       2         3   \n",
       "583     0           0        1       0         1         1       5         5   \n",
       "586     0           1        1       1         1         0       5         5   \n",
       "597     0           1        1       1         0         1       4         3   \n",
       "603     0           1        1       1         1         1       5         3   \n",
       "605     0           0        1       0         0         0       5         5   \n",
       "610     0           1        1       0         0         1       3         5   \n",
       "626     0           0        0       1         1         1       3         2   \n",
       "637     0           1        0       1         1         1       4         4   \n",
       "639     0           0        1       1         0         0       4         3   \n",
       "640     1           0        1       1         0         0       5         4   \n",
       "\n",
       "     goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
       "163      5     2     5       4         0  11   9   0  \n",
       "172      5     4     4       5         0  10  10   1  \n",
       "440      5     4     5       3         0   7   0   0  \n",
       "519      1     1     1       2         0   8   7   0  \n",
       "563      1     2     3       5         0   7   0   0  \n",
       "567      1     2     2       5         0   4   0   0  \n",
       "583      5     1     1       3         0   8   6   0  \n",
       "586      5     1     3       5         0   8   8   0  \n",
       "597      3     1     1       4         0   9   0   0  \n",
       "603      1     1     1       5         0   5   0   0  \n",
       "605      5     2     3       2         0   5   0   0  \n",
       "610      4     1     4       1         0   8   0   0  \n",
       "626      2     4     2       5         0   7   5   0  \n",
       "637      3     1     3       5         0   7   7   0  \n",
       "639      2     1     3       5         0   5   8   0  \n",
       "640      3     4     3       3         0   7   7   0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outliers = df[df[\"G3\"]<2.5]\n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "english-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confidential-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.399075500770415"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"G1\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ready-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"G2\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-toner",
   "metadata": {},
   "source": [
    "#### Dropping rows if:\n",
    "\n",
    "G3 considered as outliers when G1 or G2 above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caroline-count",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  \\\n",
       "0         0    0   18        0        0        0     4     4     0     4   \n",
       "1         0    0   17        0        0        1     1     1     0     2   \n",
       "2         0    0   15        0        1        1     1     1     0     2   \n",
       "3         0    0   15        0        0        1     4     2     1     3   \n",
       "4         0    0   16        0        0        1     3     3     2     2   \n",
       "..      ...  ...  ...      ...      ...      ...   ...   ...   ...   ...   \n",
       "644       1    0   19        1        0        1     2     3     3     2   \n",
       "645       1    0   18        0        1        1     3     1     4     3   \n",
       "646       1    0   18        0        0        1     1     1     2     2   \n",
       "647       1    1   17        0        1        1     3     1     3     3   \n",
       "648       1    1   18        1        1        1     3     2     3     2   \n",
       "\n",
       "     reason  guardian  traveltime  studytime  failures  schoolsup  famsup  \\\n",
       "0         0         0           2          2         0          1       0   \n",
       "1         0         1           1          2         0          0       1   \n",
       "2         1         0           1          2         0          1       0   \n",
       "3         2         0           1          3         0          0       1   \n",
       "4         2         1           1          2         0          0       1   \n",
       "..      ...       ...         ...        ...       ...        ...     ...   \n",
       "644       0         0           1          3         1          0       0   \n",
       "645       0         0           1          2         0          0       1   \n",
       "646       0         0           2          2         0          0       0   \n",
       "647       0         0           2          1         0          0       0   \n",
       "648       0         0           3          1         0          0       0   \n",
       "\n",
       "     paid  activities  nursery  higher  internet  romantic  famrel  freetime  \\\n",
       "0       0           0        1       1         0         0       4         3   \n",
       "1       0           0        0       1         1         0       5         3   \n",
       "2       0           0        1       1         1         0       4         3   \n",
       "3       0           1        1       1         1         1       3         2   \n",
       "4       0           0        1       1         0         0       4         3   \n",
       "..    ...         ...      ...     ...       ...       ...     ...       ...   \n",
       "644     0           1        0       1         1         0       5         4   \n",
       "645     0           0        1       1         1         0       4         3   \n",
       "646     0           1        1       1         0         0       1         1   \n",
       "647     0           0        0       1         1         0       2         4   \n",
       "648     0           0        0       1         1         0       4         4   \n",
       "\n",
       "     goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
       "0        4     1     1       3         4   0  11  11  \n",
       "1        3     1     1       3         2   9  11  11  \n",
       "2        2     2     3       3         6  12  13  12  \n",
       "3        2     1     1       5         0  14  14  14  \n",
       "4        2     1     2       5         0  11  13  13  \n",
       "..     ...   ...   ...     ...       ...  ..  ..  ..  \n",
       "644      2     1     2       5         4  10  11  10  \n",
       "645      4     1     1       1         4  15  15  16  \n",
       "646      1     1     1       5         6  11  12   9  \n",
       "647      5     3     4       2         6  10  10  10  \n",
       "648      1     3     4       5         4  10  11  11  \n",
       "\n",
       "[633 rows x 33 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_idx = df[df[\"G3\"] < 2.5].index\n",
    "len(outlier_idx) \n",
    "df = df.drop(outlier_idx)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-exposure",
   "metadata": {},
   "source": [
    "### Reassign features and target dataframe after dropping outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "directed-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.drop(features_list, axis = 1)\n",
    "features = df.drop([\"G3\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-victim",
   "metadata": {},
   "source": [
    "## 4. Grid Search Cross Validation\n",
    "\n",
    "Here the real thing begins. We will use Sci-Kit Learn's GridSearchCV to find the optimal hypter parameters for the model for this problem.\n",
    "\n",
    "#### To be tuned:\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm\n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-letters",
   "metadata": {},
   "source": [
    "#### Search/Tuning Flow:\n",
    "1. Define Model\n",
    "2. Wrap Keras model with KerasRegressor of Sci-Kit Learn\n",
    "3. Define Grid Parameters\n",
    "4. Train model with Grid Parameters defined\n",
    "5. Summarize Results\n",
    "\n",
    "Note: Set verbose = 0, as it's gonna be long list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-jacket",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-postcard",
   "metadata": {},
   "source": [
    "#### Split Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affecting-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "republican-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affecting-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "y_test_scaled = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "annual-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled = scaler.inverse_transform(X_train_scaled).astype('float64') \n",
    "y_train_unscaled = scaler.inverse_transform(y_train_scaled).astype('float64') \n",
    "X_test_unscaled = scaler.inverse_transform(X_test_scaled).astype('float64') \n",
    "y_test_unscaled = scaler.inverse_transform(y_test_scaled).astype('float64') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "medical-despite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 32), (443, 32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-times",
   "metadata": {},
   "source": [
    "### Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-bandwidth",
   "metadata": {},
   "source": [
    "### Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "\n",
    "1. ###### Batch Size & Number of Epochs with Training Optimization Algorithm (This section)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm \n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n",
    "\n",
    "Based on the best parameter obtained, we can further fine tune it. For instance, the Adam optimizer used with default Learning Rate of 0.001. Here, GridSearchCV is used again to search for best Learning Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "considerable-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,281\n",
      "Trainable params: 5,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "timely-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "specialized-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "batch_size = [10, 50]\n",
    "epochs = [10, 50, 100, 200, 500, 700, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ultimate-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.009181 using {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.279537 (0.185846) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'SGD'}\n",
      "-0.153868 (0.186314) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'RMSprop'}\n",
      "-0.283783 (0.180655) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'Adagrad'}\n",
      "-0.017520 (0.004620) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'Adam'}\n",
      "-0.016705 (0.002449) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "-0.281458 (0.192481) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "-0.022869 (0.007195) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'Adagrad'}\n",
      "-0.151504 (0.196080) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "-0.017360 (0.004792) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.152149 (0.195659) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.019997 (0.005539) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.012150 (0.002359) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.151648 (0.195987) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'SGD'}\n",
      "-0.147137 (0.190845) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'RMSprop'}\n",
      "-0.015185 (0.002828) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'Adagrad'}\n",
      "-0.011459 (0.003277) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'Adam'}\n",
      "-0.010346 (0.002566) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'SGD'}\n",
      "-0.011955 (0.004506) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'RMSprop'}\n",
      "-0.016548 (0.005266) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'Adagrad'}\n",
      "-0.010020 (0.003102) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'Adam'}\n",
      "-0.009606 (0.001954) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'SGD'}\n",
      "-0.012910 (0.002452) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'RMSprop'}\n",
      "-0.013756 (0.003341) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adagrad'}\n",
      "-0.009181 (0.003678) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.290223 (0.187667) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.147466 (0.190564) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.016944 (0.004093) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.147154 (0.190795) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.169644 (0.183346) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'SGD'}\n",
      "-0.025840 (0.003522) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'RMSprop'}\n",
      "-0.167838 (0.184527) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'Adagrad'}\n",
      "-0.162138 (0.188616) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'Adam'}\n",
      "-0.166561 (0.185516) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "-0.012595 (0.004544) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "-0.026488 (0.008357) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adagrad'}\n",
      "-0.287659 (0.191292) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "-0.021396 (0.004066) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.017523 (0.002887) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.024120 (0.006072) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.151252 (0.187917) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.020752 (0.002579) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'SGD'}\n",
      "-0.016128 (0.005276) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'RMSprop'}\n",
      "-0.148937 (0.181778) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'Adagrad'}\n",
      "-0.013592 (0.003179) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'Adam'}\n",
      "-0.014516 (0.005352) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'SGD'}\n",
      "-0.014056 (0.004813) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'RMSprop'}\n",
      "-0.017728 (0.002687) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'Adagrad'}\n",
      "-0.014298 (0.001329) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'Adam'}\n",
      "-0.013747 (0.002332) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'SGD'}\n",
      "-0.014231 (0.003759) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'RMSprop'}\n",
      "-0.016384 (0.004249) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'Adagrad'}\n",
      "-0.012592 (0.000909) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.022778 (0.002853) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.150281 (0.188579) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.148360 (0.182190) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.014415 (0.000676) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-religion",
   "metadata": {},
   "source": [
    "Some reading on the negative values on scoring : \n",
    "\n",
    "https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-excitement",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. ###### Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (This section)\n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n",
    "\n",
    "Based on the best parameter obtained, we can further fine tune it. For instance, the Adam optimizer used with default Learning Rate of 0.001. Here, GridSearchCV is used again to search for best Learning Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "prescribed-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_Adam(learn_rate=0.001):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    optimizer = Adam(lr = learn_rate)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_Adam = KerasRegressor(build_fn=create_model_Adam, epochs=700, batch_size=10,  verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "seeing-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.005, 0.1]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model_Adam, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "equipped-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.008193 using {'learn_rate': 0.01}\n",
      "-0.149856 (0.197258) with: {'learn_rate': 0.001}\n",
      "-0.008193 (0.002773) with: {'learn_rate': 0.01}\n",
      "-0.008898 (0.002184) with: {'learn_rate': 0.005}\n",
      "-0.417256 (0.009306) with: {'learn_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-professor",
   "metadata": {},
   "source": [
    "### Network Weight Initialization\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. ###### Network Weight Initialization(This section)\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mental-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_weight_init(init_mode = 'uniform'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu', kernel_initializer=init_mode),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_weight_init = KerasRegressor(build_fn=create_model_weight_init, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "legal-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_weight_init, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "divided-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.007511 using {'init_mode': 'lecun_uniform'}\n",
      "-0.009289 (0.001876) with: {'init_mode': 'uniform'}\n",
      "-0.007511 (0.000750) with: {'init_mode': 'lecun_uniform'}\n",
      "-0.010183 (0.000559) with: {'init_mode': 'normal'}\n",
      "-0.419239 (0.008901) with: {'init_mode': 'zero'}\n",
      "-0.008358 (0.000608) with: {'init_mode': 'glorot_normal'}\n",
      "-0.010530 (0.002378) with: {'init_mode': 'glorot_uniform'}\n",
      "-0.146964 (0.194618) with: {'init_mode': 'he_normal'}\n",
      "-0.009648 (0.001557) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-violence",
   "metadata": {},
   "source": [
    "### Neuron Activation Function\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(lecun_uniform)\n",
    "5. ###### Neuron Activation Function (This section)\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distant-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_act_func(activation='relu', output_activation = 'sigmoid'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu', kernel_initializer='lecun_uniform'),\n",
    "        Dense(64, activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_act_func = KerasRegressor(build_fn=create_model_act_func, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "charming-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "output_activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation, output_activation=output_activation)\n",
    "grid = GridSearchCV(estimator=model_act_func, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rubber-former",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.007706 using {'activation': 'linear', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softmax', 'output_activation': 'softmax'}\n",
      "-0.009161 (0.001533) with: {'activation': 'softmax', 'output_activation': 'softplus'}\n",
      "-0.024258 (0.008749) with: {'activation': 'softmax', 'output_activation': 'softsign'}\n",
      "-0.286598 (0.181235) with: {'activation': 'softmax', 'output_activation': 'relu'}\n",
      "-0.024121 (0.008951) with: {'activation': 'softmax', 'output_activation': 'tanh'}\n",
      "-0.010295 (0.001206) with: {'activation': 'softmax', 'output_activation': 'hard_sigmoid'}\n",
      "-0.028771 (0.003188) with: {'activation': 'softmax', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softplus', 'output_activation': 'softmax'}\n",
      "-0.008475 (0.000064) with: {'activation': 'softplus', 'output_activation': 'softplus'}\n",
      "-0.010775 (0.001842) with: {'activation': 'softplus', 'output_activation': 'softsign'}\n",
      "-0.419239 (0.008901) with: {'activation': 'softplus', 'output_activation': 'relu'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softplus', 'output_activation': 'tanh'}\n",
      "-0.011639 (0.003424) with: {'activation': 'softplus', 'output_activation': 'hard_sigmoid'}\n",
      "-0.008378 (0.001354) with: {'activation': 'softplus', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softsign', 'output_activation': 'softmax'}\n",
      "-0.011089 (0.002055) with: {'activation': 'softsign', 'output_activation': 'softplus'}\n",
      "-0.012855 (0.001549) with: {'activation': 'softsign', 'output_activation': 'softsign'}\n",
      "-0.010529 (0.001922) with: {'activation': 'softsign', 'output_activation': 'relu'}\n",
      "-0.009947 (0.002298) with: {'activation': 'softsign', 'output_activation': 'tanh'}\n",
      "-0.009909 (0.001789) with: {'activation': 'softsign', 'output_activation': 'hard_sigmoid'}\n",
      "-0.010643 (0.000951) with: {'activation': 'softsign', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'tanh', 'output_activation': 'softmax'}\n",
      "-0.008445 (0.001837) with: {'activation': 'tanh', 'output_activation': 'softplus'}\n",
      "-0.011304 (0.001610) with: {'activation': 'tanh', 'output_activation': 'softsign'}\n",
      "-0.283108 (0.190627) with: {'activation': 'tanh', 'output_activation': 'relu'}\n",
      "-0.010336 (0.000477) with: {'activation': 'tanh', 'output_activation': 'tanh'}\n",
      "-0.010848 (0.001205) with: {'activation': 'tanh', 'output_activation': 'hard_sigmoid'}\n",
      "-0.010669 (0.001457) with: {'activation': 'tanh', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'sigmoid', 'output_activation': 'softmax'}\n",
      "-0.008795 (0.001676) with: {'activation': 'sigmoid', 'output_activation': 'softplus'}\n",
      "-0.010369 (0.000915) with: {'activation': 'sigmoid', 'output_activation': 'softsign'}\n",
      "-0.146223 (0.195138) with: {'activation': 'sigmoid', 'output_activation': 'relu'}\n",
      "-0.009023 (0.000562) with: {'activation': 'sigmoid', 'output_activation': 'tanh'}\n",
      "-0.010864 (0.000667) with: {'activation': 'sigmoid', 'output_activation': 'hard_sigmoid'}\n",
      "-0.009131 (0.001044) with: {'activation': 'sigmoid', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'hard_sigmoid', 'output_activation': 'softmax'}\n",
      "-0.008525 (0.000430) with: {'activation': 'hard_sigmoid', 'output_activation': 'softplus'}\n",
      "-0.010965 (0.002323) with: {'activation': 'hard_sigmoid', 'output_activation': 'softsign'}\n",
      "-0.280799 (0.189432) with: {'activation': 'hard_sigmoid', 'output_activation': 'relu'}\n",
      "-0.010008 (0.001598) with: {'activation': 'hard_sigmoid', 'output_activation': 'tanh'}\n",
      "-0.009042 (0.002803) with: {'activation': 'hard_sigmoid', 'output_activation': 'hard_sigmoid'}\n",
      "-0.009524 (0.001091) with: {'activation': 'hard_sigmoid', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'linear', 'output_activation': 'softmax'}\n",
      "-0.153807 (0.190055) with: {'activation': 'linear', 'output_activation': 'softplus'}\n",
      "-0.011334 (0.002098) with: {'activation': 'linear', 'output_activation': 'softsign'}\n",
      "-0.287136 (0.195375) with: {'activation': 'linear', 'output_activation': 'relu'}\n",
      "-0.168630 (0.008872) with: {'activation': 'linear', 'output_activation': 'tanh'}\n",
      "-0.008507 (0.002478) with: {'activation': 'linear', 'output_activation': 'hard_sigmoid'}\n",
      "-0.007706 (0.001413) with: {'activation': 'linear', 'output_activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-strengthening",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. ###### Dropout Regularization (This section)\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-niger",
   "metadata": {},
   "source": [
    "### Check if the model is overfitting. \n",
    "If overfit --> add Dropout layer to regularize it, else skip having dropout layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "technical-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_current(activation= 'linear', output_activation = 'linear'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation=activation, kernel_initializer='lecun_uniform'),\n",
    "        Dense(64, activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_current = create_model_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "boxed-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_current.fit(X_train, y_train, epochs=700, batch_size=10,  verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "delayed-benchmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "liked-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00700\n",
      "Mean Absolute Error: 0.05615\n"
     ]
    }
   ],
   "source": [
    "mean_squared_e = mean_squared_error(y_test, model_current.predict(X_test))\n",
    "mean_absolute_e = mean_absolute_error(y_test, model_current.predict(X_test))\n",
    "\n",
    "print(\"Mean Squared Error: {:.5f}\".format(mean_squared_e))\n",
    "print(\"Mean Absolute Error: {:.5f}\".format(mean_absolute_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-gates",
   "metadata": {},
   "source": [
    "Based on the graph, it's overfitting, hence, will use Dropout to regularize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "comparative-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dropout(dropout_rate=0.0, weight_constraint=0):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(weight_constraint),\n",
    "             ),\n",
    "        Dense(64, activation='linear'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_dropout = KerasRegressor(build_fn=create_model_dropout, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "elegant-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model_dropout, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ranging-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.005696 using {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-0.008676 (0.003370) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "-0.006832 (0.002109) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "-0.006418 (0.000472) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "-0.007079 (0.000357) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "-0.006290 (0.000567) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "-0.009926 (0.002168) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "-0.005843 (0.000249) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "-0.007070 (0.000789) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "-0.006986 (0.001584) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "-0.007178 (0.001208) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "-0.006401 (0.000703) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "-0.007322 (0.001090) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "-0.006321 (0.000185) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "-0.006330 (0.001531) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "-0.005696 (0.000961) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-0.006039 (0.001164) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "-0.005972 (0.000927) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "-0.006441 (0.000620) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "-0.005949 (0.000229) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "-0.005938 (0.001228) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "-0.012160 (0.002270) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "-0.006102 (0.000794) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "-0.006866 (0.000340) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "-0.005879 (0.000905) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "-0.009077 (0.004544) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "-0.005770 (0.000746) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "-0.005991 (0.001093) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "-0.006973 (0.001444) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "-0.006232 (0.001181) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "-0.006957 (0.001568) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "-0.007830 (0.002930) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "-0.011096 (0.005059) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "-0.006223 (0.001395) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "-0.007421 (0.002853) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "-0.085004 (0.105859) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "-0.007378 (0.001353) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "-0.007049 (0.000998) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "-0.006178 (0.001011) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "-0.007577 (0.002851) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "-0.007179 (0.001591) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "-0.007776 (0.001627) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "-0.016252 (0.010666) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "-0.008409 (0.001422) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "-0.007769 (0.002212) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "-0.008180 (0.002944) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-privilege",
   "metadata": {},
   "source": [
    "### Neuron Activation Function\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 500, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer(This section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "auburn-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_neurons_hidden_layer(neurons_h_layer_1 = 1, neurons_h_layer_2 = 1):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(5),\n",
    "             ),\n",
    "        Dense(neurons_h_layer_1, activation='linear'),\n",
    "        Dropout(0.2),\n",
    "        Dense(neurons_h_layer_2, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_neurons_hidden_layer = KerasRegressor(build_fn=create_model_neurons_hidden_layer, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "incorporated-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "neurons_h_layer_1 = [8, 16, 32, 64, 128]\n",
    "neurons_h_layer_2 = [16, 32, 64, 128]\n",
    "param_grid = dict(neurons_h_layer_1=neurons_h_layer_1, neurons_h_layer_2=neurons_h_layer_2)\n",
    "grid = GridSearchCV(estimator=model_neurons_hidden_layer, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hispanic-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.004469 using {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 32}\n",
      "-0.005880 (0.001828) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 16}\n",
      "-0.004469 (0.001343) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 32}\n",
      "-0.005019 (0.002687) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 64}\n",
      "-0.004588 (0.001767) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 128}\n",
      "-0.005357 (0.002798) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 16}\n",
      "-0.005053 (0.001243) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 32}\n",
      "-0.004617 (0.001724) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 64}\n",
      "-0.005306 (0.001086) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 128}\n",
      "-0.005038 (0.001186) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 16}\n",
      "-0.006015 (0.001433) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 32}\n",
      "-0.005585 (0.002128) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 64}\n",
      "-0.007054 (0.003031) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 128}\n",
      "-0.004879 (0.002587) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 16}\n",
      "-0.005504 (0.003539) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 32}\n",
      "-0.004811 (0.001322) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 64}\n",
      "-0.006063 (0.002176) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 128}\n",
      "-0.004519 (0.001635) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 16}\n",
      "-0.005425 (0.002256) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 32}\n",
      "-0.004894 (0.001835) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 64}\n",
      "-0.012672 (0.008984) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 128}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-mechanism",
   "metadata": {},
   "source": [
    "### Optimal Parameters\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 500, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. Dropout Regularization (dropout_rate: 0.0, weight_constraint: 5)\n",
    "7. Number of Neurons in the Hidden Layer(First Hidden layer = 8, Second Hidden layer = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-sending",
   "metadata": {},
   "source": [
    "#### Grid Searched CV's model\n",
    "\n",
    "With the best parameter GridSearchCV could find based on given set of parameters grid. Now we instantiate a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "experimental-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_gridsearched():\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(5),\n",
    "             ),\n",
    "        Dense(8, activation='linear'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_gridsearched = create_model_gridsearched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "current-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1253 - mse: 0.1253 - mae: 0.2791 - val_loss: 0.0603 - val_mse: 0.0603 - val_mae: 0.1960\n",
      "Epoch 2/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0384 - mse: 0.0384 - mae: 0.1576 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1376\n",
      "Epoch 3/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0295 - mse: 0.0295 - mae: 0.1368 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1383\n",
      "Epoch 4/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0262 - mse: 0.0262 - mae: 0.1303 - val_loss: 0.0213 - val_mse: 0.0213 - val_mae: 0.1229\n",
      "Epoch 5/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0220 - mse: 0.0220 - mae: 0.1220 - val_loss: 0.0235 - val_mse: 0.0235 - val_mae: 0.1211\n",
      "Epoch 6/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.1113 - val_loss: 0.0319 - val_mse: 0.0319 - val_mae: 0.1432\n",
      "Epoch 7/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0182 - mse: 0.0182 - mae: 0.1062 - val_loss: 0.0243 - val_mse: 0.0243 - val_mae: 0.1284\n",
      "Epoch 8/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1244 - val_loss: 0.0257 - val_mse: 0.0257 - val_mae: 0.1316\n",
      "Epoch 9/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0175 - mse: 0.0175 - mae: 0.1039 - val_loss: 0.0283 - val_mse: 0.0283 - val_mae: 0.1475\n",
      "Epoch 10/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0977 - val_loss: 0.0136 - val_mse: 0.0136 - val_mae: 0.0999\n",
      "Epoch 11/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - mae: 0.0815 - val_loss: 0.0166 - val_mse: 0.0166 - val_mae: 0.1095\n",
      "Epoch 12/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0789 - val_loss: 0.0153 - val_mse: 0.0153 - val_mae: 0.1001\n",
      "Epoch 13/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0822 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0857\n",
      "Epoch 14/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0899 - val_loss: 0.0129 - val_mse: 0.0129 - val_mae: 0.0966\n",
      "Epoch 15/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0897 - val_loss: 0.0151 - val_mse: 0.0151 - val_mae: 0.1010\n",
      "Epoch 16/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0785 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0851\n",
      "Epoch 17/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0675 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0734\n",
      "Epoch 18/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0703 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0773\n",
      "Epoch 19/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0725 - val_loss: 0.0124 - val_mse: 0.0124 - val_mae: 0.0870\n",
      "Epoch 20/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0746 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0880\n",
      "Epoch 21/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0779 - val_loss: 0.0143 - val_mse: 0.0143 - val_mae: 0.0972\n",
      "Epoch 22/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0744 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0709\n",
      "Epoch 23/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0885 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0771\n",
      "Epoch 24/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0099 - mse: 0.0099 - mae: 0.0763 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0782\n",
      "Epoch 25/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0756 - val_loss: 0.0145 - val_mse: 0.0145 - val_mae: 0.0963\n",
      "Epoch 26/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - mae: 0.0788 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0890\n",
      "Epoch 27/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0732 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0797\n",
      "Epoch 28/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0657 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0899\n",
      "Epoch 29/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0691 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0834\n",
      "Epoch 30/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0734 - val_loss: 0.0124 - val_mse: 0.0124 - val_mae: 0.0904\n",
      "Epoch 31/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0659 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0826\n",
      "Epoch 32/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0659 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0795\n",
      "Epoch 33/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0667 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0776\n",
      "Epoch 34/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0735 - val_loss: 0.0111 - val_mse: 0.0111 - val_mae: 0.0839\n",
      "Epoch 35/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0733 - val_loss: 0.0240 - val_mse: 0.0240 - val_mae: 0.1250\n",
      "Epoch 36/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0972 - val_loss: 0.0169 - val_mse: 0.0169 - val_mae: 0.1026\n",
      "Epoch 37/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0964 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0838\n",
      "Epoch 38/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0698 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0874\n",
      "Epoch 39/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0938 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0701\n",
      "Epoch 40/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0765 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0755\n",
      "Epoch 41/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0658 - val_loss: 0.0161 - val_mse: 0.0161 - val_mae: 0.1041\n",
      "Epoch 42/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0771 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0776\n",
      "Epoch 43/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0868 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0758\n",
      "Epoch 44/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0757 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0672\n",
      "Epoch 45/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0734 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0754\n",
      "Epoch 46/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0726 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0733\n",
      "Epoch 47/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0661 - val_loss: 0.0111 - val_mse: 0.0111 - val_mae: 0.0862\n",
      "Epoch 48/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0699 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0731\n",
      "Epoch 49/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0740 - val_loss: 0.0145 - val_mse: 0.0145 - val_mae: 0.0984\n",
      "Epoch 50/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0808 - val_loss: 0.0213 - val_mse: 0.0213 - val_mae: 0.1181\n",
      "Epoch 51/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - mae: 0.0808 - val_loss: 0.0204 - val_mse: 0.0204 - val_mae: 0.1170\n",
      "Epoch 52/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0770 - val_loss: 0.0150 - val_mse: 0.0150 - val_mae: 0.1006\n",
      "Epoch 53/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0733 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0746\n",
      "Epoch 54/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0726 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0654\n",
      "Epoch 55/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0689 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0704\n",
      "Epoch 56/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0589 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0758\n",
      "Epoch 57/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0700 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0709\n",
      "Epoch 58/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0717 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0800\n",
      "Epoch 59/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0887 - val_loss: 0.0199 - val_mse: 0.0199 - val_mae: 0.1133\n",
      "Epoch 60/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.1063 - val_loss: 0.0203 - val_mse: 0.0203 - val_mae: 0.1144\n",
      "Epoch 61/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0977 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0783\n",
      "Epoch 62/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0747 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0687\n",
      "Epoch 63/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0784 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0837\n",
      "Epoch 64/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0787 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0810\n",
      "Epoch 65/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0732 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0759\n",
      "Epoch 66/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0710 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0749\n",
      "Epoch 67/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0849 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0690\n",
      "Epoch 68/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0941 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0765\n",
      "Epoch 69/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0906 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0838\n",
      "Epoch 70/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0758 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0580\n",
      "Epoch 71/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0791 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0714\n",
      "Epoch 72/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0617 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0610\n",
      "Epoch 73/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0674 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0595\n",
      "Epoch 74/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0637 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0666\n",
      "Epoch 75/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0626 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0821\n",
      "Epoch 76/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0760 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0693\n",
      "Epoch 77/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0665 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0838\n",
      "Epoch 78/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0708 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0767\n",
      "Epoch 79/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0734 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0800\n",
      "Epoch 80/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0664 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0719\n",
      "Epoch 81/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0631 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0604\n",
      "Epoch 82/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0742 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0827\n",
      "Epoch 83/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0705 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0783\n",
      "Epoch 84/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0724 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0686\n",
      "Epoch 85/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0652 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0667\n",
      "Epoch 86/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0939 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0851\n",
      "Epoch 87/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0770 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0736\n",
      "Epoch 88/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0657 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0649\n",
      "Epoch 89/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0632 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0739\n",
      "Epoch 90/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0647 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0678\n",
      "Epoch 91/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0644\n",
      "Epoch 92/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0667 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0722\n",
      "Epoch 93/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0632 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0595\n",
      "Epoch 94/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0697 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0635\n",
      "Epoch 95/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0601 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0633\n",
      "Epoch 96/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0644 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0607\n",
      "Epoch 97/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0682 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0681\n",
      "Epoch 98/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0644 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0568\n",
      "Epoch 99/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0632 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0841\n",
      "Epoch 100/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0703 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0716\n",
      "Epoch 101/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0672 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0632\n",
      "Epoch 102/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0737 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0757\n",
      "Epoch 103/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0796 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0875\n",
      "Epoch 104/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0673 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0747\n",
      "Epoch 105/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0673 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0598\n",
      "Epoch 106/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0654 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0624\n",
      "Epoch 107/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0581 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0597\n",
      "Epoch 108/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0605 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0673\n",
      "Epoch 109/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0672 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0606\n",
      "Epoch 110/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0571 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0640\n",
      "Epoch 111/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0679 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0850\n",
      "Epoch 112/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0779 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0834\n",
      "Epoch 113/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0711 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0699\n",
      "Epoch 114/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0655 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0722\n",
      "Epoch 115/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0765 - val_loss: 0.0110 - val_mse: 0.0110 - val_mae: 0.0843\n",
      "Epoch 116/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0732 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0672\n",
      "Epoch 117/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0678 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0562\n",
      "Epoch 118/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0738\n",
      "Epoch 119/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0649 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0645\n",
      "Epoch 120/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0664 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0698\n",
      "Epoch 121/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0652 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0707\n",
      "Epoch 122/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0632 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0713\n",
      "Epoch 123/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0661 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0604\n",
      "Epoch 124/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0581 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0596\n",
      "Epoch 125/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0648 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0617\n",
      "Epoch 126/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0672 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0613\n",
      "Epoch 127/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0619 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0633\n",
      "Epoch 128/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0687 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0664\n",
      "Epoch 129/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0642 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0723\n",
      "Epoch 130/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0619 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0646\n",
      "Epoch 131/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0625 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0543\n",
      "Epoch 132/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0588 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0601\n",
      "Epoch 133/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0609 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0688\n",
      "Epoch 134/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0585 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0576\n",
      "Epoch 135/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0577 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0769\n",
      "Epoch 136/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0560 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0552\n",
      "Epoch 137/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0672 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0712\n",
      "Epoch 138/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0671 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0635\n",
      "Epoch 139/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0645 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0789\n",
      "Epoch 140/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0740 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0518\n",
      "Epoch 141/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0746 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0605\n",
      "Epoch 142/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0709 - val_loss: 0.0132 - val_mse: 0.0132 - val_mae: 0.0942\n",
      "Epoch 143/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - mae: 0.0798 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0640\n",
      "Epoch 144/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0729 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0630\n",
      "Epoch 145/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0578 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0611\n",
      "Epoch 146/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0667 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0838\n",
      "Epoch 147/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0641 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0539\n",
      "Epoch 148/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0618 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0627\n",
      "Epoch 149/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0761 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0758\n",
      "Epoch 150/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0612 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0685 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0697\n",
      "Epoch 152/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0651 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0700\n",
      "Epoch 153/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0690 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0681\n",
      "Epoch 154/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0658 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0573\n",
      "Epoch 155/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0800 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0570\n",
      "Epoch 156/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0733 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0786\n",
      "Epoch 157/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0976 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0745\n",
      "Epoch 158/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - mae: 0.0808 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0610\n",
      "Epoch 159/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0587 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0735\n",
      "Epoch 160/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0695 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0676\n",
      "Epoch 161/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0730 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0802\n",
      "Epoch 162/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0802 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0727\n",
      "Epoch 163/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0724 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0637\n",
      "Epoch 164/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0693 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0697\n",
      "Epoch 165/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0613 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0604\n",
      "Epoch 166/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0756 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0838\n",
      "Epoch 167/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0684 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0710\n",
      "Epoch 168/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0623 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0588\n",
      "Epoch 169/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0587 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0595\n",
      "Epoch 170/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0705 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0675\n",
      "Epoch 171/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0602 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0816\n",
      "Epoch 172/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0753 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0660\n",
      "Epoch 173/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0641 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0513\n",
      "Epoch 174/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0638 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0641\n",
      "Epoch 175/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0629 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0640\n",
      "Epoch 176/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0711 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0627\n",
      "Epoch 177/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0695 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0734\n",
      "Epoch 178/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0681 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0648\n",
      "Epoch 179/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0716 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0781\n",
      "Epoch 180/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0654 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0569\n",
      "Epoch 181/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0582 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0665\n",
      "Epoch 182/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0600 - val_loss: 0.0110 - val_mse: 0.0110 - val_mae: 0.0857\n",
      "Epoch 183/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0709 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0789\n",
      "Epoch 184/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0678 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0808\n",
      "Epoch 185/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0727 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0747\n",
      "Epoch 186/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0697 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0632\n",
      "Epoch 187/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0808 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0728\n",
      "Epoch 188/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0597 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0656\n",
      "Epoch 189/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0630 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0665\n",
      "Epoch 190/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0605 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0653\n",
      "Epoch 191/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0655 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0815\n",
      "Epoch 192/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0807 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0702\n",
      "Epoch 193/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0699 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0586\n",
      "Epoch 194/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0643 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0629\n",
      "Epoch 195/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0605 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0677\n",
      "Epoch 196/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0552 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0636\n",
      "Epoch 197/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0593 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0576\n",
      "Epoch 198/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0603 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0636\n",
      "Epoch 199/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0700 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0721\n",
      "Epoch 200/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0662 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0786 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0811\n",
      "Epoch 202/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0678 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0712\n",
      "Epoch 203/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0631 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0719\n",
      "Epoch 204/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0609 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0682\n",
      "Epoch 205/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0662 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0692\n",
      "Epoch 206/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0583 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0555\n",
      "Epoch 207/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0645 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0638\n",
      "Epoch 208/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0601 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0584\n",
      "Epoch 209/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0622 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0842\n",
      "Epoch 210/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0655 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0681\n",
      "Epoch 211/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0631 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0703\n",
      "Epoch 212/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0564 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0701\n",
      "Epoch 213/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0656 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0603\n",
      "Epoch 214/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0595 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0544\n",
      "Epoch 215/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0622 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0530\n",
      "Epoch 216/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0654 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0620\n",
      "Epoch 217/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0750 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0867\n",
      "Epoch 218/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0695 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0672\n",
      "Epoch 219/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0663 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0651\n",
      "Epoch 220/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0629 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0745\n",
      "Epoch 221/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0653 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0619\n",
      "Epoch 222/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0591 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0608\n",
      "Epoch 223/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0595 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0592\n",
      "Epoch 224/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0650 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0525\n",
      "Epoch 225/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0634 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0733\n",
      "Epoch 226/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0958 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0771\n",
      "Epoch 227/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0712 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0729\n",
      "Epoch 228/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0664 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0719\n",
      "Epoch 229/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0640 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0763\n",
      "Epoch 230/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0583 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0646\n",
      "Epoch 231/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0615 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0562\n",
      "Epoch 232/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0624 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0540\n",
      "Epoch 233/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0602 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0543\n",
      "Epoch 234/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0601 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0558\n",
      "Epoch 235/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0609 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0682\n",
      "Epoch 236/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0633 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0647\n",
      "Epoch 237/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0585 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0513\n",
      "Epoch 238/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0627 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0516\n",
      "Epoch 239/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0662 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0601\n",
      "Epoch 240/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0553 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0647\n",
      "Epoch 241/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0640 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0577\n",
      "Epoch 242/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0726 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0824\n",
      "Epoch 243/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0594 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0687\n",
      "Epoch 244/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0596 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0621\n",
      "Epoch 245/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0603 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0859\n",
      "Epoch 246/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0740 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0816\n",
      "Epoch 247/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0665 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0743\n",
      "Epoch 248/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0745\n",
      "Epoch 249/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0638 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0722\n",
      "Epoch 250/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0614 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0720 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0786\n",
      "Epoch 252/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0652 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0581\n",
      "Epoch 253/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0651 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0580\n",
      "Epoch 254/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0598 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0662\n",
      "Epoch 255/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0764 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0528\n",
      "Epoch 256/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0689 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0505\n",
      "Epoch 257/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0628 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0551\n",
      "Epoch 258/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0700 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0632\n",
      "Epoch 259/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0597 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0763\n",
      "Epoch 260/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0617 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0683\n",
      "Epoch 261/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0601 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0613\n",
      "Epoch 262/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0598 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0619\n",
      "Epoch 263/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0564 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0635\n",
      "Epoch 264/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0633 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 265/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0571 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0623\n",
      "Epoch 266/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0609 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0640\n",
      "Epoch 267/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0740 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0585\n",
      "Epoch 268/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0557 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0577\n",
      "Epoch 269/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0653 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0696\n",
      "Epoch 270/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0745 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0696\n",
      "Epoch 271/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0714 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0581\n",
      "Epoch 272/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0657 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0588\n",
      "Epoch 273/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0681 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0564\n",
      "Epoch 274/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0575 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0540\n",
      "Epoch 275/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0613 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0583\n",
      "Epoch 276/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0612 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0551\n",
      "Epoch 277/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0592 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0522\n",
      "Epoch 278/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0656 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0589\n",
      "Epoch 279/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0639 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0570\n",
      "Epoch 280/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0599 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0538\n",
      "Epoch 281/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0604 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0525\n",
      "Epoch 282/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0701 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0567\n",
      "Epoch 283/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0675 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0557\n",
      "Epoch 284/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0722 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0761\n",
      "Epoch 285/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - mae: 0.0860 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0734\n",
      "Epoch 286/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0691 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0674\n",
      "Epoch 287/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0635 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0626\n",
      "Epoch 288/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0646 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0785\n",
      "Epoch 289/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0603 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0590\n",
      "Epoch 290/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0683 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0668\n",
      "Epoch 291/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0739 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0845\n",
      "Epoch 292/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0677 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0653\n",
      "Epoch 293/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0615 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0654\n",
      "Epoch 294/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0620 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0666\n",
      "Epoch 295/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0693 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0630\n",
      "Epoch 296/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0549 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0600\n",
      "Epoch 297/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0629 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0660\n",
      "Epoch 298/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0647 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0544\n",
      "Epoch 299/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0585 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0788\n",
      "Epoch 300/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0634 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0616 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0568\n",
      "Epoch 302/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0673 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0605\n",
      "Epoch 303/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0587 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0542\n",
      "Epoch 304/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0670 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0631\n",
      "Epoch 305/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0629 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0605\n",
      "Epoch 306/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0619 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0661\n",
      "Epoch 307/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0599 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0700\n",
      "Epoch 308/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0722 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0517\n",
      "Epoch 309/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0583 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0639\n",
      "Epoch 310/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0607 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0633\n",
      "Epoch 311/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0591 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0673\n",
      "Epoch 312/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0608 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0753\n",
      "Epoch 313/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0671 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0620\n",
      "Epoch 314/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0572 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0628\n",
      "Epoch 315/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0557 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0564\n",
      "Epoch 316/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0578 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0660\n",
      "Epoch 317/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0684 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0650\n",
      "Epoch 318/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0574 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0625\n",
      "Epoch 319/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0584 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0611\n",
      "Epoch 320/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0636 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0641\n",
      "Epoch 321/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0627 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0753\n",
      "Epoch 322/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0644 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0612\n",
      "Epoch 323/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0583 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0538\n",
      "Epoch 324/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0633 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0768\n",
      "Epoch 325/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0647 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0789\n",
      "Epoch 326/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0633 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0524\n",
      "Epoch 327/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0632 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0534\n",
      "Epoch 328/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0700 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0819\n",
      "Epoch 329/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0839 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0733\n",
      "Epoch 330/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0727 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0710\n",
      "Epoch 331/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0830 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0557\n",
      "Epoch 332/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0677 - val_loss: 0.0135 - val_mse: 0.0135 - val_mae: 0.0930\n",
      "Epoch 333/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0736 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0671\n",
      "Epoch 334/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0628 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0604\n",
      "Epoch 335/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0622 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0870\n",
      "Epoch 336/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0684 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0770\n",
      "Epoch 337/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0689 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0604\n",
      "Epoch 338/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0611 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0664\n",
      "Epoch 339/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0609 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0557\n",
      "Epoch 340/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0674 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0625\n",
      "Epoch 341/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0594 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0527\n",
      "Epoch 342/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0622 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0686\n",
      "Epoch 343/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0655 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0632\n",
      "Epoch 344/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0587 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0568\n",
      "Epoch 345/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0600 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0561\n",
      "Epoch 346/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0660 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0614\n",
      "Epoch 347/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0606 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0624\n",
      "Epoch 348/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0638 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0756\n",
      "Epoch 349/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0645 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0843\n",
      "Epoch 350/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0622 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0634 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0865\n",
      "Epoch 352/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0618 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0830\n",
      "Epoch 353/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0684 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0607\n",
      "Epoch 354/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0628 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0659\n",
      "Epoch 355/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0639 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0597\n",
      "Epoch 356/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0624 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0565\n",
      "Epoch 357/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0643 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0581\n",
      "Epoch 358/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0637 - val_loss: 0.0108 - val_mse: 0.0108 - val_mae: 0.0828\n",
      "Epoch 359/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0765 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0653\n",
      "Epoch 360/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0655 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0572\n",
      "Epoch 361/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0599 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0625\n",
      "Epoch 362/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0633 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0561\n",
      "Epoch 363/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0536 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0616\n",
      "Epoch 364/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0562 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0520\n",
      "Epoch 365/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0579 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0542\n",
      "Epoch 366/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0586 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0583\n",
      "Epoch 367/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0677 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0609\n",
      "Epoch 368/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0589 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0693\n",
      "Epoch 369/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0640 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0708\n",
      "Epoch 370/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0565 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0537\n",
      "Epoch 371/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0595 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0537\n",
      "Epoch 372/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0594 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0741\n",
      "Epoch 373/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0610 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0761\n",
      "Epoch 374/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0607 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0740\n",
      "Epoch 375/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0660\n",
      "Epoch 376/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0665 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0703\n",
      "Epoch 377/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0711 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0733\n",
      "Epoch 378/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0627 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0699\n",
      "Epoch 379/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0598 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0547\n",
      "Epoch 380/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0642 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0598\n",
      "Epoch 381/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0606 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0529\n",
      "Epoch 382/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0631 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0591\n",
      "Epoch 383/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0579 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0626\n",
      "Epoch 384/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0607 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0581\n",
      "Epoch 385/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0619 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0569\n",
      "Epoch 386/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0629 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0700\n",
      "Epoch 387/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0618 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0582\n",
      "Epoch 388/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0597 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 389/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0616 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0634\n",
      "Epoch 390/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0600 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0617\n",
      "Epoch 391/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0582 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0532\n",
      "Epoch 392/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0614 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0606\n",
      "Epoch 393/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0610 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0579\n",
      "Epoch 394/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0599 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0635\n",
      "Epoch 395/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0634 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0794\n",
      "Epoch 396/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0630 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0839\n",
      "Epoch 397/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0601 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0682\n",
      "Epoch 398/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0581 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0622\n",
      "Epoch 399/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0574 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0601\n",
      "Epoch 400/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0592 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0679 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0643\n",
      "Epoch 402/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0650 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0570\n",
      "Epoch 403/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0622 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0667\n",
      "Epoch 404/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0620 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0531\n",
      "Epoch 405/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0618 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0551\n",
      "Epoch 406/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0592 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0553\n",
      "Epoch 407/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0627 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0721\n",
      "Epoch 408/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0602 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0582\n",
      "Epoch 409/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0617 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0585\n",
      "Epoch 410/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0615 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0664\n",
      "Epoch 411/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0662 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0593\n",
      "Epoch 412/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0650 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0555\n",
      "Epoch 413/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0676 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0679\n",
      "Epoch 414/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0674 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0688\n",
      "Epoch 415/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0667 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0813\n",
      "Epoch 416/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0601 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0669\n",
      "Epoch 417/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0587 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0661\n",
      "Epoch 418/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0581 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0557\n",
      "Epoch 419/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0586 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0708\n",
      "Epoch 420/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0666 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0579\n",
      "Epoch 421/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0573 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0569\n",
      "Epoch 422/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0558 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0544\n",
      "Epoch 423/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0611 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0661\n",
      "Epoch 424/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0618 - val_loss: 0.0111 - val_mse: 0.0111 - val_mae: 0.0845\n",
      "Epoch 425/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0635 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0611\n",
      "Epoch 426/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0672 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0703\n",
      "Epoch 427/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0594 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0643\n",
      "Epoch 428/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0632 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0618\n",
      "Epoch 429/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0670 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0569\n",
      "Epoch 430/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0573 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0569\n",
      "Epoch 431/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0573 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0600\n",
      "Epoch 432/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0554 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0636\n",
      "Epoch 433/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0558 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0621\n",
      "Epoch 434/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0605 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0619\n",
      "Epoch 435/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0597 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0586\n",
      "Epoch 436/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0574 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 437/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0583 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0635\n",
      "Epoch 438/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0669 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0872\n",
      "Epoch 439/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0683 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0783\n",
      "Epoch 440/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0631 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0681\n",
      "Epoch 441/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0591 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0544\n",
      "Epoch 442/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0577 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0613\n",
      "Epoch 443/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0617 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0580\n",
      "Epoch 444/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0665 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0654\n",
      "Epoch 445/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0722 - val_loss: 0.0143 - val_mse: 0.0143 - val_mae: 0.0965\n",
      "Epoch 446/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0711 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0575\n",
      "Epoch 447/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0688 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0613\n",
      "Epoch 448/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0602 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0653\n",
      "Epoch 449/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0593 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0586\n",
      "Epoch 450/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0600 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0611 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0550\n",
      "Epoch 452/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0608 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0857\n",
      "Epoch 453/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0672 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0725\n",
      "Epoch 454/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0639 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0668\n",
      "Epoch 455/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0620 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0559\n",
      "Epoch 456/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0663 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0691\n",
      "Epoch 457/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0600 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0710\n",
      "Epoch 458/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0621 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0596\n",
      "Epoch 459/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0618 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0605\n",
      "Epoch 460/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0594 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0640\n",
      "Epoch 461/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0576 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0644\n",
      "Epoch 462/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0587 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0596\n",
      "Epoch 463/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0599 - val_loss: 0.0110 - val_mse: 0.0110 - val_mae: 0.0879\n",
      "Epoch 464/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0665 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0801\n",
      "Epoch 465/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0688 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0619\n",
      "Epoch 466/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0617 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0561\n",
      "Epoch 467/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0607 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0685\n",
      "Epoch 468/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0627 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0556\n",
      "Epoch 469/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0604 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0623\n",
      "Epoch 470/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0642 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0647\n",
      "Epoch 471/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0595 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0632\n",
      "Epoch 472/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0599 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0596\n",
      "Epoch 473/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0624 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0597\n",
      "Epoch 474/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0593 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0653\n",
      "Epoch 475/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0629 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0488\n",
      "Epoch 476/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0622 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0513\n",
      "Epoch 477/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0573 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 478/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0595 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0611\n",
      "Epoch 479/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0624 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0595\n",
      "Epoch 480/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0653 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0611\n",
      "Epoch 481/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0600 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0522\n",
      "Epoch 482/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0602 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0531\n",
      "Epoch 483/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0599 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0633\n",
      "Epoch 484/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0555 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0541\n",
      "Epoch 485/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0548 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0562\n",
      "Epoch 486/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0587 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0610\n",
      "Epoch 487/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0566 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0586\n",
      "Epoch 488/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0573 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0659\n",
      "Epoch 489/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0642 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0625\n",
      "Epoch 490/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0572 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0672\n",
      "Epoch 491/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0648 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0626\n",
      "Epoch 492/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0540 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0536\n",
      "Epoch 493/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0587 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0817\n",
      "Epoch 494/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0633 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0507\n",
      "Epoch 495/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0612 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0601\n",
      "Epoch 496/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0590 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0585\n",
      "Epoch 497/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0589 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0742\n",
      "Epoch 498/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0658 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0629\n",
      "Epoch 499/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0620 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0798\n",
      "Epoch 500/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0623 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0595 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0657\n",
      "Epoch 502/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0659 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0575\n",
      "Epoch 503/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0570 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0610\n",
      "Epoch 504/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0636 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0603\n",
      "Epoch 505/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0602 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0595\n",
      "Epoch 506/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0653 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0574\n",
      "Epoch 507/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0582 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0600\n",
      "Epoch 508/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0606 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0621\n",
      "Epoch 509/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0640 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0609\n",
      "Epoch 510/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0614 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0584\n",
      "Epoch 511/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0527\n",
      "Epoch 512/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0584 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0626\n",
      "Epoch 513/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0618 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0673\n",
      "Epoch 514/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0584 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0551\n",
      "Epoch 515/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0591 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0584\n",
      "Epoch 516/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0578 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0555\n",
      "Epoch 517/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0691 - val_loss: 0.0129 - val_mse: 0.0129 - val_mae: 0.0909\n",
      "Epoch 518/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - mae: 0.0835 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0808\n",
      "Epoch 519/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0801 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0618\n",
      "Epoch 520/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0759 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0866\n",
      "Epoch 521/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0711 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0575\n",
      "Epoch 522/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0621 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0678\n",
      "Epoch 523/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0661 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0860\n",
      "Epoch 524/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0663 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0809\n",
      "Epoch 525/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0631 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0582\n",
      "Epoch 526/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0625 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0547\n",
      "Epoch 527/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0621 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0559\n",
      "Epoch 528/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0655 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0727\n",
      "Epoch 529/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0623 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0650\n",
      "Epoch 530/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0561 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0524\n",
      "Epoch 531/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0578 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0594\n",
      "Epoch 532/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0549 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0663\n",
      "Epoch 533/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0622 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0652\n",
      "Epoch 534/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0638 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0559\n",
      "Epoch 535/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0582 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0545\n",
      "Epoch 536/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0546 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0661\n",
      "Epoch 537/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0601 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0605\n",
      "Epoch 538/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0612 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0526\n",
      "Epoch 539/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0689 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0603\n",
      "Epoch 540/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0633 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0634\n",
      "Epoch 541/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0563 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0525\n",
      "Epoch 542/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0608 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0503\n",
      "Epoch 543/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0610 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0549\n",
      "Epoch 544/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0624 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0531\n",
      "Epoch 545/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0591 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0501\n",
      "Epoch 546/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0642 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0574\n",
      "Epoch 547/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0666 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0757\n",
      "Epoch 548/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0627 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0501\n",
      "Epoch 549/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0624 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0674\n",
      "Epoch 550/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0743 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0601 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0545\n",
      "Epoch 552/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0602 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0503\n",
      "Epoch 553/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0615 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0581\n",
      "Epoch 554/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0561 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0606\n",
      "Epoch 555/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0569 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0635\n",
      "Epoch 556/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0609 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0575\n",
      "Epoch 557/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0609 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0632\n",
      "Epoch 558/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0660 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0725\n",
      "Epoch 559/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0665 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0621\n",
      "Epoch 560/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0583 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0575\n",
      "Epoch 561/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0689 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0557\n",
      "Epoch 562/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0643 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0717\n",
      "Epoch 563/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0653 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0554\n",
      "Epoch 564/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0624 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0541\n",
      "Epoch 565/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0614 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0781\n",
      "Epoch 566/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0728 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0720\n",
      "Epoch 567/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0610 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0550\n",
      "Epoch 568/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0638 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0648\n",
      "Epoch 569/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0631 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0672\n",
      "Epoch 570/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0603 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0588\n",
      "Epoch 571/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0625 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0580\n",
      "Epoch 572/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0546 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0661\n",
      "Epoch 573/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0590 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0673\n",
      "Epoch 574/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0541 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0512\n",
      "Epoch 575/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0655 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0702\n",
      "Epoch 576/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0636 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0583\n",
      "Epoch 577/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0609 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0731\n",
      "Epoch 578/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0662 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0878\n",
      "Epoch 579/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0633 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0695\n",
      "Epoch 580/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0555 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0635\n",
      "Epoch 581/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0616 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0541\n",
      "Epoch 582/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0591 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0582\n",
      "Epoch 583/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0586 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0617\n",
      "Epoch 584/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0610 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0633\n",
      "Epoch 585/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0634 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0771\n",
      "Epoch 586/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0576 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0596\n",
      "Epoch 587/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0510 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0581\n",
      "Epoch 588/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0623 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0598\n",
      "Epoch 589/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0631 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0542\n",
      "Epoch 590/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0607 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0603\n",
      "Epoch 591/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0553 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0619\n",
      "Epoch 592/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0556 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0556\n",
      "Epoch 593/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0622 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0628\n",
      "Epoch 594/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0611 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0569\n",
      "Epoch 595/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0562 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0663\n",
      "Epoch 596/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0637 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0818\n",
      "Epoch 597/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0721 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0662\n",
      "Epoch 598/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0752 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0900\n",
      "Epoch 599/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0737 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0723\n",
      "Epoch 600/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0670 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0676 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0691\n",
      "Epoch 602/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0737 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0770\n",
      "Epoch 603/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0674 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0797\n",
      "Epoch 604/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0683 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0648\n",
      "Epoch 605/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0668 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0542\n",
      "Epoch 606/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0557 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0510\n",
      "Epoch 607/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0613 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0545\n",
      "Epoch 608/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0582 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0623\n",
      "Epoch 609/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0615 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0665\n",
      "Epoch 610/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0637 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0588\n",
      "Epoch 611/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0573 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0593\n",
      "Epoch 612/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0581 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0596\n",
      "Epoch 613/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0586 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0563\n",
      "Epoch 614/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0629 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0639\n",
      "Epoch 615/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0674 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0587\n",
      "Epoch 616/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0641 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0533\n",
      "Epoch 617/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0706 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0580\n",
      "Epoch 618/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0580 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0684\n",
      "Epoch 619/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0637 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0684\n",
      "Epoch 620/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0613 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0541\n",
      "Epoch 621/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0635 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0619\n",
      "Epoch 622/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0591 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0556\n",
      "Epoch 623/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0548 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0635\n",
      "Epoch 624/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0627 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0660\n",
      "Epoch 625/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0564 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0613\n",
      "Epoch 626/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0571 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0525\n",
      "Epoch 627/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0518 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0578\n",
      "Epoch 628/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0640 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0602\n",
      "Epoch 629/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0632 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0784\n",
      "Epoch 630/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0656 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0577\n",
      "Epoch 631/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0564 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0564\n",
      "Epoch 632/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0565 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0535\n",
      "Epoch 633/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0565 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0560\n",
      "Epoch 634/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0571 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0646\n",
      "Epoch 635/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0599 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0681\n",
      "Epoch 636/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0598 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0735\n",
      "Epoch 637/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0581 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0666\n",
      "Epoch 638/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0611\n",
      "Epoch 639/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0560 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0603\n",
      "Epoch 640/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0603 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0613\n",
      "Epoch 641/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0562 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0543\n",
      "Epoch 642/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0564 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0576\n",
      "Epoch 643/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0589 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0661\n",
      "Epoch 644/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0592 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0573\n",
      "Epoch 645/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0614 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0544\n",
      "Epoch 646/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0629 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0608\n",
      "Epoch 647/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0591 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0694\n",
      "Epoch 648/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0586 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0684\n",
      "Epoch 649/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0575 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0604\n",
      "Epoch 650/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0602 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 651/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0569 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0659\n",
      "Epoch 652/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0618 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0690\n",
      "Epoch 653/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0622 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0791\n",
      "Epoch 654/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0674 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0637\n",
      "Epoch 655/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0582 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0589\n",
      "Epoch 656/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0571 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0484\n",
      "Epoch 657/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0630 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0589\n",
      "Epoch 658/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0576 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0707\n",
      "Epoch 659/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0628 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0610\n",
      "Epoch 660/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0598 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0611\n",
      "Epoch 661/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0616 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0680\n",
      "Epoch 662/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0653 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0671\n",
      "Epoch 663/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0613 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0613\n",
      "Epoch 664/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0635 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0654\n",
      "Epoch 665/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0620 - val_loss: 0.0085 - val_mse: 0.0085 - val_mae: 0.0749\n",
      "Epoch 666/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0642 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0781\n",
      "Epoch 667/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0553 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0656\n",
      "Epoch 668/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0602 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0620\n",
      "Epoch 669/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0632 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0540\n",
      "Epoch 670/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0581 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0589\n",
      "Epoch 671/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0616 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0584\n",
      "Epoch 672/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0651 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0617\n",
      "Epoch 673/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0590 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0629\n",
      "Epoch 674/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0608 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0576\n",
      "Epoch 675/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0599 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0558\n",
      "Epoch 676/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0553 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0641\n",
      "Epoch 677/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0656 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0596\n",
      "Epoch 678/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0560 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0568\n",
      "Epoch 679/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0592 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0605\n",
      "Epoch 680/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0623 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0612\n",
      "Epoch 681/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0624 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0629\n",
      "Epoch 682/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0603 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0627\n",
      "Epoch 683/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0643 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0604\n",
      "Epoch 684/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0579 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0786\n",
      "Epoch 685/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0672 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0514\n",
      "Epoch 686/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0615 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0542\n",
      "Epoch 687/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0619 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0580\n",
      "Epoch 688/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0632 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0556\n",
      "Epoch 689/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0584 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0592\n",
      "Epoch 690/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0573 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0617\n",
      "Epoch 691/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0692 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0631\n",
      "Epoch 692/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0646 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0566\n",
      "Epoch 693/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0632 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0660\n",
      "Epoch 694/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0669 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0591\n",
      "Epoch 695/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0641 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0571\n",
      "Epoch 696/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0588 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0714\n",
      "Epoch 697/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0611 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0687\n",
      "Epoch 698/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0639 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0651\n",
      "Epoch 699/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0626 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0574\n",
      "Epoch 700/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0629 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0522\n"
     ]
    }
   ],
   "source": [
    "history_model_gridsearched = model_gridsearched.fit(X_test_scaled, y_test_scaled, epochs=700, batch_size=10,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "offshore-community",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(history.history.keys())\n",
    "plt.plot(history_model_gridsearched.history['loss'])\n",
    "plt.plot(history_model_gridsearched.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "transsexual-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = X_test.values.astype(int)\n",
    "y_test_np = y_test.values.astype(int)\n",
    "X_test_np = convert_to_tensor(X_test_np)\n",
    "y_test_np = convert_to_tensor(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "legendary-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 5.7227 - mse: 5.7227 - mae: 2.2019\n",
      "Mean Squared Error : 5.72268\n",
      "Mean Absolute Error : 2.20194\n"
     ]
    }
   ],
   "source": [
    "loss, mean_sq_e, mean_abs_e = model_gridsearched.evaluate(X_test_np, y_test_np)\n",
    "\n",
    "print(\"Mean Squared Error : {:.5f}\".format(mean_sq_e))\n",
    "print(\"Mean Absolute Error : {:.5f}\".format(mean_abs_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-sympathy",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eleven-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted G3's score is 14.02\n"
     ]
    }
   ],
   "source": [
    "df2 = features.iloc[2] # Test with the 14th Student \n",
    "Xnew = np.array([df2])\n",
    "Xnew.reshape(-1, 1)\n",
    "\n",
    "Xnew= scaler.transform(Xnew)\n",
    "ynew= model_gridsearched.predict(Xnew)\n",
    "\n",
    "# #invert normalize\n",
    "ynew = scaler.inverse_transform(ynew) \n",
    "Xnew = scaler.inverse_transform(Xnew)\n",
    "\n",
    "#print(\"X=%s, Predicted=%s\" % (Xnew[0], ynew[0]))\n",
    "print(\"Predicted G3's score is {:.2f}\".format(float(ynew)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "foreign-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_gridsearched.predict(X_test_scaled)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = np.round(predictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "governing-wyoming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.17\n",
      "Difference : 0.170\n",
      "1\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.09\n",
      "Difference : 0.090\n",
      "2\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.32\n",
      "Difference : 0.680\n",
      "3\n",
      "Real Values : 19.0\n",
      "Predicted G3's score is 18.35\n",
      "Difference : 0.650\n",
      "4\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.53\n",
      "Difference : 0.530\n",
      "5\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 16.14\n",
      "Difference : 1.140\n",
      "6\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "7\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.17\n",
      "Difference : 1.170\n",
      "8\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.05\n",
      "Difference : 1.050\n",
      "9\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.96\n",
      "Difference : 0.040\n",
      "10\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.82\n",
      "Difference : 0.180\n",
      "11\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.47\n",
      "Difference : 0.470\n",
      "12\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.87\n",
      "Difference : 0.870\n",
      "13\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.41\n",
      "Difference : 0.590\n",
      "14\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.32\n",
      "Difference : 0.320\n",
      "15\n",
      "Real Values : 6.0\n",
      "Predicted G3's score is 7.78\n",
      "Difference : 1.780\n",
      "16\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.01\n",
      "Difference : 0.990\n",
      "17\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.02\n",
      "Difference : 0.020\n",
      "18\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 15.25\n",
      "Difference : 1.250\n",
      "19\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.61\n",
      "Difference : 0.390\n",
      "20\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.69\n",
      "Difference : 0.690\n",
      "21\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.54\n",
      "Difference : 0.460\n",
      "22\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.70\n",
      "Difference : 0.300\n",
      "23\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.44\n",
      "Difference : 0.560\n",
      "24\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.49\n",
      "Difference : 0.490\n",
      "25\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.59\n",
      "Difference : 0.410\n",
      "26\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.86\n",
      "Difference : 0.140\n",
      "27\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 11.93\n",
      "Difference : 1.070\n",
      "28\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.70\n",
      "Difference : 0.300\n",
      "29\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.44\n",
      "Difference : 0.440\n",
      "30\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.92\n",
      "Difference : 0.920\n",
      "31\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.38\n",
      "Difference : 0.620\n",
      "32\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 9.16\n",
      "Difference : 1.160\n",
      "33\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 13.07\n",
      "Difference : 1.930\n",
      "34\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.68\n",
      "Difference : 0.320\n",
      "35\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.18\n",
      "Difference : 0.820\n",
      "36\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.93\n",
      "Difference : 1.070\n",
      "37\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.59\n",
      "Difference : 0.590\n",
      "38\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "39\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.25\n",
      "Difference : 0.250\n",
      "40\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.19\n",
      "Difference : 0.190\n",
      "41\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 17.94\n",
      "Difference : 0.940\n",
      "42\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.96\n",
      "Difference : 0.040\n",
      "43\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.90\n",
      "Difference : 0.900\n",
      "44\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 11.17\n",
      "Difference : 1.170\n",
      "45\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.92\n",
      "Difference : 0.920\n",
      "46\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.56\n",
      "Difference : 1.440\n",
      "47\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.82\n",
      "Difference : 0.180\n",
      "48\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.24\n",
      "Difference : 0.760\n",
      "49\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.52\n",
      "Difference : 0.480\n",
      "50\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 17.41\n",
      "Difference : 0.410\n",
      "51\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.08\n",
      "Difference : 0.080\n",
      "52\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.28\n",
      "Difference : 0.280\n",
      "53\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.32\n",
      "Difference : 0.320\n",
      "54\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.64\n",
      "Difference : 0.640\n",
      "55\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.69\n",
      "Difference : 0.310\n",
      "56\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 18.07\n",
      "Difference : 1.070\n",
      "57\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.57\n",
      "Difference : 0.570\n",
      "58\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.40\n",
      "Difference : 0.400\n",
      "59\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.83\n",
      "Difference : 0.170\n",
      "60\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.33\n",
      "Difference : 0.670\n",
      "61\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 8.91\n",
      "Difference : 1.090\n",
      "62\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.07\n",
      "Difference : 0.930\n",
      "63\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.63\n",
      "Difference : 0.630\n",
      "64\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.87\n",
      "Difference : 0.870\n",
      "65\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.630\n",
      "66\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.76\n",
      "Difference : 0.240\n",
      "67\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.55\n",
      "Difference : 0.450\n",
      "68\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.73\n",
      "Difference : 0.270\n",
      "69\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.71\n",
      "Difference : 0.290\n",
      "70\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.81\n",
      "Difference : 0.810\n",
      "71\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.04\n",
      "Difference : 0.040\n",
      "72\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.05\n",
      "Difference : 0.050\n",
      "73\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.64\n",
      "Difference : 0.360\n",
      "74\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 14.69\n",
      "Difference : 1.310\n",
      "75\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 18.00\n",
      "Difference : 0.000\n",
      "76\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.63\n",
      "Difference : 0.370\n",
      "77\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.59\n",
      "Difference : 0.590\n",
      "78\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.40\n",
      "Difference : 0.600\n",
      "79\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.68\n",
      "Difference : 0.320\n",
      "80\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.370\n",
      "81\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 17.12\n",
      "Difference : 0.880\n",
      "82\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.70\n",
      "Difference : 0.700\n",
      "83\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 12.22\n",
      "Difference : 3.780\n",
      "84\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.44\n",
      "Difference : 0.560\n",
      "85\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.23\n",
      "Difference : 0.230\n",
      "86\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 18.24\n",
      "Difference : 0.240\n",
      "87\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.88\n",
      "Difference : 0.120\n",
      "88\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 11.51\n",
      "Difference : 1.510\n",
      "89\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.53\n",
      "Difference : 0.530\n",
      "90\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 0.310\n",
      "91\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.79\n",
      "Difference : 0.790\n",
      "92\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 17.07\n",
      "Difference : 1.070\n",
      "93\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.71\n",
      "Difference : 0.290\n",
      "94\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 13.58\n",
      "Difference : 1.580\n",
      "95\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.49\n",
      "Difference : 0.490\n",
      "96\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.59\n",
      "Difference : 0.410\n",
      "97\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.79\n",
      "Difference : 0.790\n",
      "98\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.26\n",
      "Difference : 0.740\n",
      "99\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.94\n",
      "Difference : 1.060\n",
      "100\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 9.12\n",
      "Difference : 1.120\n",
      "101\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.58\n",
      "Difference : 0.580\n",
      "102\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.63\n",
      "Difference : 0.630\n",
      "103\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.67\n",
      "Difference : 0.330\n",
      "104\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.12\n",
      "Difference : 0.120\n",
      "105\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 1.310\n",
      "106\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.63\n",
      "Difference : 0.630\n",
      "107\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.26\n",
      "Difference : 1.260\n",
      "108\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.81\n",
      "Difference : 0.810\n",
      "109\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.15\n",
      "Difference : 0.850\n",
      "110\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.08\n",
      "Difference : 0.920\n",
      "111\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.93\n",
      "Difference : 0.930\n",
      "112\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.43\n",
      "Difference : 0.570\n",
      "113\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.11\n",
      "Difference : 0.110\n",
      "114\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 14.85\n",
      "Difference : 1.150\n",
      "115\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.33\n",
      "Difference : 0.330\n",
      "116\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.97\n",
      "Difference : 0.970\n",
      "117\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.76\n",
      "Difference : 0.760\n",
      "118\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.45\n",
      "Difference : 0.450\n",
      "119\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.58\n",
      "Difference : 0.580\n",
      "120\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.65\n",
      "Difference : 0.650\n",
      "121\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.83\n",
      "Difference : 0.830\n",
      "122\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 14.56\n",
      "Difference : 2.440\n",
      "123\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.45\n",
      "Difference : 0.450\n",
      "124\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.63\n",
      "Difference : 0.630\n",
      "125\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.71\n",
      "Difference : 0.710\n",
      "126\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.06\n",
      "Difference : 0.060\n",
      "127\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.77\n",
      "Difference : 0.230\n",
      "128\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 11.91\n",
      "Difference : 1.090\n",
      "129\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.63\n",
      "Difference : 0.370\n",
      "130\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.25\n",
      "Difference : 0.750\n",
      "131\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.13\n",
      "Difference : 0.130\n",
      "132\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.78\n",
      "Difference : 0.780\n",
      "133\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.55\n",
      "Difference : 0.550\n",
      "134\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.18\n",
      "Difference : 0.180\n",
      "135\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.64\n",
      "Difference : 0.360\n",
      "136\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.63\n",
      "Difference : 0.630\n",
      "137\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.60\n",
      "Difference : 0.400\n",
      "138\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.28\n",
      "Difference : 1.720\n",
      "139\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.54\n",
      "Difference : 0.540\n",
      "140\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.90\n",
      "Difference : 0.900\n",
      "141\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.00\n",
      "Difference : 1.000\n",
      "142\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.36\n",
      "Difference : 0.360\n",
      "143\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.26\n",
      "Difference : 0.740\n",
      "144\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.75\n",
      "Difference : 0.250\n",
      "145\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.51\n",
      "Difference : 0.510\n",
      "146\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.53\n",
      "Difference : 0.530\n",
      "147\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.48\n",
      "Difference : 1.480\n",
      "148\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.47\n",
      "Difference : 0.530\n",
      "149\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.45\n",
      "Difference : 0.550\n",
      "150\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.27\n",
      "Difference : 0.270\n",
      "151\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.43\n",
      "Difference : 0.430\n",
      "152\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.07\n",
      "Difference : 0.930\n",
      "153\n",
      "Real Values : 7.0\n",
      "Predicted G3's score is 8.76\n",
      "Difference : 1.760\n",
      "154\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 16.71\n",
      "Difference : 1.290\n",
      "155\n",
      "Real Values : 6.0\n",
      "Predicted G3's score is 6.69\n",
      "Difference : 0.690\n",
      "156\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.48\n",
      "Difference : 1.520\n",
      "157\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.22\n",
      "Difference : 0.780\n",
      "158\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.34\n",
      "Difference : 0.660\n",
      "159\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.22\n",
      "Difference : 0.220\n",
      "160\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.42\n",
      "Difference : 0.580\n",
      "161\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 16.46\n",
      "Difference : 1.540\n",
      "162\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.12\n",
      "Difference : 0.880\n",
      "163\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.87\n",
      "Difference : 0.870\n",
      "164\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 0.690\n",
      "165\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.18\n",
      "Difference : 0.180\n",
      "166\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.22\n",
      "Difference : 0.220\n",
      "167\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.27\n",
      "Difference : 0.730\n",
      "168\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.94\n",
      "Difference : 0.060\n",
      "169\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.19\n",
      "Difference : 0.190\n",
      "170\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.45\n",
      "Difference : 0.450\n",
      "171\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.05\n",
      "Difference : 0.050\n",
      "172\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.38\n",
      "Difference : 0.380\n",
      "173\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.77\n",
      "Difference : 0.770\n",
      "174\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.52\n",
      "Difference : 0.520\n",
      "175\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.64\n",
      "Difference : 1.640\n",
      "176\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.59\n",
      "Difference : 0.590\n",
      "177\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.84\n",
      "Difference : 0.840\n",
      "178\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.55\n",
      "Difference : 0.450\n",
      "179\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.630\n",
      "180\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.12\n",
      "Difference : 0.120\n",
      "181\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.08\n",
      "Difference : 0.080\n",
      "182\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "183\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 14.36\n",
      "Difference : 1.360\n",
      "184\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.37\n",
      "Difference : 0.630\n",
      "185\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.70\n",
      "Difference : 0.700\n",
      "186\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.33\n",
      "Difference : 0.670\n",
      "187\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.24\n",
      "Difference : 1.240\n",
      "188\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.33\n",
      "Difference : 0.330\n",
      "189\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.70\n",
      "Difference : 0.300\n"
     ]
    }
   ],
   "source": [
    "diff_list = []\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    \n",
    "    g_truth = float(y_test_np[i])\n",
    "    p = float(predictions[i])\n",
    "    diff = np.abs(p-g_truth)\n",
    "    diff_list.append(diff)\n",
    "    print(i)  \n",
    "    print(\"Real Values :\", g_truth)\n",
    "    print(\"Predicted G3's score is {:.2f}\".format(p))\n",
    "    print(\"Difference : {:.3f}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-making",
   "metadata": {},
   "source": [
    "### Mean Absolute Error after applying genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "laughing-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.65742\n"
     ]
    }
   ],
   "source": [
    "mean_abs_e_on_test_set = sum(diff_list) / len(y_test)\n",
    "# mean_abs_e_on_test_set = mean_absolute_error(y_test_np, predictions)\n",
    "print(\"Mean Absolute Error : {:.5f}\".format(mean_abs_e_on_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-corpus",
   "metadata": {},
   "source": [
    "#### Check those with difference(between predicted and ground truth) that exceeds 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "scientific-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceed3idx = []\n",
    "def check(diff_list):\n",
    "    for idx, diff in enumerate(diff_list):\n",
    "        if diff > 3:\n",
    "            exceed3idx.append(idx)\n",
    "    print(exceed3idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "restricted-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83]\n"
     ]
    }
   ],
   "source": [
    "check(diff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "unsigned-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exceed3idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
