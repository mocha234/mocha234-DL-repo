{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solved-diary",
   "metadata": {},
   "source": [
    "# StudentPerformance_GridSearchCrossValidation\n",
    "This is a duplicate notebook from StudentPerformance_with_NN. In this notebook I will use Grid Search Cross Validation to search for optimum hyperparameter instead of doing it empirically in the previous notebook.\n",
    "\n",
    "Hyperparameters Grid Search 101: \n",
    "\n",
    "https://elutins.medium.com/grid-searching-in-machine-learning-quick-explanation-and-python-implementation-550552200596\n",
    "\n",
    "References: \n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-algeria",
   "metadata": {},
   "source": [
    "### Other Notebooks(in this learnign \"series\"):\n",
    "1. [Genetic Algorithm with Python](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_with_Python.ipynb) --> Understanding Genetic Algorithm with Python(without libraries)\n",
    "2. [Predicting Student Performance(Empirically)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_with_NN.ipynb) --> Empirically tune hyper-parameters\n",
    "\n",
    "***This Notebook*** 3. [Predicting Student Performance(SKLearn's GridSearchCV)](ww) --> Tune hyper-parameters with Sci-Kit Learn's GridSearchCV\n",
    "\n",
    "4. [Predicting Student Performance(Genetic Algorithm with PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_PyGAD.ipynb) --> Tune hyper-parameters with GA using PyGAD\n",
    "5. [Predicting Student Performance(Genetic Algorithm with Trained Model + PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_Tensorflow_PyGAD.ipynb) --> Tune GA hyper-parameters using PyGAD of a trained Model\n",
    "6. [Some Findings, Comparison, Summary](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_summary.ipynb) --> Summary of this learning \"series\"\n",
    "7. [Using GA to find the Best Hyperparameters](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_with_HParamter_GA_Tuning.ipynb) --> Using GA to find the Best Hyperparameters, encoding desire hyperparameter into gene in string of chromosome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-trinity",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "1. Dataset\n",
    "2. Packages Needed\n",
    "3. Data Preprocessing\n",
    "4. Grid Search Cross Validation\n",
    "5. Inference\n",
    "6. Some findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-manufacturer",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-provider",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. \n",
    "\n",
    "Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). \n",
    "\n",
    "### Note: In this notebook, I used the Dataset with Portuguese Language\n",
    "\n",
    "In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. \n",
    "\n",
    "Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. \n",
    "\n",
    "This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd-period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-small",
   "metadata": {},
   "source": [
    "### Attribute Information:\n",
    "\n",
    "### Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n",
    "\n",
    "* 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)* \n",
    "* 2 sex - student's sex (binary: 'F' - female or 'M' - male)\n",
    "* 3 age - student's age (numeric: from 15 to 22)\n",
    "* 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n",
    "* 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n",
    "* 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n",
    "* 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
    "* 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n",
    "* 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "* 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "* 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
    "* 12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n",
    "* 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "* 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "* 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "* 16 schoolsup - extra educational support (binary: yes or no)\n",
    "* 17 famsup - family educational support (binary: yes or no)\n",
    "* 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "* 19 activities - extra-curricular activities (binary: yes or no)\n",
    "* 20 nursery - attended nursery school (binary: yes or no)\n",
    "* 21 higher - wants to take higher education (binary: yes or no)\n",
    "* 22 internet - Internet access at home (binary: yes or no)\n",
    "* 23 romantic - with a romantic relationship (binary: yes or no)\n",
    "* 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "* 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "* 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "* 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "* 29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "* 30 absences - number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "#### these grades are related with the course subject, Math or Portuguese:\n",
    "* 31 G1 - first period grade (numeric: from 0 to 20)\n",
    "* 31 G2 - second period grade (numeric: from 0 to 20)\n",
    "* 32 G3 - final grade (numeric: from 0 to 20, output target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-staff",
   "metadata": {},
   "source": [
    "## 2. Packages Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "integrated-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-roulette",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "\n",
    "Here, I will find for empty data, encode categorical data, plot some graph to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  G3  \n",
       "0       1     1       3         4   0  11  11  \n",
       "1       1     1       3         2   9  11  11  \n",
       "2       2     3       3         6  12  13  12  \n",
       "3       1     1       5         0  14  14  14  \n",
       "4       1     2       5         0  11  13  13  \n",
       "..    ...   ...     ...       ...  ..  ..  ..  \n",
       "644     1     2       5         4  10  11  10  \n",
       "645     1     1       1         4  15  15  16  \n",
       "646     1     1       5         6  11  12   9  \n",
       "647     3     4       2         6  10  10  10  \n",
       "648     3     4       5         4  10  11  11  \n",
       "\n",
       "[649 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"student-por.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-sending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.744222</td>\n",
       "      <td>2.514638</td>\n",
       "      <td>2.306626</td>\n",
       "      <td>1.568567</td>\n",
       "      <td>1.930663</td>\n",
       "      <td>0.221880</td>\n",
       "      <td>3.930663</td>\n",
       "      <td>3.180277</td>\n",
       "      <td>3.184900</td>\n",
       "      <td>1.502311</td>\n",
       "      <td>2.280431</td>\n",
       "      <td>3.536210</td>\n",
       "      <td>3.659476</td>\n",
       "      <td>11.399076</td>\n",
       "      <td>11.570108</td>\n",
       "      <td>11.906009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.218138</td>\n",
       "      <td>1.134552</td>\n",
       "      <td>1.099931</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.593235</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>1.051093</td>\n",
       "      <td>1.175766</td>\n",
       "      <td>0.924834</td>\n",
       "      <td>1.284380</td>\n",
       "      <td>1.446259</td>\n",
       "      <td>4.640759</td>\n",
       "      <td>2.745265</td>\n",
       "      <td>2.913639</td>\n",
       "      <td>3.230656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean    16.744222    2.514638    2.306626    1.568567    1.930663    0.221880   \n",
       "std      1.218138    1.134552    1.099931    0.748660    0.829510    0.593235   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    2.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean     3.930663    3.180277    3.184900    1.502311    2.280431    3.536210   \n",
       "std      0.955717    1.051093    1.175766    0.924834    1.284380    1.446259   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    2.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences          G1          G2          G3  \n",
       "count  649.000000  649.000000  649.000000  649.000000  \n",
       "mean     3.659476   11.399076   11.570108   11.906009  \n",
       "std      4.640759    2.745265    2.913639    3.230656  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000   10.000000   10.000000   10.000000  \n",
       "50%      2.000000   11.000000   11.000000   12.000000  \n",
       "75%      6.000000   13.000000   13.000000   14.000000  \n",
       "max     32.000000   19.000000   19.000000   19.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infectious-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum() \n",
    "# Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entitled-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
      "\n",
      "Number of Features: 32\n"
     ]
    }
   ],
   "source": [
    "features_list = list(df.columns)[:-1]\n",
    "print(\"Features: {x}\".format(x = features_list))\n",
    "print(\"\\nNumber of Features: {x}\".format(x = len(features_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brutal-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "# # Check datatype of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brilliant-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  \n",
       "0       1     1       3         4   0  11  \n",
       "1       1     1       3         2   9  11  \n",
       "2       2     3       3         6  12  13  \n",
       "3       1     1       5         0  14  14  \n",
       "4       1     2       5         0  11  13  \n",
       "..    ...   ...     ...       ...  ..  ..  \n",
       "644     1     2       5         4  10  11  \n",
       "645     1     1       1         4  15  15  \n",
       "646     1     1       5         6  11  12  \n",
       "647     3     4       2         6  10  10  \n",
       "648     3     4       5         4  10  11  \n",
       "\n",
       "[649 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop([\"G3\"], axis = 1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "small-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     G3\n",
       "0    11\n",
       "1    11\n",
       "2    12\n",
       "3    14\n",
       "4    13\n",
       "..   ..\n",
       "644  10\n",
       "645  16\n",
       "646   9\n",
       "647  10\n",
       "648  11\n",
       "\n",
       "[649 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.drop(features_list, axis = 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opponent-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_school = {'GP' : 0, 'MS' : 1}\n",
    "mapping_sex = {'F' : 0, 'M' : 1}\n",
    "mapping_address = {'U' : 0, 'R' : 1}\n",
    "mapping_famsize = {'GT3' : 0, 'LE3' : 1}\n",
    "mapping_pstatus = {'A' : 0, 'T' : 1}\n",
    "mapping_mjob = {'at_home' : 0, 'health' : 1, 'other' : 2, 'services' : 3, 'teacher' : 4}\n",
    "mapping_fjob = {'at_home' : 0, 'health' : 1, 'other' : 2, 'services' : 3, 'teacher' : 4}\n",
    "mapping_reason = {'course' : 0, 'other' : 1, 'home' : 2, 'reputation' : 3}\n",
    "mapping_guardian = {'mother' : 0, 'father' : 1, 'other': 2}\n",
    "mapping_schoolsup = {'no' : 0, 'yes' : 1}\n",
    "mapping_famsup = {'no' : 0, 'yes' : 1}\n",
    "mapping_romantic = {'no' : 0, 'yes' : 1}\n",
    "mapping_paid = {'no' : 0, 'yes' : 1}\n",
    "mapping_activities = {'no' : 0, 'yes' : 1}\n",
    "mapping_nursery = {'no' : 0, 'yes' : 1}\n",
    "mapping_higher = {'no' : 0, 'yes' : 1}\n",
    "mapping_internet = {'no' : 0, 'yes' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "strong-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['school'] = features['school'].map(mapping_school)\n",
    "df['sex'] = features['sex'].map(mapping_sex)\n",
    "df['address'] = features['address'].map(mapping_address)\n",
    "df['famsize'] = features['famsize'].map(mapping_famsize)\n",
    "df['Pstatus'] = features['Pstatus'].map(mapping_pstatus)\n",
    "df['Mjob'] = features['Mjob'].map(mapping_mjob)\n",
    "df['Fjob'] = features['Fjob'].map(mapping_fjob)\n",
    "df['reason'] = features['reason'].map(mapping_reason)\n",
    "df['guardian'] = features['guardian'].map(mapping_guardian).astype('Int64')\n",
    "df['famsup'] = features['famsup'].map(mapping_famsup)\n",
    "df['schoolsup'] = features['schoolsup'].map(mapping_schoolsup)\n",
    "df['romantic'] = features['romantic'].map(mapping_romantic)\n",
    "df['paid'] = features['paid'].map(mapping_paid)\n",
    "df['activities'] = features['activities'].map(mapping_activities)\n",
    "df['nursery'] = features['nursery'].map(mapping_nursery)\n",
    "df['higher'] = features['higher'].map(mapping_higher)\n",
    "df['internet'] = features['internet'].map(mapping_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conditional-italic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>R</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>MS</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>R</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0       GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1       GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2       GP   F   15       U     LE3       T     1     1   at_home     other   \n",
       "3       GP   F   15       U     GT3       T     4     2    health  services   \n",
       "4       GP   F   16       U     GT3       T     3     3     other     other   \n",
       "..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   \n",
       "644     MS   F   19       R     GT3       T     2     3  services     other   \n",
       "645     MS   F   18       U     LE3       T     3     1   teacher  services   \n",
       "646     MS   F   18       U     GT3       T     1     1     other     other   \n",
       "647     MS   M   17       U     LE3       T     3     1  services  services   \n",
       "648     MS   M   18       R     LE3       T     3     2  services     other   \n",
       "\n",
       "     reason guardian  traveltime  studytime  failures schoolsup famsup paid  \\\n",
       "0    course   mother           2          2         0       yes     no   no   \n",
       "1    course   father           1          2         0        no    yes   no   \n",
       "2     other   mother           1          2         0       yes     no   no   \n",
       "3      home   mother           1          3         0        no    yes   no   \n",
       "4      home   father           1          2         0        no    yes   no   \n",
       "..      ...      ...         ...        ...       ...       ...    ...  ...   \n",
       "644  course   mother           1          3         1        no     no   no   \n",
       "645  course   mother           1          2         0        no    yes   no   \n",
       "646  course   mother           2          2         0        no     no   no   \n",
       "647  course   mother           2          1         0        no     no   no   \n",
       "648  course   mother           3          1         0        no     no   no   \n",
       "\n",
       "    activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "0           no     yes    yes       no       no       4         3      4   \n",
       "1           no      no    yes      yes       no       5         3      3   \n",
       "2           no     yes    yes      yes       no       4         3      2   \n",
       "3          yes     yes    yes      yes      yes       3         2      2   \n",
       "4           no     yes    yes       no       no       4         3      2   \n",
       "..         ...     ...    ...      ...      ...     ...       ...    ...   \n",
       "644        yes      no    yes      yes       no       5         4      2   \n",
       "645         no     yes    yes      yes       no       4         3      4   \n",
       "646        yes     yes    yes       no       no       1         1      1   \n",
       "647         no      no    yes      yes       no       2         4      5   \n",
       "648         no      no    yes      yes       no       4         4      1   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  \n",
       "0       1     1       3         4   0  11  \n",
       "1       1     1       3         2   9  11  \n",
       "2       2     3       3         6  12  13  \n",
       "3       1     1       5         0  14  14  \n",
       "4       1     2       5         0  11  13  \n",
       "..    ...   ...     ...       ...  ..  ..  \n",
       "644     1     2       5         4  10  11  \n",
       "645     1     1       1         4  15  15  \n",
       "646     1     1       5         6  11  12  \n",
       "647     3     4       2         6  10  10  \n",
       "648     3     4       5         4  10  11  \n",
       "\n",
       "[649 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "literary-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "failing-qualification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='school', data=features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vocational-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features.hist(bins=20, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-secretariat",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "As I go through the data, I found there are students who achieve 0 in their G3 depsite having more than half during their G1 & G2 examinations. \n",
    "The outliers might affect model's performance, hence I decided to drop those outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "young-preview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=target[\"G3\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-frost",
   "metadata": {},
   "source": [
    "#### Drop Outliers data if certain requirement satisfied.\n",
    "\n",
    "G3 with score of 2.5 and less considered as Outliers. To understand this data, I will list down these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "twenty-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  \\\n",
       "163       0    1   18        0        1        1     1     1     2     2   \n",
       "172       0    1   16        0        0        1     3     3     2     3   \n",
       "440       1    1   16        0        0        1     1     1     0     3   \n",
       "519       1    1   16        1        0        1     2     1     2     3   \n",
       "563       1    1   17        0        0        1     2     2     2     2   \n",
       "567       1    1   18        1        0        1     3     2     3     2   \n",
       "583       1    0   18        1        0        1     2     2     2     2   \n",
       "586       1    0   17        0        0        1     4     2     4     3   \n",
       "597       1    0   18        1        0        1     2     2     0     2   \n",
       "603       1    0   18        1        1        0     4     2     4     2   \n",
       "605       1    0   19        0        0        1     1     1     0     3   \n",
       "610       1    0   19        1        0        0     1     1     0     0   \n",
       "626       1    0   18        1        0        1     4     4     2     4   \n",
       "637       1    1   18        1        0        1     2     1     2     2   \n",
       "639       1    1   19        1        0        1     1     1     2     3   \n",
       "640       1    1   18        1        0        1     4     2     2     2   \n",
       "\n",
       "     reason  guardian  traveltime  studytime  failures  schoolsup  famsup  \\\n",
       "163       0         0           1          1         2          0       0   \n",
       "172       0         1           1          2         1          0       1   \n",
       "440       2         0           2          2         0          0       1   \n",
       "519       3         0           2          2         0          0       0   \n",
       "563       0         0           1          1         1          0       0   \n",
       "567       0         0           1          1         1          0       0   \n",
       "583       1         0           2          1         1          0       0   \n",
       "586       2         0           1          2         0          1       1   \n",
       "597       0         0           3          2         1          0       0   \n",
       "603       3         0           1          2         0          0       0   \n",
       "605       1         1           2          1         1          0       0   \n",
       "610       0         2           2          2         3          0       1   \n",
       "626       1         1           3          2         0          0       1   \n",
       "637       1         0           2          1         0          0       0   \n",
       "639       1         0           2          1         1          0       0   \n",
       "640       2         1           2          1         1          0       0   \n",
       "\n",
       "     paid  activities  nursery  higher  internet  romantic  famrel  freetime  \\\n",
       "163     0           0        1       0         1         1       2         3   \n",
       "172     0           0        1       1         1         1       4         5   \n",
       "440     0           1        1       1         0         1       5         4   \n",
       "519     0           1        1       1         1         0       5         2   \n",
       "563     0           1        1       1         0         1       1         2   \n",
       "567     0           0        1       0         1         0       2         3   \n",
       "583     0           0        1       0         1         1       5         5   \n",
       "586     0           1        1       1         1         0       5         5   \n",
       "597     0           1        1       1         0         1       4         3   \n",
       "603     0           1        1       1         1         1       5         3   \n",
       "605     0           0        1       0         0         0       5         5   \n",
       "610     0           1        1       0         0         1       3         5   \n",
       "626     0           0        0       1         1         1       3         2   \n",
       "637     0           1        0       1         1         1       4         4   \n",
       "639     0           0        1       1         0         0       4         3   \n",
       "640     1           0        1       1         0         0       5         4   \n",
       "\n",
       "     goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
       "163      5     2     5       4         0  11   9   0  \n",
       "172      5     4     4       5         0  10  10   1  \n",
       "440      5     4     5       3         0   7   0   0  \n",
       "519      1     1     1       2         0   8   7   0  \n",
       "563      1     2     3       5         0   7   0   0  \n",
       "567      1     2     2       5         0   4   0   0  \n",
       "583      5     1     1       3         0   8   6   0  \n",
       "586      5     1     3       5         0   8   8   0  \n",
       "597      3     1     1       4         0   9   0   0  \n",
       "603      1     1     1       5         0   5   0   0  \n",
       "605      5     2     3       2         0   5   0   0  \n",
       "610      4     1     4       1         0   8   0   0  \n",
       "626      2     4     2       5         0   7   5   0  \n",
       "637      3     1     3       5         0   7   7   0  \n",
       "639      2     1     3       5         0   5   8   0  \n",
       "640      3     4     3       3         0   7   7   0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outliers = df[df[\"G3\"]<2.5]\n",
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "english-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confidential-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.399075500770415"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"G1\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ready-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"G2\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-toner",
   "metadata": {},
   "source": [
    "#### Dropping rows if:\n",
    "\n",
    "G3 considered as outliers when G1 or G2 above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caroline-count",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  \\\n",
       "0         0    0   18        0        0        0     4     4     0     4   \n",
       "1         0    0   17        0        0        1     1     1     0     2   \n",
       "2         0    0   15        0        1        1     1     1     0     2   \n",
       "3         0    0   15        0        0        1     4     2     1     3   \n",
       "4         0    0   16        0        0        1     3     3     2     2   \n",
       "..      ...  ...  ...      ...      ...      ...   ...   ...   ...   ...   \n",
       "644       1    0   19        1        0        1     2     3     3     2   \n",
       "645       1    0   18        0        1        1     3     1     4     3   \n",
       "646       1    0   18        0        0        1     1     1     2     2   \n",
       "647       1    1   17        0        1        1     3     1     3     3   \n",
       "648       1    1   18        1        1        1     3     2     3     2   \n",
       "\n",
       "     reason  guardian  traveltime  studytime  failures  schoolsup  famsup  \\\n",
       "0         0         0           2          2         0          1       0   \n",
       "1         0         1           1          2         0          0       1   \n",
       "2         1         0           1          2         0          1       0   \n",
       "3         2         0           1          3         0          0       1   \n",
       "4         2         1           1          2         0          0       1   \n",
       "..      ...       ...         ...        ...       ...        ...     ...   \n",
       "644       0         0           1          3         1          0       0   \n",
       "645       0         0           1          2         0          0       1   \n",
       "646       0         0           2          2         0          0       0   \n",
       "647       0         0           2          1         0          0       0   \n",
       "648       0         0           3          1         0          0       0   \n",
       "\n",
       "     paid  activities  nursery  higher  internet  romantic  famrel  freetime  \\\n",
       "0       0           0        1       1         0         0       4         3   \n",
       "1       0           0        0       1         1         0       5         3   \n",
       "2       0           0        1       1         1         0       4         3   \n",
       "3       0           1        1       1         1         1       3         2   \n",
       "4       0           0        1       1         0         0       4         3   \n",
       "..    ...         ...      ...     ...       ...       ...     ...       ...   \n",
       "644     0           1        0       1         1         0       5         4   \n",
       "645     0           0        1       1         1         0       4         3   \n",
       "646     0           1        1       1         0         0       1         1   \n",
       "647     0           0        0       1         1         0       2         4   \n",
       "648     0           0        0       1         1         0       4         4   \n",
       "\n",
       "     goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
       "0        4     1     1       3         4   0  11  11  \n",
       "1        3     1     1       3         2   9  11  11  \n",
       "2        2     2     3       3         6  12  13  12  \n",
       "3        2     1     1       5         0  14  14  14  \n",
       "4        2     1     2       5         0  11  13  13  \n",
       "..     ...   ...   ...     ...       ...  ..  ..  ..  \n",
       "644      2     1     2       5         4  10  11  10  \n",
       "645      4     1     1       1         4  15  15  16  \n",
       "646      1     1     1       5         6  11  12   9  \n",
       "647      5     3     4       2         6  10  10  10  \n",
       "648      1     3     4       5         4  10  11  11  \n",
       "\n",
       "[633 rows x 33 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_idx = df[df[\"G3\"] < 2.5].index\n",
    "len(outlier_idx) \n",
    "df = df.drop(outlier_idx)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-exposure",
   "metadata": {},
   "source": [
    "### Reassign features and target dataframe after dropping outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "directed-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.drop(features_list, axis = 1)\n",
    "features = df.drop([\"G3\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-victim",
   "metadata": {},
   "source": [
    "## 4. Grid Search Cross Validation\n",
    "\n",
    "Here the real thing begins. We will use Sci-Kit Learn's GridSearchCV to find the optimal hypter parameters for the model for this problem.\n",
    "\n",
    "#### To be tuned:\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm\n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-letters",
   "metadata": {},
   "source": [
    "#### Search/Tuning Flow:\n",
    "1. Define Model\n",
    "2. Wrap Keras model with KerasRegressor of Sci-Kit Learn\n",
    "3. Define Grid Parameters\n",
    "4. Train model with Grid Parameters defined\n",
    "5. Summarize Results\n",
    "\n",
    "Note: Set verbose = 0, as it's gonna be long list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-jacket",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-postcard",
   "metadata": {},
   "source": [
    "#### Split Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affecting-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "republican-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affecting-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "y_test_scaled = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "annual-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled = scaler.inverse_transform(X_train_scaled).astype('float64') \n",
    "y_train_unscaled = scaler.inverse_transform(y_train_scaled).astype('float64') \n",
    "X_test_unscaled = scaler.inverse_transform(X_test_scaled).astype('float64') \n",
    "y_test_unscaled = scaler.inverse_transform(y_test_scaled).astype('float64') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "medical-despite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 32), (443, 32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-times",
   "metadata": {},
   "source": [
    "### Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-bandwidth",
   "metadata": {},
   "source": [
    "### Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "\n",
    "1. ###### Batch Size & Number of Epochs with Training Optimization Algorithm (This section)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm \n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n",
    "\n",
    "Based on the best parameter obtained, we can further fine tune it. For instance, the Adam optimizer used with default Learning Rate of 0.001. Here, GridSearchCV is used again to search for best Learning Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "considerable-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,281\n",
      "Trainable params: 5,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "timely-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "specialized-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "batch_size = [10, 50]\n",
    "epochs = [10, 50, 100, 200, 500, 700, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ultimate-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.009181 using {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.279537 (0.185846) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'SGD'}\n",
      "-0.153868 (0.186314) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'RMSprop'}\n",
      "-0.283783 (0.180655) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'Adagrad'}\n",
      "-0.017520 (0.004620) with: {'batch_size': 10, 'epochs': 10, 'optimizer': 'Adam'}\n",
      "-0.016705 (0.002449) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "-0.281458 (0.192481) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "-0.022869 (0.007195) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'Adagrad'}\n",
      "-0.151504 (0.196080) with: {'batch_size': 10, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "-0.017360 (0.004792) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.152149 (0.195659) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.019997 (0.005539) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.012150 (0.002359) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.151648 (0.195987) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'SGD'}\n",
      "-0.147137 (0.190845) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'RMSprop'}\n",
      "-0.015185 (0.002828) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'Adagrad'}\n",
      "-0.011459 (0.003277) with: {'batch_size': 10, 'epochs': 200, 'optimizer': 'Adam'}\n",
      "-0.010346 (0.002566) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'SGD'}\n",
      "-0.011955 (0.004506) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'RMSprop'}\n",
      "-0.016548 (0.005266) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'Adagrad'}\n",
      "-0.010020 (0.003102) with: {'batch_size': 10, 'epochs': 500, 'optimizer': 'Adam'}\n",
      "-0.009606 (0.001954) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'SGD'}\n",
      "-0.012910 (0.002452) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'RMSprop'}\n",
      "-0.013756 (0.003341) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adagrad'}\n",
      "-0.009181 (0.003678) with: {'batch_size': 10, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.290223 (0.187667) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.147466 (0.190564) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.016944 (0.004093) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.147154 (0.190795) with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.169644 (0.183346) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'SGD'}\n",
      "-0.025840 (0.003522) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'RMSprop'}\n",
      "-0.167838 (0.184527) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'Adagrad'}\n",
      "-0.162138 (0.188616) with: {'batch_size': 50, 'epochs': 10, 'optimizer': 'Adam'}\n",
      "-0.166561 (0.185516) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'SGD'}\n",
      "-0.012595 (0.004544) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'RMSprop'}\n",
      "-0.026488 (0.008357) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adagrad'}\n",
      "-0.287659 (0.191292) with: {'batch_size': 50, 'epochs': 50, 'optimizer': 'Adam'}\n",
      "-0.021396 (0.004066) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.017523 (0.002887) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.024120 (0.006072) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.151252 (0.187917) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n",
      "-0.020752 (0.002579) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'SGD'}\n",
      "-0.016128 (0.005276) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'RMSprop'}\n",
      "-0.148937 (0.181778) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'Adagrad'}\n",
      "-0.013592 (0.003179) with: {'batch_size': 50, 'epochs': 200, 'optimizer': 'Adam'}\n",
      "-0.014516 (0.005352) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'SGD'}\n",
      "-0.014056 (0.004813) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'RMSprop'}\n",
      "-0.017728 (0.002687) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'Adagrad'}\n",
      "-0.014298 (0.001329) with: {'batch_size': 50, 'epochs': 500, 'optimizer': 'Adam'}\n",
      "-0.013747 (0.002332) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'SGD'}\n",
      "-0.014231 (0.003759) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'RMSprop'}\n",
      "-0.016384 (0.004249) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'Adagrad'}\n",
      "-0.012592 (0.000909) with: {'batch_size': 50, 'epochs': 700, 'optimizer': 'Adam'}\n",
      "-0.022778 (0.002853) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'SGD'}\n",
      "-0.150281 (0.188579) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'RMSprop'}\n",
      "-0.148360 (0.182190) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adagrad'}\n",
      "-0.014415 (0.000676) with: {'batch_size': 50, 'epochs': 100, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-religion",
   "metadata": {},
   "source": [
    "Some reading on the negative values on scoring : \n",
    "\n",
    "https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-excitement",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. ###### Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (This section)\n",
    "4. Network Weight Initialization\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n",
    "\n",
    "Based on the best parameter obtained, we can further fine tune it. For instance, the Adam optimizer used with default Learning Rate of 0.001. Here, GridSearchCV is used again to search for best Learning Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "prescribed-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_Adam(learn_rate=0.001):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    optimizer = Adam(lr = learn_rate)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_Adam = KerasRegressor(build_fn=create_model_Adam, epochs=700, batch_size=10,  verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "seeing-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.005, 0.1]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model_Adam, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "equipped-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.008193 using {'learn_rate': 0.01}\n",
      "-0.149856 (0.197258) with: {'learn_rate': 0.001}\n",
      "-0.008193 (0.002773) with: {'learn_rate': 0.01}\n",
      "-0.008898 (0.002184) with: {'learn_rate': 0.005}\n",
      "-0.417256 (0.009306) with: {'learn_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-professor",
   "metadata": {},
   "source": [
    "### Network Weight Initialization\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. ###### Network Weight Initialization(This section)\n",
    "5. Neuron Activation Function\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mental-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_weight_init(init_mode = 'uniform'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu', kernel_initializer=init_mode),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_weight_init = KerasRegressor(build_fn=create_model_weight_init, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "legal-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_weight_init, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "divided-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.007511 using {'init_mode': 'lecun_uniform'}\n",
      "-0.009289 (0.001876) with: {'init_mode': 'uniform'}\n",
      "-0.007511 (0.000750) with: {'init_mode': 'lecun_uniform'}\n",
      "-0.010183 (0.000559) with: {'init_mode': 'normal'}\n",
      "-0.419239 (0.008901) with: {'init_mode': 'zero'}\n",
      "-0.008358 (0.000608) with: {'init_mode': 'glorot_normal'}\n",
      "-0.010530 (0.002378) with: {'init_mode': 'glorot_uniform'}\n",
      "-0.146964 (0.194618) with: {'init_mode': 'he_normal'}\n",
      "-0.009648 (0.001557) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-violence",
   "metadata": {},
   "source": [
    "### Neuron Activation Function\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(lecun_uniform)\n",
    "5. ###### Neuron Activation Function (This section)\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distant-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_act_func(activation='relu', output_activation = 'sigmoid'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation='relu', kernel_initializer='lecun_uniform'),\n",
    "        Dense(64, activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_act_func = KerasRegressor(build_fn=create_model_act_func, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "charming-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "output_activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation, output_activation=output_activation)\n",
    "grid = GridSearchCV(estimator=model_act_func, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rubber-former",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.007706 using {'activation': 'linear', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softmax', 'output_activation': 'softmax'}\n",
      "-0.009161 (0.001533) with: {'activation': 'softmax', 'output_activation': 'softplus'}\n",
      "-0.024258 (0.008749) with: {'activation': 'softmax', 'output_activation': 'softsign'}\n",
      "-0.286598 (0.181235) with: {'activation': 'softmax', 'output_activation': 'relu'}\n",
      "-0.024121 (0.008951) with: {'activation': 'softmax', 'output_activation': 'tanh'}\n",
      "-0.010295 (0.001206) with: {'activation': 'softmax', 'output_activation': 'hard_sigmoid'}\n",
      "-0.028771 (0.003188) with: {'activation': 'softmax', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softplus', 'output_activation': 'softmax'}\n",
      "-0.008475 (0.000064) with: {'activation': 'softplus', 'output_activation': 'softplus'}\n",
      "-0.010775 (0.001842) with: {'activation': 'softplus', 'output_activation': 'softsign'}\n",
      "-0.419239 (0.008901) with: {'activation': 'softplus', 'output_activation': 'relu'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softplus', 'output_activation': 'tanh'}\n",
      "-0.011639 (0.003424) with: {'activation': 'softplus', 'output_activation': 'hard_sigmoid'}\n",
      "-0.008378 (0.001354) with: {'activation': 'softplus', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'softsign', 'output_activation': 'softmax'}\n",
      "-0.011089 (0.002055) with: {'activation': 'softsign', 'output_activation': 'softplus'}\n",
      "-0.012855 (0.001549) with: {'activation': 'softsign', 'output_activation': 'softsign'}\n",
      "-0.010529 (0.001922) with: {'activation': 'softsign', 'output_activation': 'relu'}\n",
      "-0.009947 (0.002298) with: {'activation': 'softsign', 'output_activation': 'tanh'}\n",
      "-0.009909 (0.001789) with: {'activation': 'softsign', 'output_activation': 'hard_sigmoid'}\n",
      "-0.010643 (0.000951) with: {'activation': 'softsign', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'tanh', 'output_activation': 'softmax'}\n",
      "-0.008445 (0.001837) with: {'activation': 'tanh', 'output_activation': 'softplus'}\n",
      "-0.011304 (0.001610) with: {'activation': 'tanh', 'output_activation': 'softsign'}\n",
      "-0.283108 (0.190627) with: {'activation': 'tanh', 'output_activation': 'relu'}\n",
      "-0.010336 (0.000477) with: {'activation': 'tanh', 'output_activation': 'tanh'}\n",
      "-0.010848 (0.001205) with: {'activation': 'tanh', 'output_activation': 'hard_sigmoid'}\n",
      "-0.010669 (0.001457) with: {'activation': 'tanh', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'sigmoid', 'output_activation': 'softmax'}\n",
      "-0.008795 (0.001676) with: {'activation': 'sigmoid', 'output_activation': 'softplus'}\n",
      "-0.010369 (0.000915) with: {'activation': 'sigmoid', 'output_activation': 'softsign'}\n",
      "-0.146223 (0.195138) with: {'activation': 'sigmoid', 'output_activation': 'relu'}\n",
      "-0.009023 (0.000562) with: {'activation': 'sigmoid', 'output_activation': 'tanh'}\n",
      "-0.010864 (0.000667) with: {'activation': 'sigmoid', 'output_activation': 'hard_sigmoid'}\n",
      "-0.009131 (0.001044) with: {'activation': 'sigmoid', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'hard_sigmoid', 'output_activation': 'softmax'}\n",
      "-0.008525 (0.000430) with: {'activation': 'hard_sigmoid', 'output_activation': 'softplus'}\n",
      "-0.010965 (0.002323) with: {'activation': 'hard_sigmoid', 'output_activation': 'softsign'}\n",
      "-0.280799 (0.189432) with: {'activation': 'hard_sigmoid', 'output_activation': 'relu'}\n",
      "-0.010008 (0.001598) with: {'activation': 'hard_sigmoid', 'output_activation': 'tanh'}\n",
      "-0.009042 (0.002803) with: {'activation': 'hard_sigmoid', 'output_activation': 'hard_sigmoid'}\n",
      "-0.009524 (0.001091) with: {'activation': 'hard_sigmoid', 'output_activation': 'linear'}\n",
      "-0.168630 (0.008872) with: {'activation': 'linear', 'output_activation': 'softmax'}\n",
      "-0.153807 (0.190055) with: {'activation': 'linear', 'output_activation': 'softplus'}\n",
      "-0.011334 (0.002098) with: {'activation': 'linear', 'output_activation': 'softsign'}\n",
      "-0.287136 (0.195375) with: {'activation': 'linear', 'output_activation': 'relu'}\n",
      "-0.168630 (0.008872) with: {'activation': 'linear', 'output_activation': 'tanh'}\n",
      "-0.008507 (0.002478) with: {'activation': 'linear', 'output_activation': 'hard_sigmoid'}\n",
      "-0.007706 (0.001413) with: {'activation': 'linear', 'output_activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-strengthening",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 700, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. ###### Dropout Regularization (This section)\n",
    "7. Number of Neurons in the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-niger",
   "metadata": {},
   "source": [
    "### Check if the model is overfitting. \n",
    "If overfit --> add Dropout layer to regularize it, else skip having dropout layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "technical-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_current(activation= 'linear', output_activation = 'linear'):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=32, activation=activation, kernel_initializer='lecun_uniform'),\n",
    "        Dense(64, activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_current = create_model_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "boxed-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_current.fit(X_train, y_train, epochs=700, batch_size=10,  verbose=0, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "delayed-benchmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "liked-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00700\n",
      "Mean Absolute Error: 0.05615\n"
     ]
    }
   ],
   "source": [
    "mean_squared_e = mean_squared_error(y_test, model_current.predict(X_test))\n",
    "mean_absolute_e = mean_absolute_error(y_test, model_current.predict(X_test))\n",
    "\n",
    "print(\"Mean Squared Error: {:.5f}\".format(mean_squared_e))\n",
    "print(\"Mean Absolute Error: {:.5f}\".format(mean_absolute_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-gates",
   "metadata": {},
   "source": [
    "Based on the graph, it's overfitting, hence, will use Dropout to regularize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "comparative-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dropout(dropout_rate=0.0, weight_constraint=0):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(weight_constraint),\n",
    "             ),\n",
    "        Dense(64, activation='linear'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_dropout = KerasRegressor(build_fn=create_model_dropout, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "elegant-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model_dropout, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ranging-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.005696 using {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-0.008676 (0.003370) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "-0.006832 (0.002109) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "-0.006418 (0.000472) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "-0.007079 (0.000357) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "-0.006290 (0.000567) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "-0.009926 (0.002168) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "-0.005843 (0.000249) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "-0.007070 (0.000789) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "-0.006986 (0.001584) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "-0.007178 (0.001208) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "-0.006401 (0.000703) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "-0.007322 (0.001090) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "-0.006321 (0.000185) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "-0.006330 (0.001531) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "-0.005696 (0.000961) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-0.006039 (0.001164) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "-0.005972 (0.000927) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "-0.006441 (0.000620) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "-0.005949 (0.000229) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "-0.005938 (0.001228) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "-0.012160 (0.002270) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "-0.006102 (0.000794) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "-0.006866 (0.000340) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "-0.005879 (0.000905) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "-0.009077 (0.004544) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "-0.005770 (0.000746) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "-0.005991 (0.001093) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "-0.006973 (0.001444) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "-0.006232 (0.001181) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "-0.006957 (0.001568) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "-0.007830 (0.002930) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "-0.011096 (0.005059) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "-0.006223 (0.001395) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "-0.007421 (0.002853) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "-0.085004 (0.105859) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "-0.007378 (0.001353) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "-0.007049 (0.000998) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "-0.006178 (0.001011) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "-0.007577 (0.002851) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "-0.007179 (0.001591) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "-0.007776 (0.001627) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "-0.016252 (0.010666) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "-0.008409 (0.001422) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "-0.007769 (0.002212) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "-0.008180 (0.002944) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-privilege",
   "metadata": {},
   "source": [
    "### Neuron Activation Function\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 500, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. Dropout Regularization\n",
    "7. Number of Neurons in the Hidden Layer(This section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "auburn-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_neurons_hidden_layer(neurons_h_layer_1 = 1, neurons_h_layer_2 = 1):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(5),\n",
    "             ),\n",
    "        Dense(neurons_h_layer_1, activation='linear'),\n",
    "        Dropout(0.2),\n",
    "        Dense(neurons_h_layer_2, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_neurons_hidden_layer = KerasRegressor(build_fn=create_model_neurons_hidden_layer, epochs=700, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "incorporated-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "neurons_h_layer_1 = [8, 16, 32, 64, 128]\n",
    "neurons_h_layer_2 = [16, 32, 64, 128]\n",
    "param_grid = dict(neurons_h_layer_1=neurons_h_layer_1, neurons_h_layer_2=neurons_h_layer_2)\n",
    "grid = GridSearchCV(estimator=model_neurons_hidden_layer, param_grid=param_grid, n_jobs=None, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hispanic-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.004469 using {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 32}\n",
      "-0.005880 (0.001828) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 16}\n",
      "-0.004469 (0.001343) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 32}\n",
      "-0.005019 (0.002687) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 64}\n",
      "-0.004588 (0.001767) with: {'neurons_h_layer_1': 8, 'neurons_h_layer_2': 128}\n",
      "-0.005357 (0.002798) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 16}\n",
      "-0.005053 (0.001243) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 32}\n",
      "-0.004617 (0.001724) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 64}\n",
      "-0.005306 (0.001086) with: {'neurons_h_layer_1': 16, 'neurons_h_layer_2': 128}\n",
      "-0.005038 (0.001186) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 16}\n",
      "-0.006015 (0.001433) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 32}\n",
      "-0.005585 (0.002128) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 64}\n",
      "-0.007054 (0.003031) with: {'neurons_h_layer_1': 32, 'neurons_h_layer_2': 128}\n",
      "-0.004879 (0.002587) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 16}\n",
      "-0.005504 (0.003539) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 32}\n",
      "-0.004811 (0.001322) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 64}\n",
      "-0.006063 (0.002176) with: {'neurons_h_layer_1': 64, 'neurons_h_layer_2': 128}\n",
      "-0.004519 (0.001635) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 16}\n",
      "-0.005425 (0.002256) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 32}\n",
      "-0.004894 (0.001835) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 64}\n",
      "-0.012672 (0.008984) with: {'neurons_h_layer_1': 128, 'neurons_h_layer_2': 128}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-mechanism",
   "metadata": {},
   "source": [
    "### Optimal Parameters\n",
    "\n",
    "1. Batch Size & Number of Epochs with Training Optimization Algorithm \n",
    "(Best Params : Batch Size: 10, Epochs = 500, Optimzation Algorithm: Adam)\n",
    "3. Learning Rate & Momentum(if applies) of the optimal Training Optimization Algorithm (Learning Rate = 0.01)\n",
    "4. Network Weight Initialization(glorot_normal)\n",
    "5. Neuron Activation Function (activation=tanH, output_activation = SoftPlus)\n",
    "6. Dropout Regularization (dropout_rate: 0.0, weight_constraint: 5)\n",
    "7. Number of Neurons in the Hidden Layer(First Hidden layer = 8, Second Hidden layer = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-sending",
   "metadata": {},
   "source": [
    "#### Grid Searched CV's model\n",
    "\n",
    "With the best parameter GridSearchCV could find based on given set of parameters grid. Now we instantiate a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "experimental-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_gridsearched():\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, \n",
    "              input_dim=32, \n",
    "              activation='linear', \n",
    "              kernel_initializer='lecun_uniform',\n",
    "              kernel_constraint=MaxNorm(5),\n",
    "             ),\n",
    "        Dense(8, activation='linear'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='linear'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    optimizer = Adam(lr = 0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "model_gridsearched = create_model_gridsearched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "current-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1040 - mse: 0.1040 - mae: 0.2596 - val_loss: 0.0350 - val_mse: 0.0350 - val_mae: 0.1436\n",
      "Epoch 2/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0325 - mse: 0.0325 - mae: 0.1394 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1294\n",
      "Epoch 3/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1152 - val_loss: 0.0217 - val_mse: 0.0217 - val_mae: 0.1158\n",
      "Epoch 4/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1343 - val_loss: 0.0209 - val_mse: 0.0209 - val_mae: 0.1207\n",
      "Epoch 5/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1185 - val_loss: 0.0200 - val_mse: 0.0200 - val_mae: 0.1163\n",
      "Epoch 6/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1140 - val_loss: 0.0202 - val_mse: 0.0202 - val_mae: 0.1151\n",
      "Epoch 7/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0179 - mse: 0.0179 - mae: 0.1015 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.1142\n",
      "Epoch 8/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0922 - val_loss: 0.0213 - val_mse: 0.0213 - val_mae: 0.1221\n",
      "Epoch 9/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0880 - val_loss: 0.0150 - val_mse: 0.0150 - val_mae: 0.1010\n",
      "Epoch 10/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0917 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0991\n",
      "Epoch 11/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0949 - val_loss: 0.0185 - val_mse: 0.0185 - val_mae: 0.1137\n",
      "Epoch 12/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0795 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0866\n",
      "Epoch 13/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0857 - val_loss: 0.0187 - val_mse: 0.0187 - val_mae: 0.1119\n",
      "Epoch 14/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0817 - val_loss: 0.0126 - val_mse: 0.0126 - val_mae: 0.0925\n",
      "Epoch 15/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0825 - val_loss: 0.0134 - val_mse: 0.0134 - val_mae: 0.0919\n",
      "Epoch 16/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0775 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0787\n",
      "Epoch 17/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - mse: 0.0102 - mae: 0.0797 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0986\n",
      "Epoch 18/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0809 - val_loss: 0.0162 - val_mse: 0.0162 - val_mae: 0.1055\n",
      "Epoch 19/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0839 - val_loss: 0.0222 - val_mse: 0.0222 - val_mae: 0.1229\n",
      "Epoch 20/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0840 - val_loss: 0.0155 - val_mse: 0.0155 - val_mae: 0.1000\n",
      "Epoch 21/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0873 - val_loss: 0.0163 - val_mse: 0.0163 - val_mae: 0.1039\n",
      "Epoch 22/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0877 - val_loss: 0.0138 - val_mse: 0.0138 - val_mae: 0.0942\n",
      "Epoch 23/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0802 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0968\n",
      "Epoch 24/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0761 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0754\n",
      "Epoch 25/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0732 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0876\n",
      "Epoch 26/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - mae: 0.0836 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0834\n",
      "Epoch 27/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0745 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0766\n",
      "Epoch 28/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0114 - mse: 0.0114 - mae: 0.0886 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0846\n",
      "Epoch 29/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0706 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0724\n",
      "Epoch 30/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0692 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0613\n",
      "Epoch 31/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0886 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0858\n",
      "Epoch 32/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0831 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0691\n",
      "Epoch 33/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0749 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0726\n",
      "Epoch 34/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0735 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0880\n",
      "Epoch 35/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0865 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0749\n",
      "Epoch 36/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0775 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0796\n",
      "Epoch 37/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0870 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0736\n",
      "Epoch 38/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1266 - val_loss: 0.0235 - val_mse: 0.0235 - val_mae: 0.1232\n",
      "Epoch 39/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0954 - val_loss: 0.0181 - val_mse: 0.0181 - val_mae: 0.1061\n",
      "Epoch 40/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0809 - val_loss: 0.0108 - val_mse: 0.0108 - val_mae: 0.0846\n",
      "Epoch 41/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0949 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0689\n",
      "Epoch 42/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0756 - val_loss: 0.0138 - val_mse: 0.0138 - val_mae: 0.0949\n",
      "Epoch 43/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0962 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0766\n",
      "Epoch 44/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0802 - val_loss: 0.0137 - val_mse: 0.0137 - val_mae: 0.0959\n",
      "Epoch 45/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0729 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0689\n",
      "Epoch 46/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0720 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0835\n",
      "Epoch 47/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0729 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0746\n",
      "Epoch 48/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0658 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0613\n",
      "Epoch 49/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0719 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0579\n",
      "Epoch 50/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0735 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0680\n",
      "Epoch 51/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0622 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0816\n",
      "Epoch 52/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0666 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0632\n",
      "Epoch 53/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0651 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0579\n",
      "Epoch 54/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0683 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0795\n",
      "Epoch 55/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0659 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0657\n",
      "Epoch 56/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0689 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0623\n",
      "Epoch 57/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0766 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0678\n",
      "Epoch 58/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0645 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0745\n",
      "Epoch 59/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0734 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0599\n",
      "Epoch 60/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0718 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0790\n",
      "Epoch 61/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0807 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0665\n",
      "Epoch 62/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0684 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0744\n",
      "Epoch 63/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0623 - val_loss: 0.0125 - val_mse: 0.0125 - val_mae: 0.0900\n",
      "Epoch 64/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0674 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0674\n",
      "Epoch 65/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0601 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0609\n",
      "Epoch 66/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0630 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0762\n",
      "Epoch 67/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0753 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0768\n",
      "Epoch 68/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0734 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0673\n",
      "Epoch 69/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0642 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0685\n",
      "Epoch 70/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0605 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0696\n",
      "Epoch 71/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0689 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0618\n",
      "Epoch 72/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0682 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0786\n",
      "Epoch 73/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0686 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0779\n",
      "Epoch 74/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0727 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0683\n",
      "Epoch 75/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0615 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0727\n",
      "Epoch 76/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0636 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0651\n",
      "Epoch 77/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0617 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0679\n",
      "Epoch 78/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0720 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0593\n",
      "Epoch 79/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0798 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0654\n",
      "Epoch 80/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0694 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0761\n",
      "Epoch 81/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0696 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0801\n",
      "Epoch 82/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0721 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0859\n",
      "Epoch 83/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0722 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0788\n",
      "Epoch 84/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0614 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0694\n",
      "Epoch 85/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0812 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0691\n",
      "Epoch 86/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0790 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0990\n",
      "Epoch 87/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0720 - val_loss: 0.0125 - val_mse: 0.0125 - val_mae: 0.0895\n",
      "Epoch 88/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0912 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0727\n",
      "Epoch 89/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0703 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0678\n",
      "Epoch 90/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0636 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 91/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0650 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0813\n",
      "Epoch 92/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0625 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0833\n",
      "Epoch 93/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0741 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0634\n",
      "Epoch 94/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0637 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0614\n",
      "Epoch 95/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0596 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0689\n",
      "Epoch 96/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0581 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0649\n",
      "Epoch 97/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0628 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0655\n",
      "Epoch 98/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0700 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0660\n",
      "Epoch 99/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0694 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0732\n",
      "Epoch 100/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0776 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0646\n",
      "Epoch 101/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0707 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0584\n",
      "Epoch 102/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0660 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0634\n",
      "Epoch 103/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0775 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0663\n",
      "Epoch 104/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0652 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0680\n",
      "Epoch 105/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0608 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0575\n",
      "Epoch 106/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0656 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0664\n",
      "Epoch 107/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0620 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0558\n",
      "Epoch 108/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0593 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0663\n",
      "Epoch 109/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0615 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0604\n",
      "Epoch 110/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0624 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0626\n",
      "Epoch 111/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0728 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0655\n",
      "Epoch 112/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0828 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0688\n",
      "Epoch 113/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0664 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0662\n",
      "Epoch 114/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0649 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0717\n",
      "Epoch 115/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0683 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0688\n",
      "Epoch 116/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0786 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0616\n",
      "Epoch 117/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0642 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0598\n",
      "Epoch 118/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0588 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0656\n",
      "Epoch 119/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0590 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0787\n",
      "Epoch 120/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0702 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0866\n",
      "Epoch 121/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0111 - mse: 0.0111 - mae: 0.0832 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0668\n",
      "Epoch 122/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0665 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0646\n",
      "Epoch 123/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0645 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0632\n",
      "Epoch 124/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0737 - val_loss: 0.0136 - val_mse: 0.0136 - val_mae: 0.0951\n",
      "Epoch 125/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0608 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0685\n",
      "Epoch 126/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0671 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0648\n",
      "Epoch 127/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0604 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0847\n",
      "Epoch 128/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0583 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0700\n",
      "Epoch 129/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0636 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0696\n",
      "Epoch 130/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0666 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0712\n",
      "Epoch 131/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0628 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0624\n",
      "Epoch 132/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0595 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0695\n",
      "Epoch 133/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0673 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0675\n",
      "Epoch 134/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0865 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0626\n",
      "Epoch 135/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0865 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0695\n",
      "Epoch 136/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0713 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0712\n",
      "Epoch 137/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0725 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0708\n",
      "Epoch 138/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0708 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0751\n",
      "Epoch 139/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0747 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0676\n",
      "Epoch 140/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0585 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0737\n",
      "Epoch 141/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0643 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0575\n",
      "Epoch 142/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0626 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0734\n",
      "Epoch 143/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0711 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0615\n",
      "Epoch 144/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0647 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0643\n",
      "Epoch 145/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0630 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0766\n",
      "Epoch 146/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0607 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0630\n",
      "Epoch 147/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0617 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0643\n",
      "Epoch 148/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0618 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0673\n",
      "Epoch 149/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0665 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0547\n",
      "Epoch 150/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0725 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0582 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0678\n",
      "Epoch 152/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0627 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0637\n",
      "Epoch 153/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0636 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0572\n",
      "Epoch 154/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0600 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0669\n",
      "Epoch 155/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0614 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0638\n",
      "Epoch 156/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0615 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0650\n",
      "Epoch 157/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0634 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0783\n",
      "Epoch 158/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0632 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0693\n",
      "Epoch 159/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0604 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0642\n",
      "Epoch 160/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0610 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0586\n",
      "Epoch 161/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0624 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0578\n",
      "Epoch 162/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0617 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0629\n",
      "Epoch 163/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0674 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0686\n",
      "Epoch 164/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0652 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0649\n",
      "Epoch 165/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0653 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0614\n",
      "Epoch 166/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0576 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0649\n",
      "Epoch 167/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0600 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0537\n",
      "Epoch 168/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0740 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0681\n",
      "Epoch 169/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0683 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0626\n",
      "Epoch 170/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0618 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0680\n",
      "Epoch 171/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0622 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0607\n",
      "Epoch 172/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0562 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0722\n",
      "Epoch 173/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0587 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0613\n",
      "Epoch 174/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0683 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0652\n",
      "Epoch 175/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0616 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0753\n",
      "Epoch 176/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0693 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0848\n",
      "Epoch 177/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0718 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0757\n",
      "Epoch 178/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0664 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0666\n",
      "Epoch 179/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0674 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0658\n",
      "Epoch 180/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0694 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0555\n",
      "Epoch 181/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0605 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0695\n",
      "Epoch 182/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0632 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0679\n",
      "Epoch 183/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0609 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0676\n",
      "Epoch 184/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0691 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0675\n",
      "Epoch 185/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0608 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0700\n",
      "Epoch 186/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0687 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0690\n",
      "Epoch 187/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0634 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0680\n",
      "Epoch 188/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0618\n",
      "Epoch 189/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0627 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0618\n",
      "Epoch 190/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0644 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0699\n",
      "Epoch 191/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0593 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0677\n",
      "Epoch 192/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0686 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0613\n",
      "Epoch 193/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0662 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0537\n",
      "Epoch 194/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0643 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0693\n",
      "Epoch 195/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0594 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0729\n",
      "Epoch 196/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0590 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0664\n",
      "Epoch 197/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0600 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0576\n",
      "Epoch 198/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0633 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0611\n",
      "Epoch 199/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0673 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0627\n",
      "Epoch 200/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0686 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0621 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0629\n",
      "Epoch 202/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0594 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0636\n",
      "Epoch 203/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0630 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0600\n",
      "Epoch 204/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0665 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0607\n",
      "Epoch 205/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0701 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0607\n",
      "Epoch 206/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0570 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0551\n",
      "Epoch 207/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0603 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0548\n",
      "Epoch 208/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0595 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0620\n",
      "Epoch 209/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0570 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0662\n",
      "Epoch 210/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0639 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0646\n",
      "Epoch 211/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0561 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0538\n",
      "Epoch 212/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0602 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0562\n",
      "Epoch 213/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0542 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0670\n",
      "Epoch 214/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0672 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0529\n",
      "Epoch 215/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0544 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0673\n",
      "Epoch 216/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0588 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0827\n",
      "Epoch 217/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0685 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0876\n",
      "Epoch 218/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0613 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0763\n",
      "Epoch 219/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0620 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0679\n",
      "Epoch 220/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0670 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0852\n",
      "Epoch 221/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0698 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0667\n",
      "Epoch 222/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0634 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0844\n",
      "Epoch 223/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0649 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0580\n",
      "Epoch 224/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0609 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0809\n",
      "Epoch 225/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0659 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0559\n",
      "Epoch 226/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0704 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0623\n",
      "Epoch 227/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0727 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0562\n",
      "Epoch 228/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0727 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0611\n",
      "Epoch 229/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0708 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0563\n",
      "Epoch 230/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0627 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0697\n",
      "Epoch 231/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0632 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0558\n",
      "Epoch 232/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0587 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0655\n",
      "Epoch 233/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0571 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0674\n",
      "Epoch 234/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0629 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0710\n",
      "Epoch 235/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0608 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0624\n",
      "Epoch 236/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0670 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0832\n",
      "Epoch 237/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0899 - val_loss: 0.0109 - val_mse: 0.0109 - val_mae: 0.0819\n",
      "Epoch 238/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0695 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0587\n",
      "Epoch 239/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0690 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0650\n",
      "Epoch 240/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0609 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0752\n",
      "Epoch 241/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0628 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0713\n",
      "Epoch 242/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0617 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0765\n",
      "Epoch 243/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0708 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0746\n",
      "Epoch 244/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0688 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0643\n",
      "Epoch 245/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0574 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0585\n",
      "Epoch 246/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0625 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0544\n",
      "Epoch 247/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0616 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0553\n",
      "Epoch 248/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0558 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0837\n",
      "Epoch 249/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0642 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0797\n",
      "Epoch 250/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0703 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0646 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0682\n",
      "Epoch 252/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0731 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0603\n",
      "Epoch 253/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0692 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0579\n",
      "Epoch 254/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0606 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0700\n",
      "Epoch 255/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0563 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0608\n",
      "Epoch 256/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0639 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0662\n",
      "Epoch 257/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0640 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0680\n",
      "Epoch 258/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0619 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0653\n",
      "Epoch 259/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0595 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0621\n",
      "Epoch 260/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0610 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0639\n",
      "Epoch 261/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0595 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0678\n",
      "Epoch 262/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0601 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0569\n",
      "Epoch 263/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0698 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0646\n",
      "Epoch 264/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0629 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0553\n",
      "Epoch 265/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0644 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0599\n",
      "Epoch 266/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0567 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0670\n",
      "Epoch 267/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0643 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0513\n",
      "Epoch 268/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0645 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0527\n",
      "Epoch 269/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0632 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0785\n",
      "Epoch 270/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0636 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0754\n",
      "Epoch 271/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0693 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0824\n",
      "Epoch 272/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0618 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0625\n",
      "Epoch 273/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0614 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0615\n",
      "Epoch 274/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0593 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0589\n",
      "Epoch 275/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0705 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0683\n",
      "Epoch 276/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0756 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0690\n",
      "Epoch 277/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0665 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0569\n",
      "Epoch 278/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0644 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0674\n",
      "Epoch 279/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0603 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0523\n",
      "Epoch 280/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0589 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0612\n",
      "Epoch 281/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0633 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0599\n",
      "Epoch 282/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0551 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0648\n",
      "Epoch 283/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0654 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0795\n",
      "Epoch 284/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0899 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0797\n",
      "Epoch 285/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0689 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0833\n",
      "Epoch 286/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0660 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0551\n",
      "Epoch 287/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0687 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0595\n",
      "Epoch 288/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0652 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0678\n",
      "Epoch 289/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0623 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0674\n",
      "Epoch 290/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0714 - val_loss: 0.0152 - val_mse: 0.0152 - val_mae: 0.0983\n",
      "Epoch 291/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0721 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0720\n",
      "Epoch 292/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0621 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0659\n",
      "Epoch 293/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0574 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0765\n",
      "Epoch 294/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0627 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0624\n",
      "Epoch 295/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0620 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0637\n",
      "Epoch 296/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0579 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0696\n",
      "Epoch 297/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0638 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0799\n",
      "Epoch 298/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0661 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0749\n",
      "Epoch 299/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0642 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0556\n",
      "Epoch 300/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0682 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0625 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0557\n",
      "Epoch 302/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0623 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0614\n",
      "Epoch 303/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0604 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0762\n",
      "Epoch 304/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0665 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0777\n",
      "Epoch 305/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0642 - val_loss: 0.0108 - val_mse: 0.0108 - val_mae: 0.0826\n",
      "Epoch 306/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0635 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0964\n",
      "Epoch 307/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0639 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0673\n",
      "Epoch 308/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0703 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0507\n",
      "Epoch 309/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0665 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0568\n",
      "Epoch 310/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0631 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0590\n",
      "Epoch 311/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0603 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0738\n",
      "Epoch 312/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0660 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0630\n",
      "Epoch 313/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0566 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0669\n",
      "Epoch 314/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0632 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0577\n",
      "Epoch 315/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0611 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 316/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0717 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0872\n",
      "Epoch 317/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0658 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0679\n",
      "Epoch 318/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0598 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0615\n",
      "Epoch 319/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0720 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0553\n",
      "Epoch 320/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0633 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0605\n",
      "Epoch 321/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0536 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0621\n",
      "Epoch 322/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0577 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0677\n",
      "Epoch 323/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0620 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0716\n",
      "Epoch 324/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0552 - val_loss: 0.0079 - val_mse: 0.0079 - val_mae: 0.0708\n",
      "Epoch 325/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0659 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0613\n",
      "Epoch 326/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0620 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0690\n",
      "Epoch 327/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0584 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0626\n",
      "Epoch 328/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0598 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0614\n",
      "Epoch 329/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0606 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0607\n",
      "Epoch 330/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0586 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0653\n",
      "Epoch 331/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0628 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0680\n",
      "Epoch 332/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0586 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0540\n",
      "Epoch 333/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0583 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0603\n",
      "Epoch 334/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0568 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0633\n",
      "Epoch 335/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0578 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0666\n",
      "Epoch 336/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0618 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0546\n",
      "Epoch 337/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0619 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0587\n",
      "Epoch 338/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0563 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0551\n",
      "Epoch 339/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0637 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0682\n",
      "Epoch 340/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0626 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0733\n",
      "Epoch 341/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0635 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0765\n",
      "Epoch 342/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0610 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0706\n",
      "Epoch 343/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0571 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0548\n",
      "Epoch 344/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0580 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0693\n",
      "Epoch 345/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0685 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0577\n",
      "Epoch 346/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0680 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0789\n",
      "Epoch 347/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0672 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0708\n",
      "Epoch 348/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0653 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0549\n",
      "Epoch 349/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0580 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0597\n",
      "Epoch 350/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0617 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0706 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0691\n",
      "Epoch 352/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0718 - val_loss: 0.0176 - val_mse: 0.0176 - val_mae: 0.1074\n",
      "Epoch 353/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0710 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0694\n",
      "Epoch 354/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0627 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0580\n",
      "Epoch 355/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0677 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0548\n",
      "Epoch 356/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0682 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0632\n",
      "Epoch 357/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0562 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0591\n",
      "Epoch 358/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0610 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0576\n",
      "Epoch 359/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0584 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0647\n",
      "Epoch 360/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0585 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0641\n",
      "Epoch 361/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0576 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0576\n",
      "Epoch 362/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0591 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0623\n",
      "Epoch 363/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0636 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0673\n",
      "Epoch 364/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0663 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0683\n",
      "Epoch 365/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0632 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0692\n",
      "Epoch 366/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0608 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0559\n",
      "Epoch 367/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0604 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0624\n",
      "Epoch 368/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0617 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0561\n",
      "Epoch 369/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0611 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0708\n",
      "Epoch 370/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0611 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0641\n",
      "Epoch 371/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0595 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0716\n",
      "Epoch 372/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0621 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0704\n",
      "Epoch 373/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0544 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0542\n",
      "Epoch 374/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0561 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0577\n",
      "Epoch 375/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0600 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0639\n",
      "Epoch 376/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0606 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0634\n",
      "Epoch 377/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0576 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0672\n",
      "Epoch 378/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0628 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0646\n",
      "Epoch 379/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0592 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0755\n",
      "Epoch 380/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0659 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0556\n",
      "Epoch 381/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0584 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0658\n",
      "Epoch 382/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0590\n",
      "Epoch 383/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0573 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0638\n",
      "Epoch 384/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0562 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0594\n",
      "Epoch 385/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0540 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0590\n",
      "Epoch 386/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0544 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0528\n",
      "Epoch 387/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0609 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0701\n",
      "Epoch 388/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0694 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0520\n",
      "Epoch 389/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0650 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0696\n",
      "Epoch 390/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0661 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0710\n",
      "Epoch 391/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0609 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0642\n",
      "Epoch 392/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0597 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0539\n",
      "Epoch 393/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0642 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0658\n",
      "Epoch 394/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0577 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0620\n",
      "Epoch 395/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0589 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0591\n",
      "Epoch 396/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0617 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0583\n",
      "Epoch 397/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0616 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0750\n",
      "Epoch 398/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0709 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0651\n",
      "Epoch 399/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0616 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0632\n",
      "Epoch 400/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0568 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0580 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0540\n",
      "Epoch 402/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0642 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0632\n",
      "Epoch 403/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0588 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0573\n",
      "Epoch 404/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0643 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0627\n",
      "Epoch 405/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0608 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0719\n",
      "Epoch 406/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0661 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0562\n",
      "Epoch 407/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0583 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0716\n",
      "Epoch 408/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0545 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0621\n",
      "Epoch 409/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0576 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0719\n",
      "Epoch 410/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0624 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0815\n",
      "Epoch 411/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0636 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0758\n",
      "Epoch 412/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0870 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0763\n",
      "Epoch 413/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0901 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0605\n",
      "Epoch 414/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0869 - val_loss: 0.0124 - val_mse: 0.0124 - val_mae: 0.0969\n",
      "Epoch 415/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0955 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0690\n",
      "Epoch 416/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0751 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0564\n",
      "Epoch 417/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0807 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0942\n",
      "Epoch 418/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0744 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0777\n",
      "Epoch 419/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0641 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0625\n",
      "Epoch 420/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0643 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0587\n",
      "Epoch 421/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0555 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0608\n",
      "Epoch 422/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0610 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0655\n",
      "Epoch 423/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0588 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0681\n",
      "Epoch 424/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0599 - val_loss: 0.0071 - val_mse: 0.0071 - val_mae: 0.0662\n",
      "Epoch 425/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0628 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0643\n",
      "Epoch 426/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0624 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0629\n",
      "Epoch 427/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0571 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0639\n",
      "Epoch 428/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0647 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0684\n",
      "Epoch 429/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0611 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0526\n",
      "Epoch 430/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0567 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0618\n",
      "Epoch 431/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0590 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0614\n",
      "Epoch 432/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0637 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0593\n",
      "Epoch 433/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0610 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0547\n",
      "Epoch 434/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0612 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0632\n",
      "Epoch 435/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0582 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0560\n",
      "Epoch 436/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0617 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0486\n",
      "Epoch 437/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0786 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0535\n",
      "Epoch 438/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0736 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0498\n",
      "Epoch 439/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0695 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0705\n",
      "Epoch 440/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0622 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0584\n",
      "Epoch 441/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0643 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0784\n",
      "Epoch 442/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0670 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0608\n",
      "Epoch 443/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0636 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0531\n",
      "Epoch 444/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0584 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0565\n",
      "Epoch 445/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0590 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0628\n",
      "Epoch 446/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0592 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0586\n",
      "Epoch 447/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0705 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0684\n",
      "Epoch 448/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0619 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0609\n",
      "Epoch 449/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0595 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0587\n",
      "Epoch 450/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0583 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0577 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0558\n",
      "Epoch 452/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0547 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 453/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0605 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0662\n",
      "Epoch 454/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0597 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0566\n",
      "Epoch 455/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0548 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0538\n",
      "Epoch 456/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0532 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0600\n",
      "Epoch 457/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0616 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0580\n",
      "Epoch 458/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0659 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0590\n",
      "Epoch 459/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0659 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0808\n",
      "Epoch 460/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0615 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0886\n",
      "Epoch 461/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0599 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0791\n",
      "Epoch 462/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0640 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0575\n",
      "Epoch 463/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0593 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0598\n",
      "Epoch 464/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0603 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0624\n",
      "Epoch 465/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0666 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0582\n",
      "Epoch 466/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0605 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0596\n",
      "Epoch 467/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0665 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0668\n",
      "Epoch 468/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0615 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0571\n",
      "Epoch 469/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0567\n",
      "Epoch 470/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0675 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0678\n",
      "Epoch 471/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0755 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0835\n",
      "Epoch 472/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0683 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0584\n",
      "Epoch 473/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0648 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0717\n",
      "Epoch 474/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0577 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0584\n",
      "Epoch 475/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0588 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0521\n",
      "Epoch 476/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0583 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0555\n",
      "Epoch 477/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0551 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0660\n",
      "Epoch 478/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0625 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0559\n",
      "Epoch 479/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0602 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0589\n",
      "Epoch 480/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0641 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0583\n",
      "Epoch 481/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0551 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0616\n",
      "Epoch 482/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0607 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0586\n",
      "Epoch 483/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0605 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0680\n",
      "Epoch 484/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0648 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0676\n",
      "Epoch 485/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0671 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0566\n",
      "Epoch 486/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0582 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0583\n",
      "Epoch 487/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0583 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0634\n",
      "Epoch 488/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0607 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0746\n",
      "Epoch 489/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0628 - val_loss: 0.0077 - val_mse: 0.0077 - val_mae: 0.0699\n",
      "Epoch 490/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0631 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0560\n",
      "Epoch 491/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0589 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0639\n",
      "Epoch 492/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0639 - val_loss: 0.0070 - val_mse: 0.0070 - val_mae: 0.0679\n",
      "Epoch 493/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0651 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0659\n",
      "Epoch 494/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0572 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0519\n",
      "Epoch 495/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0590 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0604\n",
      "Epoch 496/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0597 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0785\n",
      "Epoch 497/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0631 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0599\n",
      "Epoch 498/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0569 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0631\n",
      "Epoch 499/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0602 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0555\n",
      "Epoch 500/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0593 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0613 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0605\n",
      "Epoch 502/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0601 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0527\n",
      "Epoch 503/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0579 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0605\n",
      "Epoch 504/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0560 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0715\n",
      "Epoch 505/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0651 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0564\n",
      "Epoch 506/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0587 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0567\n",
      "Epoch 507/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0572 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0524\n",
      "Epoch 508/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0586 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0528\n",
      "Epoch 509/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0661 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0589\n",
      "Epoch 510/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0605 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0572\n",
      "Epoch 511/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0586 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0621\n",
      "Epoch 512/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0627 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0654\n",
      "Epoch 513/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0653 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0588\n",
      "Epoch 514/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0561 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0591\n",
      "Epoch 515/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0595 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0752\n",
      "Epoch 516/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0566 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0637\n",
      "Epoch 517/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0608 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0597\n",
      "Epoch 518/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0611 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0566\n",
      "Epoch 519/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0657 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0616\n",
      "Epoch 520/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0546 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0619\n",
      "Epoch 521/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0572 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0559\n",
      "Epoch 522/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0608 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0714\n",
      "Epoch 523/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0569 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0704\n",
      "Epoch 524/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0619 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0683\n",
      "Epoch 525/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0598 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0691\n",
      "Epoch 526/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0625 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0650\n",
      "Epoch 527/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0611 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0645\n",
      "Epoch 528/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0608 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0509\n",
      "Epoch 529/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0599 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0615\n",
      "Epoch 530/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0618 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0578\n",
      "Epoch 531/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0617 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0525\n",
      "Epoch 532/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0659 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0553\n",
      "Epoch 533/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0616 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0596\n",
      "Epoch 534/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0565 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0530\n",
      "Epoch 535/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0601 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0522\n",
      "Epoch 536/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0611 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0613\n",
      "Epoch 537/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0623 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0685\n",
      "Epoch 538/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0634 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0552\n",
      "Epoch 539/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0659\n",
      "Epoch 540/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0605 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0676\n",
      "Epoch 541/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0668 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0700\n",
      "Epoch 542/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0657 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 543/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - mae: 0.0660 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0620\n",
      "Epoch 544/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0540 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0507\n",
      "Epoch 545/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0652 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0590\n",
      "Epoch 546/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0601 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0585\n",
      "Epoch 547/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0617 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0555\n",
      "Epoch 548/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0546 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0486\n",
      "Epoch 549/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0602 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0580\n",
      "Epoch 550/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0746 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0697 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0605\n",
      "Epoch 552/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0623 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0854\n",
      "Epoch 553/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0582 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0567\n",
      "Epoch 554/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0592 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0566\n",
      "Epoch 555/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0591 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0517\n",
      "Epoch 556/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0610 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0635\n",
      "Epoch 557/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0595 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0485\n",
      "Epoch 558/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0658 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0530\n",
      "Epoch 559/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0637 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0610\n",
      "Epoch 560/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0627 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0763\n",
      "Epoch 561/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0621 - val_loss: 0.0074 - val_mse: 0.0074 - val_mae: 0.0699\n",
      "Epoch 562/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0653 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0519\n",
      "Epoch 563/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0587 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0581\n",
      "Epoch 564/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0578 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0596\n",
      "Epoch 565/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0571 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0707\n",
      "Epoch 566/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0607 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0665\n",
      "Epoch 567/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0584 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0635\n",
      "Epoch 568/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0598 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0801\n",
      "Epoch 569/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0566 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0656\n",
      "Epoch 570/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0622 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0590\n",
      "Epoch 571/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0552 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0649\n",
      "Epoch 572/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0597 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0671\n",
      "Epoch 573/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0553 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0603\n",
      "Epoch 574/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0602 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0565\n",
      "Epoch 575/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0597 - val_loss: 0.0078 - val_mse: 0.0078 - val_mae: 0.0715\n",
      "Epoch 576/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0599 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0732\n",
      "Epoch 577/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0754 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0777\n",
      "Epoch 578/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0761 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0649\n",
      "Epoch 579/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0756 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0569\n",
      "Epoch 580/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0703 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0673\n",
      "Epoch 581/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0701 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0562\n",
      "Epoch 582/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0642 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0560\n",
      "Epoch 583/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0655 - val_loss: 0.0073 - val_mse: 0.0073 - val_mae: 0.0708\n",
      "Epoch 584/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0624 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0632\n",
      "Epoch 585/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0649 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0662\n",
      "Epoch 586/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0573 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 587/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0601 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0563\n",
      "Epoch 588/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0602 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0551\n",
      "Epoch 589/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0594 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0552\n",
      "Epoch 590/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0604 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0620\n",
      "Epoch 591/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0629 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0543\n",
      "Epoch 592/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0689 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0869\n",
      "Epoch 593/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0633 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0820\n",
      "Epoch 594/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0717 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0575\n",
      "Epoch 595/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0736 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0662\n",
      "Epoch 596/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0681 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0534\n",
      "Epoch 597/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0649 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0546\n",
      "Epoch 598/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0607 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0597\n",
      "Epoch 599/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0570 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0548\n",
      "Epoch 600/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0571 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0554 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0568\n",
      "Epoch 602/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0640 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0582\n",
      "Epoch 603/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0603 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0571\n",
      "Epoch 604/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0632 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0540\n",
      "Epoch 605/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0557 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0593\n",
      "Epoch 606/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0576 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0675\n",
      "Epoch 607/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0554 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0609\n",
      "Epoch 608/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0621 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 609/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0578 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0640\n",
      "Epoch 610/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0605 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0556\n",
      "Epoch 611/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0593 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0742\n",
      "Epoch 612/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0600 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0628\n",
      "Epoch 613/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0600 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0620\n",
      "Epoch 614/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0579 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0699\n",
      "Epoch 615/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0729 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0553\n",
      "Epoch 616/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0606 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0650\n",
      "Epoch 617/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0585 - val_loss: 0.0125 - val_mse: 0.0125 - val_mae: 0.0968\n",
      "Epoch 618/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0724 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0803\n",
      "Epoch 619/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0762 - val_loss: 0.0204 - val_mse: 0.0204 - val_mae: 0.1146\n",
      "Epoch 620/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0762 - val_loss: 0.0060 - val_mse: 0.0060 - val_mae: 0.0634\n",
      "Epoch 621/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0635 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0577\n",
      "Epoch 622/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0631 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0562\n",
      "Epoch 623/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - mae: 0.0640 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0553\n",
      "Epoch 624/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0626 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0659\n",
      "Epoch 625/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0570 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0605\n",
      "Epoch 626/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0561 - val_loss: 0.0061 - val_mse: 0.0061 - val_mae: 0.0649\n",
      "Epoch 627/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0603 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0602\n",
      "Epoch 628/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0617 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0584\n",
      "Epoch 629/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0618 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0593\n",
      "Epoch 630/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0574 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0513\n",
      "Epoch 631/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0563 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0571\n",
      "Epoch 632/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0558 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0591\n",
      "Epoch 633/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0586 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0564\n",
      "Epoch 634/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0588 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0593\n",
      "Epoch 635/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0585 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0581\n",
      "Epoch 636/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0603 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0543\n",
      "Epoch 637/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0545\n",
      "Epoch 638/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0580 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0604\n",
      "Epoch 639/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0561 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0603\n",
      "Epoch 640/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0599 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0604\n",
      "Epoch 641/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0534\n",
      "Epoch 642/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0596 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0623\n",
      "Epoch 643/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0676 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0729\n",
      "Epoch 644/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0584 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0545\n",
      "Epoch 645/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0581 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0816\n",
      "Epoch 646/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0645 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0636\n",
      "Epoch 647/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0588 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0605\n",
      "Epoch 648/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0590 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0632\n",
      "Epoch 649/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0636 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0810\n",
      "Epoch 650/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0587 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 651/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0631 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0580\n",
      "Epoch 652/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0771 - val_loss: 0.0125 - val_mse: 0.0125 - val_mae: 0.0945\n",
      "Epoch 653/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0690 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0504\n",
      "Epoch 654/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - mae: 0.0740 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0522\n",
      "Epoch 655/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0620 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0620\n",
      "Epoch 656/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0579 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0589\n",
      "Epoch 657/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0598 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0590\n",
      "Epoch 658/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0621 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0581\n",
      "Epoch 659/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0622 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0555\n",
      "Epoch 660/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0583 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0562\n",
      "Epoch 661/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0590 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0614\n",
      "Epoch 662/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0581 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0613\n",
      "Epoch 663/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0575 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0714\n",
      "Epoch 664/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0673 - val_loss: 0.0129 - val_mse: 0.0129 - val_mae: 0.0912\n",
      "Epoch 665/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0737 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0728\n",
      "Epoch 666/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0747 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0579\n",
      "Epoch 667/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0652 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0725\n",
      "Epoch 668/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0659 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0671\n",
      "Epoch 669/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0605 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0618\n",
      "Epoch 670/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0565 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0639\n",
      "Epoch 671/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0564 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0607\n",
      "Epoch 672/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0553 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0630\n",
      "Epoch 673/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0605 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0548\n",
      "Epoch 674/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0583 - val_loss: 0.0057 - val_mse: 0.0057 - val_mae: 0.0629\n",
      "Epoch 675/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0572 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0606\n",
      "Epoch 676/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0634 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0593\n",
      "Epoch 677/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0592 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0528\n",
      "Epoch 678/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0533 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0595\n",
      "Epoch 679/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0554 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0529\n",
      "Epoch 680/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0552 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0693\n",
      "Epoch 681/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0560 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0551\n",
      "Epoch 682/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0543 - val_loss: 0.0064 - val_mse: 0.0064 - val_mae: 0.0644\n",
      "Epoch 683/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0589 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0572\n",
      "Epoch 684/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0609 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0599\n",
      "Epoch 685/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0629 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0621\n",
      "Epoch 686/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0576 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0560\n",
      "Epoch 687/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0591 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0560\n",
      "Epoch 688/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0615 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0580\n",
      "Epoch 689/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0593 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0649\n",
      "Epoch 690/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0612 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0539\n",
      "Epoch 691/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0540 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0539\n",
      "Epoch 692/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0576 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0597\n",
      "Epoch 693/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0567 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0550\n",
      "Epoch 694/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0578 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0551\n",
      "Epoch 695/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0565 - val_loss: 0.0059 - val_mse: 0.0059 - val_mae: 0.0640\n",
      "Epoch 696/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0577 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0548\n",
      "Epoch 697/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0598 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0541\n",
      "Epoch 698/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0618 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0675\n",
      "Epoch 699/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0589 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0571\n",
      "Epoch 700/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0589 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0554\n"
     ]
    }
   ],
   "source": [
    "history_model_gridsearched = model_gridsearched.fit(X_test_scaled, y_test_scaled, epochs=700, batch_size=10,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "offshore-community",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(history.history.keys())\n",
    "plt.plot(history_model_gridsearched.history['loss'])\n",
    "plt.plot(history_model_gridsearched.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "reasonable-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10399740934371948,\n",
       " 0.03253186121582985,\n",
       " 0.023053597658872604,\n",
       " 0.029644176363945007,\n",
       " 0.02225816808640957,\n",
       " 0.019718622788786888,\n",
       " 0.017929518595337868,\n",
       " 0.013369293883442879,\n",
       " 0.012996506877243519,\n",
       " 0.013857857324182987,\n",
       " 0.014434670098125935,\n",
       " 0.010739844292402267,\n",
       " 0.012159669771790504,\n",
       " 0.010768739506602287,\n",
       " 0.011763906106352806,\n",
       " 0.009809491224586964,\n",
       " 0.010180520825088024,\n",
       " 0.011213229037821293,\n",
       " 0.010517808608710766,\n",
       " 0.0131069365888834,\n",
       " 0.012181895785033703,\n",
       " 0.012774921953678131,\n",
       " 0.010782976634800434,\n",
       " 0.009843857027590275,\n",
       " 0.010036194697022438,\n",
       " 0.011370913125574589,\n",
       " 0.008828938007354736,\n",
       " 0.011360861361026764,\n",
       " 0.009361164644360542,\n",
       " 0.008612081408500671,\n",
       " 0.012462437152862549,\n",
       " 0.010995544493198395,\n",
       " 0.00849576573818922,\n",
       " 0.009159090928733349,\n",
       " 0.011231154203414917,\n",
       " 0.00923752412199974,\n",
       " 0.011333287693560123,\n",
       " 0.02374911494553089,\n",
       " 0.01521648745983839,\n",
       " 0.010950444266200066,\n",
       " 0.014444834552705288,\n",
       " 0.008674785494804382,\n",
       " 0.014833337627351284,\n",
       " 0.010262202471494675,\n",
       " 0.00928662158548832,\n",
       " 0.008898445405066013,\n",
       " 0.00856054574251175,\n",
       " 0.007112044375389814,\n",
       " 0.00831551942974329,\n",
       " 0.008093376643955708,\n",
       " 0.006405675318092108,\n",
       " 0.007938697002828121,\n",
       " 0.007222815416753292,\n",
       " 0.007650041487067938,\n",
       " 0.00792512483894825,\n",
       " 0.007531232666224241,\n",
       " 0.010332945734262466,\n",
       " 0.0067211478017270565,\n",
       " 0.008989659138023853,\n",
       " 0.008151148445904255,\n",
       " 0.011204776354134083,\n",
       " 0.007277253549546003,\n",
       " 0.005908711347728968,\n",
       " 0.00754118524491787,\n",
       " 0.005502756219357252,\n",
       " 0.006574634462594986,\n",
       " 0.00880223698914051,\n",
       " 0.008800451643764973,\n",
       " 0.006956474389880896,\n",
       " 0.006055103149265051,\n",
       " 0.007845030166208744,\n",
       " 0.007431193254888058,\n",
       " 0.007602998986840248,\n",
       " 0.00860751885920763,\n",
       " 0.00642873952165246,\n",
       " 0.006159092299640179,\n",
       " 0.00615811487659812,\n",
       " 0.008011594414710999,\n",
       " 0.01078332494944334,\n",
       " 0.007601125631481409,\n",
       " 0.007718857377767563,\n",
       " 0.009312939830124378,\n",
       " 0.00725353229790926,\n",
       " 0.00672518415376544,\n",
       " 0.010807158425450325,\n",
       " 0.010594473220407963,\n",
       " 0.008209464140236378,\n",
       " 0.011922158300876617,\n",
       " 0.008476706221699715,\n",
       " 0.006703963968902826,\n",
       " 0.007749191019684076,\n",
       " 0.006133547052741051,\n",
       " 0.009325725957751274,\n",
       " 0.006472880952060223,\n",
       " 0.005912747234106064,\n",
       " 0.00525858998298645,\n",
       " 0.006133709102869034,\n",
       " 0.007356771733611822,\n",
       " 0.007403267081826925,\n",
       " 0.01072695106267929,\n",
       " 0.008374751545488834,\n",
       " 0.007312359753996134,\n",
       " 0.010282727889716625,\n",
       " 0.007474936079233885,\n",
       " 0.0058722482062876225,\n",
       " 0.007006086874753237,\n",
       " 0.006587037350982428,\n",
       " 0.00577942281961441,\n",
       " 0.005837754812091589,\n",
       " 0.006598582491278648,\n",
       " 0.008623618632555008,\n",
       " 0.010666311718523502,\n",
       " 0.006969938054680824,\n",
       " 0.007568109314888716,\n",
       " 0.0074377162382006645,\n",
       " 0.010669497773051262,\n",
       " 0.007125315722078085,\n",
       " 0.005955717526376247,\n",
       " 0.00592237850651145,\n",
       " 0.00809120386838913,\n",
       " 0.011071196757256985,\n",
       " 0.007850362919270992,\n",
       " 0.006576427724212408,\n",
       " 0.008833643980324268,\n",
       " 0.006219458766281605,\n",
       " 0.007038634270429611,\n",
       " 0.005899503361433744,\n",
       " 0.005713475402444601,\n",
       " 0.006703547667711973,\n",
       " 0.007659791968762875,\n",
       " 0.006423912011086941,\n",
       " 0.0059636440128088,\n",
       " 0.007791733369231224,\n",
       " 0.012518397532403469,\n",
       " 0.012431034818291664,\n",
       " 0.007852072827517986,\n",
       " 0.00818472821265459,\n",
       " 0.008174307644367218,\n",
       " 0.008837753906846046,\n",
       " 0.0057678185403347015,\n",
       " 0.0066173444502055645,\n",
       " 0.006401411723345518,\n",
       " 0.00877033919095993,\n",
       " 0.006995917297899723,\n",
       " 0.006401050835847855,\n",
       " 0.006719623692333698,\n",
       " 0.006130973342806101,\n",
       " 0.006254164036363363,\n",
       " 0.007489860989153385,\n",
       " 0.00896634254604578,\n",
       " 0.005712979938834906,\n",
       " 0.007152747828513384,\n",
       " 0.0064046685583889484,\n",
       " 0.00594348507001996,\n",
       " 0.006204990670084953,\n",
       " 0.006355857942253351,\n",
       " 0.006915545091032982,\n",
       " 0.0062895044684410095,\n",
       " 0.0060106427408754826,\n",
       " 0.00655134953558445,\n",
       " 0.006643688306212425,\n",
       " 0.006823411677032709,\n",
       " 0.007349781226366758,\n",
       " 0.0063779945485293865,\n",
       " 0.007684848736971617,\n",
       " 0.005724185146391392,\n",
       " 0.006020132452249527,\n",
       " 0.009069569408893585,\n",
       " 0.00724945729598403,\n",
       " 0.006264170631766319,\n",
       " 0.006948034279048443,\n",
       " 0.005404247436672449,\n",
       " 0.005723214708268642,\n",
       " 0.007314333692193031,\n",
       " 0.006234228610992432,\n",
       " 0.0074465228244662285,\n",
       " 0.009033354930579662,\n",
       " 0.006904443725943565,\n",
       " 0.0072146193124353886,\n",
       " 0.008302517235279083,\n",
       " 0.005946687422692776,\n",
       " 0.0067018927074968815,\n",
       " 0.00630256999284029,\n",
       " 0.007641317788511515,\n",
       " 0.00686611607670784,\n",
       " 0.007214040961116552,\n",
       " 0.006422358099371195,\n",
       " 0.005898608826100826,\n",
       " 0.006528602447360754,\n",
       " 0.0069685764610767365,\n",
       " 0.005507105961441994,\n",
       " 0.007260194513946772,\n",
       " 0.007317116484045982,\n",
       " 0.006866612005978823,\n",
       " 0.006011594086885452,\n",
       " 0.0059454855509102345,\n",
       " 0.006572347600013018,\n",
       " 0.006333040539175272,\n",
       " 0.007649174425750971,\n",
       " 0.007132418919354677,\n",
       " 0.006055239122360945,\n",
       " 0.006138143595308065,\n",
       " 0.006650829222053289,\n",
       " 0.006940056569874287,\n",
       " 0.007084651384502649,\n",
       " 0.0058954027481377125,\n",
       " 0.0060388497076928616,\n",
       " 0.005827838554978371,\n",
       " 0.005242473445832729,\n",
       " 0.006541528273373842,\n",
       " 0.005105104297399521,\n",
       " 0.005803736392408609,\n",
       " 0.0050254398956894875,\n",
       " 0.007450139615684748,\n",
       " 0.0054139937274158,\n",
       " 0.005438938271254301,\n",
       " 0.007742044050246477,\n",
       " 0.006757249124348164,\n",
       " 0.006973650306463242,\n",
       " 0.007871508598327637,\n",
       " 0.007387849036604166,\n",
       " 0.006933894474059343,\n",
       " 0.007214751560240984,\n",
       " 0.00619548000395298,\n",
       " 0.007204163353890181,\n",
       " 0.008798414841294289,\n",
       " 0.009080206044018269,\n",
       " 0.008628979325294495,\n",
       " 0.008380693383514881,\n",
       " 0.006278492044657469,\n",
       " 0.006610529031604528,\n",
       " 0.005870121996849775,\n",
       " 0.005387473851442337,\n",
       " 0.006409119814634323,\n",
       " 0.006441410630941391,\n",
       " 0.007386712823063135,\n",
       " 0.012031227350234985,\n",
       " 0.007465803530067205,\n",
       " 0.007671946194022894,\n",
       " 0.006257994566112757,\n",
       " 0.006855372339487076,\n",
       " 0.006425619591027498,\n",
       " 0.008315089158713818,\n",
       " 0.0072438702918589115,\n",
       " 0.005854832008481026,\n",
       " 0.006010629236698151,\n",
       " 0.006435364484786987,\n",
       " 0.005501698702573776,\n",
       " 0.006912678480148315,\n",
       " 0.008500316180288792,\n",
       " 0.007054740097373724,\n",
       " 0.009518958628177643,\n",
       " 0.007703872863203287,\n",
       " 0.006167049985378981,\n",
       " 0.005449255462735891,\n",
       " 0.007218560669571161,\n",
       " 0.006345332134515047,\n",
       " 0.006544761825352907,\n",
       " 0.006137392483651638,\n",
       " 0.006241867784410715,\n",
       " 0.005602316465228796,\n",
       " 0.006433006376028061,\n",
       " 0.007314600516110659,\n",
       " 0.006035973317921162,\n",
       " 0.007185400929301977,\n",
       " 0.005389571189880371,\n",
       " 0.006395738571882248,\n",
       " 0.0071487645618617535,\n",
       " 0.006351529620587826,\n",
       " 0.007290066219866276,\n",
       " 0.006903913337737322,\n",
       " 0.006299372762441635,\n",
       " 0.006376168224960566,\n",
       " 0.006027453113347292,\n",
       " 0.008286532945930958,\n",
       " 0.008797264657914639,\n",
       " 0.007489971350878477,\n",
       " 0.007348458282649517,\n",
       " 0.0061363293789327145,\n",
       " 0.006011609453707933,\n",
       " 0.006863292772322893,\n",
       " 0.004993745591491461,\n",
       " 0.006852374412119389,\n",
       " 0.013462011702358723,\n",
       " 0.008424111641943455,\n",
       " 0.007187778130173683,\n",
       " 0.007241455372422934,\n",
       " 0.0068039181642234325,\n",
       " 0.005802958272397518,\n",
       " 0.008007142692804337,\n",
       " 0.007815246470272541,\n",
       " 0.006772782653570175,\n",
       " 0.00596546521410346,\n",
       " 0.006651784293353558,\n",
       " 0.006458653137087822,\n",
       " 0.0056414478458464146,\n",
       " 0.007267323788255453,\n",
       " 0.007208561524748802,\n",
       " 0.006918830331414938,\n",
       " 0.00748260784894228,\n",
       " 0.00609489344060421,\n",
       " 0.006284050177782774,\n",
       " 0.006648280192166567,\n",
       " 0.007218418177217245,\n",
       " 0.006837894208729267,\n",
       " 0.006812038365751505,\n",
       " 0.0069795330055058,\n",
       " 0.007623613812029362,\n",
       " 0.007305985316634178,\n",
       " 0.00647700484842062,\n",
       " 0.0062844716012477875,\n",
       " 0.007125548552721739,\n",
       " 0.005556902382522821,\n",
       " 0.006413561291992664,\n",
       " 0.006395949516445398,\n",
       " 0.008276832289993763,\n",
       " 0.007115498185157776,\n",
       " 0.005704703275114298,\n",
       " 0.008139361627399921,\n",
       " 0.006123713217675686,\n",
       " 0.00513513945043087,\n",
       " 0.005738192703574896,\n",
       " 0.006313750054687262,\n",
       " 0.005458056461066008,\n",
       " 0.007064977660775185,\n",
       " 0.0063886819407343864,\n",
       " 0.005398021545261145,\n",
       " 0.006348257884383202,\n",
       " 0.006618254352360964,\n",
       " 0.005816924385726452,\n",
       " 0.006695301737636328,\n",
       " 0.005349730607122183,\n",
       " 0.005283341743052006,\n",
       " 0.005392564460635185,\n",
       " 0.005759373772889376,\n",
       " 0.006213907152414322,\n",
       " 0.00662023201584816,\n",
       " 0.005332004278898239,\n",
       " 0.006357450038194656,\n",
       " 0.0061082723550498486,\n",
       " 0.006960649974644184,\n",
       " 0.006238097790628672,\n",
       " 0.005395152606070042,\n",
       " 0.0053379894234240055,\n",
       " 0.007842503488063812,\n",
       " 0.006744616199284792,\n",
       " 0.007263548206537962,\n",
       " 0.00729059474542737,\n",
       " 0.005815512500703335,\n",
       " 0.005923297721892595,\n",
       " 0.007920187897980213,\n",
       " 0.007982942275702953,\n",
       " 0.008510798215866089,\n",
       " 0.006300234701484442,\n",
       " 0.0076714144088327885,\n",
       " 0.00728520518168807,\n",
       " 0.005327933933585882,\n",
       " 0.0061722407117486,\n",
       " 0.006003436166793108,\n",
       " 0.006259390152990818,\n",
       " 0.005644151009619236,\n",
       " 0.006001717876642942,\n",
       " 0.00663216644898057,\n",
       " 0.007231107447296381,\n",
       " 0.0066647897474467754,\n",
       " 0.006500821094959974,\n",
       " 0.006261293776333332,\n",
       " 0.006332267075777054,\n",
       " 0.006270855665206909,\n",
       " 0.0066033522598445415,\n",
       " 0.0060009886510670185,\n",
       " 0.005953910294920206,\n",
       " 0.005069478880614042,\n",
       " 0.0054384153336286545,\n",
       " 0.006347693037241697,\n",
       " 0.005732453893870115,\n",
       " 0.005566535983234644,\n",
       " 0.006238235160708427,\n",
       " 0.005863029509782791,\n",
       " 0.007086861878633499,\n",
       " 0.005604918580502272,\n",
       " 0.006225474178791046,\n",
       " 0.005418088287115097,\n",
       " 0.005265009589493275,\n",
       " 0.005011237692087889,\n",
       " 0.005361540708690882,\n",
       " 0.006204442586749792,\n",
       " 0.0076854778453707695,\n",
       " 0.006492775399237871,\n",
       " 0.006913392338901758,\n",
       " 0.006022765301167965,\n",
       " 0.005931969732046127,\n",
       " 0.006493380758911371,\n",
       " 0.005964500829577446,\n",
       " 0.00556016992777586,\n",
       " 0.006156945135444403,\n",
       " 0.005969496909528971,\n",
       " 0.008188311941921711,\n",
       " 0.006223528180271387,\n",
       " 0.0052801817655563354,\n",
       " 0.005493766162544489,\n",
       " 0.006352716591209173,\n",
       " 0.0057679093442857265,\n",
       " 0.007008295506238937,\n",
       " 0.005926450248807669,\n",
       " 0.006595727521926165,\n",
       " 0.005549997091293335,\n",
       " 0.005287521984428167,\n",
       " 0.005710577592253685,\n",
       " 0.006316630635410547,\n",
       " 0.006735420320183039,\n",
       " 0.01408489141613245,\n",
       " 0.01212139893323183,\n",
       " 0.012276925146579742,\n",
       " 0.013698039576411247,\n",
       " 0.009376819245517254,\n",
       " 0.009801843203604221,\n",
       " 0.0090358667075634,\n",
       " 0.007386559620499611,\n",
       " 0.007115301210433245,\n",
       " 0.00498249800875783,\n",
       " 0.006460968405008316,\n",
       " 0.005999377463012934,\n",
       " 0.006221291143447161,\n",
       " 0.006292353849858046,\n",
       " 0.006527801044285297,\n",
       " 0.005451093427836895,\n",
       " 0.006805133074522018,\n",
       " 0.006156858056783676,\n",
       " 0.00507504353299737,\n",
       " 0.005865423008799553,\n",
       " 0.00607238058000803,\n",
       " 0.006037319079041481,\n",
       " 0.006413915194571018,\n",
       " 0.005693007726222277,\n",
       " 0.006459962110966444,\n",
       " 0.009768285788595676,\n",
       " 0.008942462503910065,\n",
       " 0.008459986187517643,\n",
       " 0.006905783899128437,\n",
       " 0.0068680355325341225,\n",
       " 0.007583886384963989,\n",
       " 0.0065740058198571205,\n",
       " 0.005323643330484629,\n",
       " 0.005521321203559637,\n",
       " 0.005687972530722618,\n",
       " 0.007864928804337978,\n",
       " 0.006432991474866867,\n",
       " 0.0062775821425020695,\n",
       " 0.005306488834321499,\n",
       " 0.005322643090039492,\n",
       " 0.004957543220371008,\n",
       " 0.006258036475628614,\n",
       " 0.005734987091273069,\n",
       " 0.0051803006790578365,\n",
       " 0.004769324790686369,\n",
       " 0.00582728860899806,\n",
       " 0.007199571933597326,\n",
       " 0.007039661519229412,\n",
       " 0.006602717097848654,\n",
       " 0.006544892210513353,\n",
       " 0.0069342381320893764,\n",
       " 0.00577527517452836,\n",
       " 0.006111240014433861,\n",
       " 0.007173394784331322,\n",
       " 0.005952035542577505,\n",
       " 0.006858983542770147,\n",
       " 0.006383682135492563,\n",
       " 0.005859619937837124,\n",
       " 0.00740374531596899,\n",
       " 0.009343626908957958,\n",
       " 0.007507058326154947,\n",
       " 0.006747295148670673,\n",
       " 0.0053249262273311615,\n",
       " 0.005990203004330397,\n",
       " 0.005426474846899509,\n",
       " 0.005360117647796869,\n",
       " 0.006285648327320814,\n",
       " 0.006006208714097738,\n",
       " 0.006877899169921875,\n",
       " 0.005346745252609253,\n",
       " 0.006600357126444578,\n",
       " 0.006271837279200554,\n",
       " 0.0068070716224610806,\n",
       " 0.007094874978065491,\n",
       " 0.0055475542321801186,\n",
       " 0.00565305957570672,\n",
       " 0.00624730484560132,\n",
       " 0.006654386408627033,\n",
       " 0.006183854769915342,\n",
       " 0.005808320362120867,\n",
       " 0.0068532563745975494,\n",
       " 0.006350369658321142,\n",
       " 0.005668165162205696,\n",
       " 0.005826620850712061,\n",
       " 0.006119119469076395,\n",
       " 0.0070403446443378925,\n",
       " 0.005472205579280853,\n",
       " 0.006235118955373764,\n",
       " 0.005762035027146339,\n",
       " 0.005726072937250137,\n",
       " 0.005467222072184086,\n",
       " 0.005654645152390003,\n",
       " 0.0053685931488871574,\n",
       " 0.006943164858967066,\n",
       " 0.006021525710821152,\n",
       " 0.005636143032461405,\n",
       " 0.005787171423435211,\n",
       " 0.0074543943628668785,\n",
       " 0.0059165651910007,\n",
       " 0.005972667597234249,\n",
       " 0.0067438771948218346,\n",
       " 0.006637612357735634,\n",
       " 0.0053163752891123295,\n",
       " 0.005479422863572836,\n",
       " 0.005694832652807236,\n",
       " 0.00646475562825799,\n",
       " 0.006022653076797724,\n",
       " 0.007218808867037296,\n",
       " 0.005030433181673288,\n",
       " 0.005359537899494171,\n",
       " 0.006014927756041288,\n",
       " 0.005605542100965977,\n",
       " 0.005886792670935392,\n",
       " 0.006045945454388857,\n",
       " 0.006349862553179264,\n",
       " 0.006146382540464401,\n",
       " 0.0059220376424491405,\n",
       " 0.005591821391135454,\n",
       " 0.006225159857422113,\n",
       " 0.006315296981483698,\n",
       " 0.006789421662688255,\n",
       " 0.006161291617900133,\n",
       " 0.005617605056613684,\n",
       " 0.005812693387269974,\n",
       " 0.006252671591937542,\n",
       " 0.006664230488240719,\n",
       " 0.006751054432243109,\n",
       " 0.005865718238055706,\n",
       " 0.006145112682133913,\n",
       " 0.007548660971224308,\n",
       " 0.006801696959882975,\n",
       " 0.0074715702794492245,\n",
       " 0.005393871571868658,\n",
       " 0.0070572057738900185,\n",
       " 0.006183640565723181,\n",
       " 0.006506443955004215,\n",
       " 0.004740203730762005,\n",
       " 0.006187222432345152,\n",
       " 0.009214834310114384,\n",
       " 0.007910165004432201,\n",
       " 0.006326403468847275,\n",
       " 0.005441103130578995,\n",
       " 0.005772349890321493,\n",
       " 0.005464746151119471,\n",
       " 0.006095737684518099,\n",
       " 0.005583126097917557,\n",
       " 0.006813704967498779,\n",
       " 0.006632933393120766,\n",
       " 0.0064170099794864655,\n",
       " 0.00618767412379384,\n",
       " 0.00684767821803689,\n",
       " 0.005784045439213514,\n",
       " 0.005548353306949139,\n",
       " 0.005581317003816366,\n",
       " 0.006402435712516308,\n",
       " 0.005845817271620035,\n",
       " 0.0062925973907113075,\n",
       " 0.005988049320876598,\n",
       " 0.006202861201018095,\n",
       " 0.005198832601308823,\n",
       " 0.006058828439563513,\n",
       " 0.005266502499580383,\n",
       " 0.0057926177978515625,\n",
       " 0.006184804253280163,\n",
       " 0.005716408137232065,\n",
       " 0.010014777071774006,\n",
       " 0.00919223390519619,\n",
       " 0.008907991461455822,\n",
       " 0.007721988949924707,\n",
       " 0.008336101658642292,\n",
       " 0.007362571079283953,\n",
       " 0.007102967705577612,\n",
       " 0.006536450702697039,\n",
       " 0.007041104603558779,\n",
       " 0.00521521270275116,\n",
       " 0.0059562972746789455,\n",
       " 0.005730737466365099,\n",
       " 0.005934888496994972,\n",
       " 0.0064330510795116425,\n",
       " 0.006640363484621048,\n",
       " 0.008078077808022499,\n",
       " 0.006693167146295309,\n",
       " 0.008067481219768524,\n",
       " 0.008420457132160664,\n",
       " 0.00737400446087122,\n",
       " 0.00697083305567503,\n",
       " 0.0058826240710914135,\n",
       " 0.0051958682015538216,\n",
       " 0.005168561358004808,\n",
       " 0.00575193902477622,\n",
       " 0.006597208324819803,\n",
       " 0.006168255116790533,\n",
       " 0.006843009497970343,\n",
       " 0.005134882405400276,\n",
       " 0.005315886810421944,\n",
       " 0.005342902150005102,\n",
       " 0.006749991327524185,\n",
       " 0.005420425441116095,\n",
       " 0.00570850120857358,\n",
       " 0.0062795705161988735,\n",
       " 0.005997261498123407,\n",
       " 0.00602361187338829,\n",
       " 0.005862521473318338,\n",
       " 0.0085753183811903,\n",
       " 0.00628224341198802,\n",
       " 0.005843774881213903,\n",
       " 0.008255116641521454,\n",
       " 0.009017450734972954,\n",
       " 0.010685459710657597,\n",
       " 0.006704164203256369,\n",
       " 0.006606977432966232,\n",
       " 0.006990337278693914,\n",
       " 0.00706647802144289,\n",
       " 0.005753133445978165,\n",
       " 0.005040714982897043,\n",
       " 0.005527976434677839,\n",
       " 0.006374566815793514,\n",
       " 0.006773583125323057,\n",
       " 0.005513662006705999,\n",
       " 0.005304189398884773,\n",
       " 0.0053210388869047165,\n",
       " 0.005564011167734861,\n",
       " 0.006711484398692846,\n",
       " 0.005324129015207291,\n",
       " 0.005878663156181574,\n",
       " 0.006156533025205135,\n",
       " 0.005652995314449072,\n",
       " 0.005000632256269455,\n",
       " 0.005937103647738695,\n",
       " 0.005930432118475437,\n",
       " 0.006060339510440826,\n",
       " 0.007113566622138023,\n",
       " 0.005990481935441494,\n",
       " 0.005900809075683355,\n",
       " 0.006949465721845627,\n",
       " 0.005703729577362537,\n",
       " 0.005826221313327551,\n",
       " 0.006880486384034157,\n",
       " 0.005652552470564842,\n",
       " 0.006536883767694235,\n",
       " 0.009656678885221481,\n",
       " 0.008113667368888855,\n",
       " 0.008683483116328716,\n",
       " 0.006469918880611658,\n",
       " 0.005731848068535328,\n",
       " 0.005580138880759478,\n",
       " 0.006199422292411327,\n",
       " 0.006610579323023558,\n",
       " 0.005708026234060526,\n",
       " 0.005724701099097729,\n",
       " 0.005752920173108578,\n",
       " 0.005354549270123243,\n",
       " 0.008308467455208302,\n",
       " 0.009003874845802784,\n",
       " 0.009381315670907497,\n",
       " 0.007217008620500565,\n",
       " 0.0069184936583042145,\n",
       " 0.005780784413218498,\n",
       " 0.005417733918875456,\n",
       " 0.005443968810141087,\n",
       " 0.005408697295933962,\n",
       " 0.005900675896555185,\n",
       " 0.005555858835577965,\n",
       " 0.005627875216305256,\n",
       " 0.006620289292186499,\n",
       " 0.005840768106281757,\n",
       " 0.004743203520774841,\n",
       " 0.005026490893214941,\n",
       " 0.005126606207340956,\n",
       " 0.005160685628652573,\n",
       " 0.004881890490651131,\n",
       " 0.005936081055551767,\n",
       " 0.006313235033303499,\n",
       " 0.006574992090463638,\n",
       " 0.005950212012976408,\n",
       " 0.005639178212732077,\n",
       " 0.006097090430557728,\n",
       " 0.005827423185110092,\n",
       " 0.006083337590098381,\n",
       " 0.0052485717460513115,\n",
       " 0.00565869826823473,\n",
       " 0.005404196213930845,\n",
       " 0.005783515051007271,\n",
       " 0.00541371013969183,\n",
       " 0.005736516322940588,\n",
       " 0.005528255831450224,\n",
       " 0.006361765321344137,\n",
       " 0.005461778491735458,\n",
       " 0.0054519460536539555]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_model_gridsearched.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-monkey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-madagascar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "transsexual-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = X_test.values.astype(int)\n",
    "y_test_np = y_test.values.astype(int)\n",
    "X_test_np = convert_to_tensor(X_test_np)\n",
    "y_test_np = convert_to_tensor(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "legendary-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 5.4109 - mse: 5.4109 - mae: 2.1248\n",
      "Mean Squared Error : 5.41093\n",
      "Mean Absolute Error : 2.12480\n"
     ]
    }
   ],
   "source": [
    "loss, mean_sq_e, mean_abs_e = model_gridsearched.evaluate(X_test_np, y_test_np)\n",
    "\n",
    "print(\"Mean Squared Error : {:.5f}\".format(mean_sq_e))\n",
    "print(\"Mean Absolute Error : {:.5f}\".format(mean_abs_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "about-energy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.410933017730713"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-sympathy",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eleven-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted G3's score is 14.02\n"
     ]
    }
   ],
   "source": [
    "df2 = features.iloc[2] # Test with the 14th Student \n",
    "Xnew = np.array([df2])\n",
    "Xnew.reshape(-1, 1)\n",
    "\n",
    "Xnew= scaler.transform(Xnew)\n",
    "ynew= model_gridsearched.predict(Xnew)\n",
    "\n",
    "# #invert normalize\n",
    "ynew = scaler.inverse_transform(ynew) \n",
    "Xnew = scaler.inverse_transform(Xnew)\n",
    "\n",
    "#print(\"X=%s, Predicted=%s\" % (Xnew[0], ynew[0]))\n",
    "print(\"Predicted G3's score is {:.2f}\".format(float(ynew)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "foreign-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_gridsearched.predict(X_test_scaled)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions = np.round(predictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "governing-wyoming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.17\n",
      "Difference : 0.170\n",
      "1\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.09\n",
      "Difference : 0.090\n",
      "2\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.32\n",
      "Difference : 0.680\n",
      "3\n",
      "Real Values : 19.0\n",
      "Predicted G3's score is 18.35\n",
      "Difference : 0.650\n",
      "4\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.53\n",
      "Difference : 0.530\n",
      "5\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 16.14\n",
      "Difference : 1.140\n",
      "6\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "7\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.17\n",
      "Difference : 1.170\n",
      "8\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.05\n",
      "Difference : 1.050\n",
      "9\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.96\n",
      "Difference : 0.040\n",
      "10\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.82\n",
      "Difference : 0.180\n",
      "11\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.47\n",
      "Difference : 0.470\n",
      "12\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.87\n",
      "Difference : 0.870\n",
      "13\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.41\n",
      "Difference : 0.590\n",
      "14\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.32\n",
      "Difference : 0.320\n",
      "15\n",
      "Real Values : 6.0\n",
      "Predicted G3's score is 7.78\n",
      "Difference : 1.780\n",
      "16\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.01\n",
      "Difference : 0.990\n",
      "17\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.02\n",
      "Difference : 0.020\n",
      "18\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 15.25\n",
      "Difference : 1.250\n",
      "19\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.61\n",
      "Difference : 0.390\n",
      "20\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.69\n",
      "Difference : 0.690\n",
      "21\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.54\n",
      "Difference : 0.460\n",
      "22\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.70\n",
      "Difference : 0.300\n",
      "23\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.44\n",
      "Difference : 0.560\n",
      "24\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.49\n",
      "Difference : 0.490\n",
      "25\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.59\n",
      "Difference : 0.410\n",
      "26\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.86\n",
      "Difference : 0.140\n",
      "27\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 11.93\n",
      "Difference : 1.070\n",
      "28\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.70\n",
      "Difference : 0.300\n",
      "29\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.44\n",
      "Difference : 0.440\n",
      "30\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.92\n",
      "Difference : 0.920\n",
      "31\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.38\n",
      "Difference : 0.620\n",
      "32\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 9.16\n",
      "Difference : 1.160\n",
      "33\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 13.07\n",
      "Difference : 1.930\n",
      "34\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.68\n",
      "Difference : 0.320\n",
      "35\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.18\n",
      "Difference : 0.820\n",
      "36\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.93\n",
      "Difference : 1.070\n",
      "37\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.59\n",
      "Difference : 0.590\n",
      "38\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "39\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.25\n",
      "Difference : 0.250\n",
      "40\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.19\n",
      "Difference : 0.190\n",
      "41\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 17.94\n",
      "Difference : 0.940\n",
      "42\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.96\n",
      "Difference : 0.040\n",
      "43\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.90\n",
      "Difference : 0.900\n",
      "44\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 11.17\n",
      "Difference : 1.170\n",
      "45\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.92\n",
      "Difference : 0.920\n",
      "46\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.56\n",
      "Difference : 1.440\n",
      "47\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.82\n",
      "Difference : 0.180\n",
      "48\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.24\n",
      "Difference : 0.760\n",
      "49\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.52\n",
      "Difference : 0.480\n",
      "50\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 17.41\n",
      "Difference : 0.410\n",
      "51\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.08\n",
      "Difference : 0.080\n",
      "52\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.28\n",
      "Difference : 0.280\n",
      "53\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.32\n",
      "Difference : 0.320\n",
      "54\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.64\n",
      "Difference : 0.640\n",
      "55\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.69\n",
      "Difference : 0.310\n",
      "56\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 18.07\n",
      "Difference : 1.070\n",
      "57\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.57\n",
      "Difference : 0.570\n",
      "58\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.40\n",
      "Difference : 0.400\n",
      "59\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.83\n",
      "Difference : 0.170\n",
      "60\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.33\n",
      "Difference : 0.670\n",
      "61\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 8.91\n",
      "Difference : 1.090\n",
      "62\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.07\n",
      "Difference : 0.930\n",
      "63\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.63\n",
      "Difference : 0.630\n",
      "64\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.87\n",
      "Difference : 0.870\n",
      "65\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.630\n",
      "66\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.76\n",
      "Difference : 0.240\n",
      "67\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.55\n",
      "Difference : 0.450\n",
      "68\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.73\n",
      "Difference : 0.270\n",
      "69\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.71\n",
      "Difference : 0.290\n",
      "70\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.81\n",
      "Difference : 0.810\n",
      "71\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.04\n",
      "Difference : 0.040\n",
      "72\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.05\n",
      "Difference : 0.050\n",
      "73\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.64\n",
      "Difference : 0.360\n",
      "74\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 14.69\n",
      "Difference : 1.310\n",
      "75\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 18.00\n",
      "Difference : 0.000\n",
      "76\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.63\n",
      "Difference : 0.370\n",
      "77\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.59\n",
      "Difference : 0.590\n",
      "78\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.40\n",
      "Difference : 0.600\n",
      "79\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.68\n",
      "Difference : 0.320\n",
      "80\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.370\n",
      "81\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 17.12\n",
      "Difference : 0.880\n",
      "82\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.70\n",
      "Difference : 0.700\n",
      "83\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 12.22\n",
      "Difference : 3.780\n",
      "84\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.44\n",
      "Difference : 0.560\n",
      "85\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.23\n",
      "Difference : 0.230\n",
      "86\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 18.24\n",
      "Difference : 0.240\n",
      "87\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.88\n",
      "Difference : 0.120\n",
      "88\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 11.51\n",
      "Difference : 1.510\n",
      "89\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.53\n",
      "Difference : 0.530\n",
      "90\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 0.310\n",
      "91\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.79\n",
      "Difference : 0.790\n",
      "92\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 17.07\n",
      "Difference : 1.070\n",
      "93\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.71\n",
      "Difference : 0.290\n",
      "94\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 13.58\n",
      "Difference : 1.580\n",
      "95\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.49\n",
      "Difference : 0.490\n",
      "96\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.59\n",
      "Difference : 0.410\n",
      "97\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.79\n",
      "Difference : 0.790\n",
      "98\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.26\n",
      "Difference : 0.740\n",
      "99\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.94\n",
      "Difference : 1.060\n",
      "100\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 9.12\n",
      "Difference : 1.120\n",
      "101\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.58\n",
      "Difference : 0.580\n",
      "102\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.63\n",
      "Difference : 0.630\n",
      "103\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.67\n",
      "Difference : 0.330\n",
      "104\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.12\n",
      "Difference : 0.120\n",
      "105\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 1.310\n",
      "106\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.63\n",
      "Difference : 0.630\n",
      "107\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.26\n",
      "Difference : 1.260\n",
      "108\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.81\n",
      "Difference : 0.810\n",
      "109\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.15\n",
      "Difference : 0.850\n",
      "110\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.08\n",
      "Difference : 0.920\n",
      "111\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.93\n",
      "Difference : 0.930\n",
      "112\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.43\n",
      "Difference : 0.570\n",
      "113\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.11\n",
      "Difference : 0.110\n",
      "114\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 14.85\n",
      "Difference : 1.150\n",
      "115\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.33\n",
      "Difference : 0.330\n",
      "116\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.97\n",
      "Difference : 0.970\n",
      "117\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.76\n",
      "Difference : 0.760\n",
      "118\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.45\n",
      "Difference : 0.450\n",
      "119\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.58\n",
      "Difference : 0.580\n",
      "120\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.65\n",
      "Difference : 0.650\n",
      "121\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.83\n",
      "Difference : 0.830\n",
      "122\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 14.56\n",
      "Difference : 2.440\n",
      "123\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.45\n",
      "Difference : 0.450\n",
      "124\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.63\n",
      "Difference : 0.630\n",
      "125\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.71\n",
      "Difference : 0.710\n",
      "126\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.06\n",
      "Difference : 0.060\n",
      "127\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.77\n",
      "Difference : 0.230\n",
      "128\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 11.91\n",
      "Difference : 1.090\n",
      "129\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.63\n",
      "Difference : 0.370\n",
      "130\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.25\n",
      "Difference : 0.750\n",
      "131\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.13\n",
      "Difference : 0.130\n",
      "132\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.78\n",
      "Difference : 0.780\n",
      "133\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.55\n",
      "Difference : 0.550\n",
      "134\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.18\n",
      "Difference : 0.180\n",
      "135\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.64\n",
      "Difference : 0.360\n",
      "136\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.63\n",
      "Difference : 0.630\n",
      "137\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.60\n",
      "Difference : 0.400\n",
      "138\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.28\n",
      "Difference : 1.720\n",
      "139\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.54\n",
      "Difference : 0.540\n",
      "140\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.90\n",
      "Difference : 0.900\n",
      "141\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 9.00\n",
      "Difference : 1.000\n",
      "142\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.36\n",
      "Difference : 0.360\n",
      "143\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.26\n",
      "Difference : 0.740\n",
      "144\n",
      "Real Values : 17.0\n",
      "Predicted G3's score is 16.75\n",
      "Difference : 0.250\n",
      "145\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.51\n",
      "Difference : 0.510\n",
      "146\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.53\n",
      "Difference : 0.530\n",
      "147\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 12.48\n",
      "Difference : 1.480\n",
      "148\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.47\n",
      "Difference : 0.530\n",
      "149\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.45\n",
      "Difference : 0.550\n",
      "150\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.27\n",
      "Difference : 0.270\n",
      "151\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.43\n",
      "Difference : 0.430\n",
      "152\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.07\n",
      "Difference : 0.930\n",
      "153\n",
      "Real Values : 7.0\n",
      "Predicted G3's score is 8.76\n",
      "Difference : 1.760\n",
      "154\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 16.71\n",
      "Difference : 1.290\n",
      "155\n",
      "Real Values : 6.0\n",
      "Predicted G3's score is 6.69\n",
      "Difference : 0.690\n",
      "156\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 12.48\n",
      "Difference : 1.520\n",
      "157\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.22\n",
      "Difference : 0.780\n",
      "158\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.34\n",
      "Difference : 0.660\n",
      "159\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 14.22\n",
      "Difference : 0.220\n",
      "160\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.42\n",
      "Difference : 0.580\n",
      "161\n",
      "Real Values : 18.0\n",
      "Predicted G3's score is 16.46\n",
      "Difference : 1.540\n",
      "162\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 15.12\n",
      "Difference : 0.880\n",
      "163\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.87\n",
      "Difference : 0.870\n",
      "164\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.31\n",
      "Difference : 0.690\n",
      "165\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 9.18\n",
      "Difference : 0.180\n",
      "166\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.22\n",
      "Difference : 0.220\n",
      "167\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.27\n",
      "Difference : 0.730\n",
      "168\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 12.94\n",
      "Difference : 0.060\n",
      "169\n",
      "Real Values : 16.0\n",
      "Predicted G3's score is 16.19\n",
      "Difference : 0.190\n",
      "170\n",
      "Real Values : 8.0\n",
      "Predicted G3's score is 8.45\n",
      "Difference : 0.450\n",
      "171\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.05\n",
      "Difference : 0.050\n",
      "172\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 13.38\n",
      "Difference : 0.380\n",
      "173\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.77\n",
      "Difference : 0.770\n",
      "174\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.52\n",
      "Difference : 0.520\n",
      "175\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.64\n",
      "Difference : 1.640\n",
      "176\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 15.59\n",
      "Difference : 0.590\n",
      "177\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.84\n",
      "Difference : 0.840\n",
      "178\n",
      "Real Values : 15.0\n",
      "Predicted G3's score is 14.55\n",
      "Difference : 0.450\n",
      "179\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.37\n",
      "Difference : 0.630\n",
      "180\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 12.12\n",
      "Difference : 0.120\n",
      "181\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.08\n",
      "Difference : 0.080\n",
      "182\n",
      "Real Values : 10.0\n",
      "Predicted G3's score is 10.25\n",
      "Difference : 0.250\n",
      "183\n",
      "Real Values : 13.0\n",
      "Predicted G3's score is 14.36\n",
      "Difference : 1.360\n",
      "184\n",
      "Real Values : 12.0\n",
      "Predicted G3's score is 11.37\n",
      "Difference : 0.630\n",
      "185\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.70\n",
      "Difference : 0.700\n",
      "186\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 10.33\n",
      "Difference : 0.670\n",
      "187\n",
      "Real Values : 9.0\n",
      "Predicted G3's score is 10.24\n",
      "Difference : 1.240\n",
      "188\n",
      "Real Values : 11.0\n",
      "Predicted G3's score is 11.33\n",
      "Difference : 0.330\n",
      "189\n",
      "Real Values : 14.0\n",
      "Predicted G3's score is 13.70\n",
      "Difference : 0.300\n"
     ]
    }
   ],
   "source": [
    "diff_list = []\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    \n",
    "    g_truth = float(y_test_np[i])\n",
    "    p = float(predictions[i])\n",
    "    diff = np.abs(p-g_truth)\n",
    "    diff_list.append(diff)\n",
    "    print(i)  \n",
    "    print(\"Real Values :\", g_truth)\n",
    "    print(\"Predicted G3's score is {:.2f}\".format(p))\n",
    "    print(\"Difference : {:.3f}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-making",
   "metadata": {},
   "source": [
    "### Mean Absolute Error after applying genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "laughing-framework",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diff_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1801139fefd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_abs_e_on_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# mean_abs_e_on_test_set = mean_absolute_error(y_test_np, predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Absolute Error : {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_abs_e_on_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diff_list' is not defined"
     ]
    }
   ],
   "source": [
    "mean_abs_e_on_test_set = sum(diff_list) / len(y_test)\n",
    "# mean_abs_e_on_test_set = mean_absolute_error(y_test_np, predictions)\n",
    "print(\"Mean Absolute Error : {:.5f}\".format(mean_abs_e_on_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-corpus",
   "metadata": {},
   "source": [
    "#### Check those with difference(between predicted and ground truth) that exceeds 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "scientific-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceed3idx = []\n",
    "def check(diff_list):\n",
    "    for idx, diff in enumerate(diff_list):\n",
    "        if diff > 3:\n",
    "            exceed3idx.append(idx)\n",
    "    print(exceed3idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "restricted-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83]\n"
     ]
    }
   ],
   "source": [
    "check(diff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "unsigned-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exceed3idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
