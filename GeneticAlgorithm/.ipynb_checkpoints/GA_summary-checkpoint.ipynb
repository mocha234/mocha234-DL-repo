{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smaller-color",
   "metadata": {},
   "source": [
    "# Some findings\n",
    "\n",
    "In this section, I will summarize up what I the findings and comparison between methods useed, focusing on the following:\n",
    " - Time\n",
    " - Computations\n",
    " - Results\n",
    "\n",
    "The method used to tune the Hyperparameter are:\n",
    "1. Empirically\n",
    "2. Grid Search Cross Validation\n",
    "3. Applying Genetic Algorithm(with Model from step 2.)\n",
    "4. Ensemble of step 2. and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-youth",
   "metadata": {},
   "source": [
    "# Notebooks Navigation:\n",
    "\n",
    "1. [Genetic Algorithm with Python](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_with_Python.ipynb) --> Understanding Genetic Algorithm with Python(without libraries)\n",
    "2. [Predicting Student Performance(Empirically)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_with_NN.ipynb) --> Empirically tune hyper-parameters\n",
    "3. [Predicting Student Performance(SKLearn's GridSearchCV)](ww) --> Tune hyper-parameters with Sci-Kit Learn's GridSearchCV\n",
    "4. [Predicting Student Performance(Genetic Algorithm with PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_PyGAD.ipynb) --> Tune hyper-parameters with GA using PyGAD\n",
    "\n",
    "5. [Predicting Student Performance(Genetic Algorithm with Trained Model + PyGAD)](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_Tensorflow_PyGAD.ipynb) --> Tune GA hyper-parameters using PyGAD of a trained Model\n",
    "\n",
    "6. [Some Findings, Comparison, Summary](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/GA_summary.ipynb) --> Summary of this learning \"series\"\n",
    "\n",
    "7. [Using GA to find the Best Hyperparameters](https://github.com/mocha234/mocha234-DL-repo/blob/main/GeneticAlgorithm/StudentPerformance_with_HParamter_GA_Tuning.ipynb) --> Using GA to find the Best Hyperparameters, encoding desire hyperparameter into gene in string of chromosome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-young",
   "metadata": {},
   "source": [
    "# Time\n",
    "\n",
    "#### Reading\n",
    "I guess the time used mainly for reading up on Genetic Algorithm as it has many things in it to be discovered, for instance, the Selection Method, Mutation Type, Crossover Method etc.\n",
    "\n",
    "#### Data Preprocessing\n",
    "On the other hand, the data preprocessing is relatively simple as there's no empty data. But I had to redo all methods used as I forget to omit the outliers which bring big difference in accuracy of the model after omitting them.\n",
    "\n",
    "#### Data normalization\n",
    "As part of Data preprocessing, I wasted quite some time in normalizing the data due to some misunderstanding on the concept. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-coupon",
   "metadata": {},
   "source": [
    "# Computations\n",
    "\n",
    "#### Empirically\n",
    "\n",
    "Doing it per experiment requries lots of time to achieve so-so results. However, with this dataset, which considered very small scale, doing it emprirically still doable. \n",
    "\n",
    "#### Grid Search Cross Validation\n",
    "\n",
    "Among the 3 approach, this takes up the most computation power as I grid search through 5 different hyperparameters with averagely more than 5 different parameter to be tested per hyperparameter.\n",
    "\n",
    "#### Applying Genetic Algorithm(with Model from step 2.)\n",
    "\n",
    "By utilizing the Grid Searched model configuration and further training it with Genetic Algorithm, the computation won't cost as much as during Grid Search Cross validating.\n",
    "\n",
    "#### Ensemble of 2. and 3.\n",
    "\n",
    "Relatively less heavy than GridSearchCV approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-tablet",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opening-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_PyGAD = 3.67074\n",
    "mae_TF_n_PyGAD = 0.78795\n",
    "mae_Manual = 0.41011\n",
    "mae_GridSearchCV = 0.65742\n",
    "mae_GA_HyperparameterTuning = 0.68742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occasional-incident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Empirically</th>\n",
       "      <th>Grid Search</th>\n",
       "      <th>GA with PyGAD</th>\n",
       "      <th>GA with PyGAD on Trained TF Model</th>\n",
       "      <th>mae_GA_HyperparameterTuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <td>0.41011</td>\n",
       "      <td>0.65742</td>\n",
       "      <td>3.67074</td>\n",
       "      <td>0.78795</td>\n",
       "      <td>0.68742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Empirically  Grid Search  GA with PyGAD  \\\n",
       "Mean Absolute Error      0.41011      0.65742        3.67074   \n",
       "\n",
       "                     GA with PyGAD on Trained TF Model  \\\n",
       "Mean Absolute Error                            0.78795   \n",
       "\n",
       "                     mae_GA_HyperparameterTuning  \n",
       "Mean Absolute Error                      0.68742  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results_array = np.array([[mae_Manual, mae_GridSearchCV, mae_PyGAD, mae_TF_n_PyGAD, mae_GA_HyperparameterTuning ]])\n",
    "\n",
    "df = pd.DataFrame(results_array, \n",
    "                  columns = [\"Empirically\", \"Grid Search\", \"GA with PyGAD\", \"GA with PyGAD on Trained TF Model\", \"mae_GA_HyperparameterTuning\"],\n",
    "                 index = [\"Mean Absolute Error\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-extension",
   "metadata": {},
   "source": [
    "## Comparing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-smooth",
   "metadata": {},
   "source": [
    "Turns out Empirically Hyperparameter Tuning yields the best results, followed by GridSearchCV and TF + PyGAD an PyGAD alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-penetration",
   "metadata": {},
   "source": [
    "I believe there are many factors to it why empirical method yield better with respect to applying Genetic Algorithm, such as:\n",
    "\n",
    "    1. I did not etensively tune the Genetic Algorithm's Hyperparameters(Selection methods, Crossover Type, Mutation Type). I only managed to try and some of them.\n",
    "    2. Empirical method takes shorter time per try, hence, I end up having more try and leads to using more time tuning with different hyperparameters as \n",
    "    3. Not enough Generations/Iteration with Genetic Algorithm?\n",
    "    4. Maybe bug code?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-vintage",
   "metadata": {},
   "source": [
    "### Best results: GA with Hyperparameter Tuning\n",
    "\n",
    "- Lesser Time needed\n",
    "- Computational Cheaper\n",
    "- Achieve Results as good as GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-product",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "PyGAD can come in handy as it is relatively easy to use due to the nature of the library. This method pis slightly better for development compared to Empirical Method and GridSearchCV as GridSearchCV takes up lots of Computation Power and Time, while Empirical Method takes a lot time to manually tune the model.\n",
    "\n",
    "On the other hand, the dataset is considered small(with 649 entries), hence Empirical Method and GridSearchCV is still relevant. However, as for image classification taks or tasks with huge dataset, I think it is not suitable to use neither Empirical Method nor GridSearchCV as every run after tuning takes a lot of time. In that case, PyGAD can be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
